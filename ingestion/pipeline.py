from __future__ import annotations

import time
from typing import Any, Optional
from loguru import logger
from tqdm import tqdm
from ingestion.crawler import crawl, crawl_mkdocs_index, crawl_sitemap, crawl_with_sitemap_progress
from ingestion.parsers_migration import extract_url_metadata
from ingestion.processors.content_processor import ContentProcessor
from app.sources_registry import get_source_config
from ingestion.chunker import chunk_text, text_hash
from ingestion.indexer import upsert_chunks
from app.services.metadata_aware_indexer import MetadataAwareIndexer
from app.services.optimized_pipeline import run_optimized_indexing


# –§—É–Ω–∫—Ü–∏—è classify_page —É–¥–∞–ª–µ–Ω–∞ - —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ContentProcessor –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã


def crawl_and_index(incremental: bool = True, strategy: str = "jina", use_cache: bool = True, reindex_mode: str = "auto", max_pages: int = None, source_name: Optional[str] = None) -> dict[str, Any]:
    """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –∫—Ä–∞—É–ª–∏–Ω–≥ ‚Üí —á–∞–Ω–∫–∏–Ω–≥ ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Üí upsert –≤ Qdrant.

    Args:
        incremental: –µ—Å–ª–∏ True ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        strategy: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è crawling (jina, http)
        use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ crawling
        reindex_mode: —Ä–µ–∂–∏–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:
            - "auto": —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            - "force": –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            - "cache_only": –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –∏ —á–∞–Ω–∫–∞–º
    """
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º {'–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é ' if incremental else ''}–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é")
    logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: strategy={strategy}, use_cache={use_cache}, reindex_mode={reindex_mode}")

    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∏—Å—Ç–æ—á–Ω–∏–∫, –ø—Ä–∏–º–µ–Ω—è–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    if source_name:
        try:
            cfg = get_source_config(source_name)
            strategy = cfg.strategy or strategy
            use_cache = cfg.use_cache if cfg.use_cache is not None else use_cache
            if max_pages is None and cfg.max_pages:
                max_pages = cfg.max_pages
            logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name} | strategy={strategy} use_cache={use_cache} max_pages={max_pages}")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ '{source_name}': {e}. –ò—Å–ø–æ–ª—å–∑—É—é –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ—É–Ω–∫—Ü–∏–∏.")

    # 1) –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π crawling —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    if reindex_mode == "cache_only":
        logger.info("–†–µ–∂–∏–º cache_only: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        from ingestion.crawl_cache import get_crawl_cache
        cache = get_crawl_cache()
        pages = []

        for url in cache.get_cached_urls():
            cached_page = cache.get_page(url)
            if cached_page:
                pages.append({
                    "url": url,
                    "html": cached_page.html,
                    "text": cached_page.text,
                    "title": cached_page.title,
                    "cached": True
                })

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                if max_pages and len(pages) >= max_pages:
                    break

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∫–µ—à–∞")
    else:
        pages = crawl_with_sitemap_progress(strategy=strategy, use_cache=use_cache, max_pages=max_pages)

    # 2) –§–æ–ª–±—ç–∫–∏ –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
    if not pages:
        logger.warning("Sitemap crawling –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º MkDocs index...")
        pages = crawl_mkdocs_index()
    if not pages:
        logger.warning("MkDocs index –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–±—É–µ–º HTTP –æ–±—Ö–æ–¥...")
        pages = crawl(strategy="http")
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    all_chunks = []
    logger.info("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏...")

    # –ù–û–í–û–ï: –µ–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –≤–º–µ—Å—Ç–æ universal_loader)
    processor = ContentProcessor()

    with tqdm(total=len(pages), desc="Processing pages") as pbar:
        for p in pages:
            url = p["url"]
            raw_content = p.get("text") or p.get("html") or ""

            if not raw_content:
                logger.warning(f"–ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è {url}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                pbar.update(1)
                continue

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π ContentProcessor (–≤–º–µ—Å—Ç–æ universal_loader)
            # –í–ù–ò–ú–ê–ù–ò–ï: —Å–∏–≥–Ω–∞—Ç—É—Ä–∞ process(raw_content, url, strategy)
            try:
                processed = processor.process(raw_content, url, strategy)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                text = processed.content or ''
                title = processed.title or 'Untitled'
                page_type = processed.page_type  # ContentProcessor —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã

                if not text:
                    logger.warning(f"–ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è {url}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    pbar.update(1)
                    continue

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è {url}: {e}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                pbar.update(1)
                continue

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
            chunks_text = chunk_text(text)

            if not chunks_text:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è {url}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                pbar.update(1)
                continue

            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Å –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            for i, ct in enumerate(chunks_text):
                # –ë–∞–∑–æ–≤—ã–π payload
                payload = {
                    "url": url,
                    "content_type": strategy if strategy in ["jina", "html"] else "auto",
                    "page_type": page_type,
                    "source": "docs-site",
                    "language": "ru",
                    "title": title,
                    "text": ct,
                    "indexed_via": strategy,
                    "indexed_at": time.time(),
                    "chunk_index": i,
                    "content_length": len(text),
                }

                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
                for key, value in (processed.metadata or {}).items():
                    if key not in ['url', 'title', 'content', 'page_type'] and value is not None:
                        payload[key] = value

                all_chunks.append({
                    "text": ct,
                    "payload": payload,
                })
            pbar.update(1)

    logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤, –Ω–∞—á–∏–Ω–∞–µ–º enhanced metadata-aware –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é...")

    # Enhanced metadata-aware –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    metadata_indexer = MetadataAwareIndexer()
    total_chunks = 0

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º enhanced metadata indexer –¥–ª—è –≤—Å–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —á–∞–Ω–∫–æ–≤
    indexed = metadata_indexer.index_chunks_with_metadata(all_chunks)
    total_chunks = indexed
    return {"pages": len(pages), "chunks": total_chunks}


def crawl_and_index_optimized(
    source_name: str = "edna_docs",
    max_pages: Optional[int] = None,
    chunk_strategy: str = "adaptive"
) -> dict[str, Any]:
    """Optimized crawling and indexing using new architecture.

    Args:
        source_name: Name of the data source to use
        max_pages: Maximum number of pages to process
        chunk_strategy: Chunking strategy ("adaptive" or "standard")

    Returns:
        Dictionary with indexing results and statistics
    """
    logger.info(f"Starting optimized indexing with source: {source_name}")

    try:
        # Use the new optimized pipeline
        result = run_optimized_indexing(
            source_name=source_name,
            max_pages=max_pages,
            chunk_strategy=chunk_strategy
        )

        if result["success"]:
            logger.info(f"‚úÖ Optimized indexing completed successfully:")
            logger.info(f"   Pages: {result['pages']}")
            logger.info(f"   Chunks: {result['chunks']}")
            logger.info(f"   Duration: {result['duration']:.2f}s")
        else:
            logger.error(f"‚ùå Optimized indexing failed: {result.get('error', 'Unknown error')}")

        return result

    except Exception as e:
        error_msg = f"Optimized indexing failed: {e}"
        logger.error(error_msg)
        return {
            "success": False,
            "error": error_msg,
            "pages": 0,
            "chunks": 0,
            "duration": 0.0
        }


if __name__ == "__main__":
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å BGE-M3 unified embeddings...")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    from app.config import CONFIG
    logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:")
    logger.info(f"  EMBEDDINGS_BACKEND: {CONFIG.embeddings_backend}")
    logger.info(f"  EMBEDDING_DEVICE: {CONFIG.embedding_device}")
    logger.info(f"  USE_SPARSE: {CONFIG.use_sparse}")
    logger.info(f"  EMBEDDING_MAX_LENGTH_DOC: {CONFIG.embedding_max_length_doc}")
    logger.info(f"  EMBEDDING_BATCH_SIZE: {CONFIG.embedding_batch_size}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
    from app.services.bge_embeddings import _get_optimal_backend_strategy
    optimal_backend = _get_optimal_backend_strategy()
    logger.info(f"  –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {optimal_backend}")

    try:
        stats = crawl_and_index(incremental=False)
        logger.success(f"‚úÖ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        logger.success(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats['pages']} —Å—Ç—Ä–∞–Ω–∏—Ü, {stats['chunks']} —á–∞–Ω–∫–æ–≤")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        raise
