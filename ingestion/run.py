"""
–ï–¥–∏–Ω—ã–π entrypoint –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ DAG
"""

import sys
import argparse
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))

from ingestion.adapters.docusaurus import DocusaurusAdapter
from ingestion.adapters.website import WebsiteAdapter
from ingestion.normalizers.base import Parser, BaseNormalizer
from ingestion.normalizers.docusaurus import DocusaurusNormalizer, URLMapper
from ingestion.normalizers.html import HtmlNormalizer, ContentExtractor
from ingestion.pipeline.chunker import UnifiedChunkerStep
from ingestion.pipeline.embedder import Embedder
from ingestion.pipeline.indexers.qdrant_writer import QdrantWriter
from ingestion.pipeline.dag import PipelineDAG
from ingestion.state.state_manager import get_state_manager
from app.config.app_config import CONFIG


def _clear_qdrant_collection(collection_name: str) -> None:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é Qdrant."""
    try:
        from qdrant_client import QdrantClient
        from app.config.app_config import CONFIG

        client = QdrantClient(
            url=CONFIG.qdrant_url,
            api_key=CONFIG.qdrant_api_key or None
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è
        try:
            collection_info = client.get_collection(collection_name)
            logger.info(f"üìä –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å–æ–¥–µ—Ä–∂–∏—Ç {collection_info.points_count} —Ç–æ—á–µ–∫")
        except Exception:
            logger.info(f"üìä –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é")
            return

        # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ç–æ—á–∫–∏ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        from qdrant_client.models import Filter
        client.delete(
            collection_name=collection_name,
            points_selector=Filter()  # –ü—É—Å—Ç–æ–π —Ñ–∏–ª—å—Ç—Ä —É–¥–∞–ª—è–µ—Ç –≤—Å–µ —Ç–æ—á–∫–∏
        )

        logger.success(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω–∞")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}: {e}")
        raise


def create_docusaurus_dag(config: Dict[str, Any]) -> PipelineDAG:
    """–°–æ–∑–¥–∞–µ—Ç DAG –¥–ª—è Docusaurus –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    steps = [
        Parser(),
        DocusaurusNormalizer(site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru")),
        URLMapper(
            site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru"),
            site_docs_prefix=config.get("site_docs_prefix", "/docs")
        ),
        UnifiedChunkerStep(
            max_tokens=config.get("chunk_max_tokens", 600),
            min_tokens=config.get("chunk_min_tokens", 350),
            overlap_base=config.get("chunk_overlap_base", 100),
            oversize_block_policy=config.get("chunk_oversize_block_policy", "split"),
            oversize_block_limit=config.get("chunk_oversize_block_limit", 1200)
        ),
        Embedder(batch_size=config.get("batch_size", 16)),
        QdrantWriter(collection_name=config.get("collection_name", CONFIG.qdrant_collection))
    ]

    return PipelineDAG(steps)


def create_website_dag(config: Dict[str, Any]) -> PipelineDAG:
    """–°–æ–∑–¥–∞–µ—Ç DAG –¥–ª—è –≤–µ–±-—Å–∞–π—Ç–æ–≤."""
    steps = [
        Parser(),
        HtmlNormalizer(),
        ContentExtractor(),
        BaseNormalizer(),
        UnifiedChunkerStep(
            max_tokens=config.get("chunk_max_tokens", 600),
            min_tokens=config.get("chunk_min_tokens", 350),
            overlap_base=config.get("chunk_overlap_base", 100),
            oversize_block_policy=config.get("chunk_oversize_block_policy", "split"),
            oversize_block_limit=config.get("chunk_oversize_block_limit", 1200)
        ),
        Embedder(batch_size=config.get("batch_size", 16)),
        QdrantWriter(collection_name=config.get("collection_name", CONFIG.qdrant_collection))
    ]

    return PipelineDAG(steps)


def run_unified_indexing(
    source_type: str,
    config: Dict[str, Any],
    reindex_mode: str = "changed",
    clear_collection: bool = False
) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.

    Args:
        source_type: –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ ("docusaurus", "website")
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        reindex_mode: –†–µ–∂–∏–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ ("full", "changed")
        clear_collection: –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    """
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")

    # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
    if clear_collection:
        logger.warning("üóëÔ∏è –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π")
        _clear_qdrant_collection(config.get("collection_name", CONFIG.qdrant_collection))

    try:
        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if source_type == "docusaurus":
            adapter = DocusaurusAdapter(
                docs_root=config["docs_root"],
                site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru"),
                site_docs_prefix=config.get("site_docs_prefix", "/docs")
            )
            dag = create_docusaurus_dag(config)

        elif source_type == "website":
            adapter = WebsiteAdapter(
                seed_urls=config["seed_urls"],
                base_url=config.get("base_url"),
                render_js=config.get("render_js", False),
                max_pages=config.get("max_pages")
            )
            dag = create_website_dag(config)

        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        writer = dag.steps[-1]  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ - QdrantWriter
        if isinstance(writer, QdrantWriter):
            logger.info("üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏...")
            writer.ensure_collection()

        # –ó–∞–ø—É—Å–∫–∞–µ–º DAG
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ DAG —Å {len(dag.steps)} —à–∞–≥–∞–º–∏:")
        for step in dag.steps:
            logger.info(f"  - {step.get_step_name()}")

        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞
        logger.info("üì• –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞...")
        documents = adapter.iter_documents()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ DAG
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ DAG...")
        stats = dag.run(documents)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        with get_state_manager() as state_manager:
            logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")

        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.success(f"üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {source_type} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        if isinstance(stats, dict):
            logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            logger.info(f"  üìÑ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {stats.get('total_chunks', 'N/A')}")
            logger.info(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats.get('processed_chunks', 'N/A')}")
            logger.info(f"  ‚ùå –û—à–∏–±–æ–∫: {stats.get('failed_chunks', 'N/A')}")
            logger.info(f"  üî¢ –ë–∞—Ç—á–µ–π: {stats.get('batches_processed', 'N/A')}")
            logger.info(f"  üéØ –ù—É–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {stats.get('zero_dense_vectors', 'N/A')}")
            logger.info(f"  üíæ –ü–æ—Å–ª–µ–¥–Ω–∏–π upsert: {stats.get('last_upsert_points', 'N/A')} —Ç–æ—á–µ–∫")

        return {
            "success": True,
            "source_type": source_type,
            "stats": stats,
            "message": f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {source_type} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"
        }

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {source_type}: {e}")
        return {
            "success": False,
            "source_type": source_type,
            "error": str(e),
            "message": f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {source_type} –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π"
        }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI."""
    parser = argparse.ArgumentParser(
        description="–ï–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"
    )

    parser.add_argument(
        "--source",
        choices=["docusaurus", "website"],
        required=True,
        help="–¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
    )

    parser.add_argument(
        "--docs-root",
        help="–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π (–¥–ª—è docusaurus)"
    )

    parser.add_argument(
        "--site-base-url",
        default="https://docs-chatcenter.edna.ru",
        help="–ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞"
    )

    parser.add_argument(
        "--site-docs-prefix",
        default="/docs",
        help="–ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ URL"
    )

    parser.add_argument(
        "--seed-urls",
        nargs="+",
        help="–ù–∞—á–∞–ª—å–Ω—ã–µ URL –¥–ª—è –æ–±—Ö–æ–¥–∞ (–¥–ª—è website)"
    )

    parser.add_argument(
        "--collection-name",
        default=CONFIG.qdrant_collection,
        help="–ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=16,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )

    parser.add_argument(
        "--chunk-max-tokens",
        type=int,
        default=600,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ"
    )

    parser.add_argument(
        "--chunk-overlap-tokens",
        type=int,
        default=120,
        help="–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Ç–æ–∫–µ–Ω–∞—Ö"
    )

    parser.add_argument(
        "--reindex-mode",
        choices=["full", "changed"],
        default="changed",
        help="–†–µ–∂–∏–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"
    )

    parser.add_argument(
        "--clear-collection",
        action="store_true",
        help="–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π (—É–¥–∞–ª—è–µ—Ç –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ)"
    )

    parser.add_argument(
        "--render-js",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Playwright –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ JS (–¥–ª—è website)"
    )

    parser.add_argument(
        "--max-pages",
        type=int,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )

    args = parser.parse_args()

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = {
        "site_base_url": args.site_base_url,
        "site_docs_prefix": args.site_docs_prefix,
        "collection_name": args.collection_name,
        "batch_size": args.batch_size,
        "chunk_max_tokens": args.chunk_max_tokens,
        "chunk_overlap_tokens": args.chunk_overlap_tokens,
        "reindex_mode": args.reindex_mode
    }

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if args.source == "docusaurus":
        if not args.docs_root:
            logger.error("–î–ª—è docusaurus –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è --docs-root")
            sys.exit(1)
        config["docs_root"] = args.docs_root

    elif args.source == "website":
        if not args.seed_urls:
            logger.error("–î–ª—è website –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è --seed-urls")
            sys.exit(1)
        config["seed_urls"] = args.seed_urls
        config["render_js"] = args.render_js
        if args.max_pages:
            config["max_pages"] = args.max_pages

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    result = run_unified_indexing(args.source, config, args.reindex_mode, args.clear_collection)

    if result["success"]:
        logger.success("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {result['stats']}")
        sys.exit(0)
    else:
        logger.error(f"‚ùå –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π: {result['error']}")
        sys.exit(1)


if __name__ == "__main__":
    main()
