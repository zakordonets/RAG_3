"""
–ï–¥–∏–Ω—ã–π entrypoint –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ DAG
"""

import sys
import argparse
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))

from ingestion.adapters.docusaurus import DocusaurusAdapter
from ingestion.adapters.website import WebsiteAdapter
from ingestion.normalizers.base import Parser, BaseNormalizer
from ingestion.normalizers.docusaurus import DocusaurusNormalizer, URLMapper
from ingestion.normalizers.html import HtmlNormalizer, ContentExtractor
from ingestion.pipeline.chunker import UnifiedChunkerStep
from ingestion.pipeline.embedder import Embedder
from ingestion.pipeline.indexers.qdrant_writer import QdrantWriter
from ingestion.pipeline.dag import PipelineDAG
from ingestion.state.state_manager import get_state_manager


def create_docusaurus_dag(config: Dict[str, Any]) -> PipelineDAG:
    """–°–æ–∑–¥–∞–µ—Ç DAG –¥–ª—è Docusaurus –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    steps = [
        Parser(),
        DocusaurusNormalizer(site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru")),
        URLMapper(
            site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru"),
            site_docs_prefix=config.get("site_docs_prefix", "/docs")
        ),
        UnifiedChunkerStep(
            max_tokens=config.get("chunk_max_tokens", 600),
            min_tokens=config.get("chunk_min_tokens", 350),
            overlap_base=config.get("chunk_overlap_base", 100),
            oversize_block_policy=config.get("chunk_oversize_block_policy", "split"),
            oversize_block_limit=config.get("chunk_oversize_block_limit", 1200)
        ),
        Embedder(batch_size=config.get("batch_size", 16)),
        QdrantWriter(collection_name=config.get("collection_name", "docs_chatcenter"))
    ]

    return PipelineDAG(steps)


def create_website_dag(config: Dict[str, Any]) -> PipelineDAG:
    """–°–æ–∑–¥–∞–µ—Ç DAG –¥–ª—è –≤–µ–±-—Å–∞–π—Ç–æ–≤."""
    steps = [
        Parser(),
        HtmlNormalizer(),
        ContentExtractor(),
        BaseNormalizer(),
        UnifiedChunkerStep(
            max_tokens=config.get("chunk_max_tokens", 600),
            min_tokens=config.get("chunk_min_tokens", 350),
            overlap_base=config.get("chunk_overlap_base", 100),
            oversize_block_policy=config.get("chunk_oversize_block_policy", "split"),
            oversize_block_limit=config.get("chunk_oversize_block_limit", 1200)
        ),
        Embedder(batch_size=config.get("batch_size", 16)),
        QdrantWriter(collection_name=config.get("collection_name", "docs_chatcenter"))
    ]

    return PipelineDAG(steps)


def run_unified_indexing(
    source_type: str,
    config: Dict[str, Any],
    reindex_mode: str = "changed"
) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.

    Args:
        source_type: –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ ("docusaurus", "website")
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        reindex_mode: –†–µ–∂–∏–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ ("full", "changed")

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    """
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")

    try:
        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if source_type == "docusaurus":
            adapter = DocusaurusAdapter(
                docs_root=config["docs_root"],
                site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru"),
                site_docs_prefix=config.get("site_docs_prefix", "/docs")
            )
            dag = create_docusaurus_dag(config)

        elif source_type == "website":
            adapter = WebsiteAdapter(
                seed_urls=config["seed_urls"],
                base_url=config.get("base_url"),
                render_js=config.get("render_js", False),
                max_pages=config.get("max_pages")
            )
            dag = create_website_dag(config)

        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã payload
        writer = dag.steps[-1]  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ - QdrantWriter
        if isinstance(writer, QdrantWriter):
            logger.info("üìã –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ payload...")
            writer.create_payload_indexes()

        # –ó–∞–ø—É—Å–∫–∞–µ–º DAG
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ DAG —Å {len(dag.steps)} —à–∞–≥–∞–º–∏:")
        for step in dag.steps:
            logger.info(f"  - {step.get_step_name()}")

        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞
        documents = adapter.iter_documents()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ DAG
        stats = dag.run(documents)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        with get_state_manager() as state_manager:
            logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")

        return {
            "success": True,
            "source_type": source_type,
            "stats": stats,
            "message": f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {source_type} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"
        }

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {source_type}: {e}")
        return {
            "success": False,
            "source_type": source_type,
            "error": str(e),
            "message": f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {source_type} –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π"
        }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI."""
    parser = argparse.ArgumentParser(
        description="–ï–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"
    )

    parser.add_argument(
        "--source",
        choices=["docusaurus", "website"],
        required=True,
        help="–¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
    )

    parser.add_argument(
        "--docs-root",
        help="–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π (–¥–ª—è docusaurus)"
    )

    parser.add_argument(
        "--site-base-url",
        default="https://docs-chatcenter.edna.ru",
        help="–ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞"
    )

    parser.add_argument(
        "--site-docs-prefix",
        default="/docs",
        help="–ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ URL"
    )

    parser.add_argument(
        "--seed-urls",
        nargs="+",
        help="–ù–∞—á–∞–ª—å–Ω—ã–µ URL –¥–ª—è –æ–±—Ö–æ–¥–∞ (–¥–ª—è website)"
    )

    parser.add_argument(
        "--collection-name",
        default="docs_chatcenter",
        help="–ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=16,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )

    parser.add_argument(
        "--chunk-max-tokens",
        type=int,
        default=600,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ"
    )

    parser.add_argument(
        "--chunk-overlap-tokens",
        type=int,
        default=120,
        help="–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Ç–æ–∫–µ–Ω–∞—Ö"
    )

    parser.add_argument(
        "--reindex-mode",
        choices=["full", "changed"],
        default="changed",
        help="–†–µ–∂–∏–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"
    )

    parser.add_argument(
        "--render-js",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Playwright –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ JS (–¥–ª—è website)"
    )

    parser.add_argument(
        "--max-pages",
        type=int,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )

    args = parser.parse_args()

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = {
        "site_base_url": args.site_base_url,
        "site_docs_prefix": args.site_docs_prefix,
        "collection_name": args.collection_name,
        "batch_size": args.batch_size,
        "chunk_max_tokens": args.chunk_max_tokens,
        "chunk_overlap_tokens": args.chunk_overlap_tokens,
        "reindex_mode": args.reindex_mode
    }

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if args.source == "docusaurus":
        if not args.docs_root:
            logger.error("–î–ª—è docusaurus –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è --docs-root")
            sys.exit(1)
        config["docs_root"] = args.docs_root

    elif args.source == "website":
        if not args.seed_urls:
            logger.error("–î–ª—è website –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è --seed-urls")
            sys.exit(1)
        config["seed_urls"] = args.seed_urls
        config["render_js"] = args.render_js
        if args.max_pages:
            config["max_pages"] = args.max_pages

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    result = run_unified_indexing(args.source, config, args.reindex_mode)

    if result["success"]:
        logger.success("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {result['stats']}")
        sys.exit(0)
    else:
        logger.error(f"‚ùå –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π: {result['error']}")
        sys.exit(1)


if __name__ == "__main__":
    main()
