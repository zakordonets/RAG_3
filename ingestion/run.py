"""
–ï–¥–∏–Ω—ã–π entrypoint –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ DAG
"""

import sys
import argparse
from pathlib import Path
from typing import Dict, Any, Optional, List
from copy import deepcopy
import yaml
from loguru import logger

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))

from ingestion.adapters.docusaurus import DocusaurusAdapter
from ingestion.adapters.website import WebsiteAdapter
from ingestion.normalizers.base import Parser, BaseNormalizer
from ingestion.normalizers.docusaurus import DocusaurusNormalizer, URLMapper
from ingestion.normalizers.html import HtmlNormalizer, ContentExtractor
from ingestion.pipeline.chunker import UnifiedChunkerStep
from ingestion.pipeline.embedder import Embedder
from ingestion.pipeline.indexers.qdrant_writer import QdrantWriter
from ingestion.pipeline.dag import PipelineDAG
from ingestion.state.state_manager import get_state_manager
from app.config.app_config import CONFIG
from ingestion.metadata.docusaurus import DocusaurusMetadataMapper


def _deep_merge_dicts(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–ª–æ–≤–∞—Ä–∏, –Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—è –æ—Ä–∏–≥–∏–Ω–∞–ª—ã."""
    merged = deepcopy(base) if base else {}
    for key, value in (override or {}).items():
        if (
            key in merged
            and isinstance(merged[key], dict)
            and isinstance(value, dict)
        ):
            merged[key] = _deep_merge_dicts(merged[key], value)
        else:
            merged[key] = deepcopy(value)
    return merged


def _build_docusaurus_metadata_mapper(meta_cfg: Optional[Dict[str, Any]]) -> Optional[DocusaurusMetadataMapper]:
    """–°–æ–∑–¥–∞–µ—Ç –º–∞–ø–ø–µ—Ä —Ç–µ–º–∞—Ç–∏–∫ Docusaurus –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    if not meta_cfg:
        return None
    domain = meta_cfg.get("domain")
    if not domain:
        logger.warning("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º metadata –º–∞–ø–ø–µ—Ä –±–µ–∑ domain")
        return None
    return DocusaurusMetadataMapper(
        domain=domain,
        section_by_dir=meta_cfg.get("section_by_dir", {}) or {},
        role_by_section=meta_cfg.get("role_by_section", {}) or {},
        platform_by_dir=meta_cfg.get("platform_by_dir", {}) or {},
        page_type_by_dir=meta_cfg.get("page_type_by_dir", {}) or {},
        fixed_section=meta_cfg.get("fixed_section"),
        fixed_role=meta_cfg.get("fixed_role"),
        fixed_platform=meta_cfg.get("fixed_platform"),
        default_section=meta_cfg.get("default_section"),
        default_role=meta_cfg.get("default_role"),
        default_platform=meta_cfg.get("default_platform"),
    )


def load_sources_from_config(config_path: str, profile: Optional[str] = None) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ YAML-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    cfg_file = Path(config_path)
    if not cfg_file.exists():
        raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")

    try:
        raw_data = yaml.safe_load(cfg_file.read_text(encoding="utf-8")) or {}
    except yaml.YAMLError as exc:
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_path}: {exc}") from exc

    data = deepcopy(raw_data)
    if profile:
        profiles = raw_data.get("profiles", {})
        if profile not in profiles:
            raise ValueError(f"–ü—Ä–æ—Ñ–∏–ª—å '{profile}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {config_path}")
        profile_section = profiles[profile]
        if "global" in profile_section:
            data["global"] = _deep_merge_dicts(
                data.get("global", {}),
                profile_section["global"]
            )
        if "sources" in profile_section:
            data.setdefault("sources", {})
            for name, overrides in profile_section["sources"].items():
                data["sources"][name] = _deep_merge_dicts(
                    data["sources"].get(name, {}),
                    overrides
                )

    global_cfg = data.get("global", {})
    global_indexing = global_cfg.get("indexing", {}) or {}
    global_qdrant = global_cfg.get("qdrant", {}) or {}

    sources_cfg = data.get("sources", {}) or {}
    normalized_sources: List[Dict[str, Any]] = []

    for source_name, source_cfg in sources_cfg.items():
        if not isinstance(source_cfg, dict):
            continue
        if not source_cfg.get("enabled", True):
            continue

        source_type = source_cfg.get("type", "docusaurus")
        if source_type not in {"docusaurus", "website"}:
            logger.warning("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ {} —Å –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º —Ç–∏–ø–æ–º {}", source_name, source_type)
            continue

        if source_type == "docusaurus":
            docs_root = source_cfg.get("docs_root")
            if not docs_root:
                raise ValueError(f"–ò—Å—Ç–æ—á–Ω–∏–∫ '{source_name}' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç docs_root")
            routing_cfg = source_cfg.get("routing", {}) or {}
            chunk_cfg = source_cfg.get("chunk", {}) or {}
            source_indexing = source_cfg.get("indexing", {}) or {}

            run_config: Dict[str, Any] = {
                "docs_root": docs_root,
                "site_base_url": source_cfg.get("site_base_url", "https://docs-chatcenter.edna.ru"),
                "site_docs_prefix": source_cfg.get("site_docs_prefix", "/docs"),
                "collection_name": source_cfg.get("collection_name", global_qdrant.get("collection", CONFIG.qdrant_collection)),
                "batch_size": source_cfg.get("batch_size", global_indexing.get("batch_size", 16)),
                "chunk_max_tokens": chunk_cfg.get("max_tokens", 600),
                "chunk_min_tokens": chunk_cfg.get("min_tokens", 350),
                "chunk_overlap_base": chunk_cfg.get("overlap_base", 100),
                "chunk_oversize_block_policy": chunk_cfg.get("oversize_block_policy", "split"),
                "chunk_oversize_block_limit": chunk_cfg.get("oversize_block_limit", 1200),
                "drop_prefix_all_levels": routing_cfg.get("drop_numeric_prefix_in_first_level", True),
                "top_level_meta": source_cfg.get("top_level_meta"),
                "max_pages": source_cfg.get("max_pages"),
                "metadata": source_cfg.get("metadata"),
            }

            normalized_sources.append({
                "name": source_name,
                "source_type": source_type,
                "config": run_config,
                "reindex_mode": source_indexing.get("reindex_mode", global_indexing.get("reindex_mode", "changed"))
            })

        elif source_type == "website":
            seed_urls = source_cfg.get("seed_urls")
            if not seed_urls:
                raise ValueError(f"–ò—Å—Ç–æ—á–Ω–∏–∫ '{source_name}' (website) –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç seed_urls")
            run_config = {
                "seed_urls": seed_urls,
                "base_url": source_cfg.get("base_url"),
                "render_js": source_cfg.get("render_js", False),
                "max_pages": source_cfg.get("max_pages"),
                "collection_name": source_cfg.get("collection_name", global_qdrant.get("collection", CONFIG.qdrant_collection)),
                "batch_size": source_cfg.get("batch_size", global_indexing.get("batch_size", 16)),
                "chunk_max_tokens": source_cfg.get("chunk_max_tokens", 600),
                "chunk_min_tokens": source_cfg.get("chunk_min_tokens", 350),
                "chunk_overlap_base": source_cfg.get("chunk_overlap_base", 100),
            }
            normalized_sources.append({
                "name": source_name,
                "source_type": source_type,
                "config": run_config,
                "reindex_mode": global_indexing.get("reindex_mode", "changed")
            })

    return normalized_sources


def _clear_qdrant_collection(collection_name: str) -> None:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é Qdrant."""
    try:
        from qdrant_client import QdrantClient
        from app.config.app_config import CONFIG

        client = QdrantClient(
            url=CONFIG.qdrant_url,
            api_key=CONFIG.qdrant_api_key or None
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è
        try:
            collection_info = client.get_collection(collection_name)
            logger.info(f"üìä –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å–æ–¥–µ—Ä–∂–∏—Ç {collection_info.points_count} —Ç–æ—á–µ–∫")
        except Exception:
            logger.info(f"üìä –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é")
            return

        # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ç–æ—á–∫–∏ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        from qdrant_client.models import Filter
        client.delete(
            collection_name=collection_name,
            points_selector=Filter()  # –ü—É—Å—Ç–æ–π —Ñ–∏–ª—å—Ç—Ä —É–¥–∞–ª—è–µ—Ç –≤—Å–µ —Ç–æ—á–∫–∏
        )

        logger.success(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω–∞")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}: {e}")
        raise


def create_docusaurus_dag(config: Dict[str, Any]) -> PipelineDAG:
    """–°–æ–∑–¥–∞–µ—Ç DAG –¥–ª—è Docusaurus –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    steps = [
        Parser(),
        DocusaurusNormalizer(site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru")),
        URLMapper(
            site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru"),
            site_docs_prefix=config.get("site_docs_prefix", "/docs")
        ),
        UnifiedChunkerStep(
            max_tokens=config.get("chunk_max_tokens", 600),
            min_tokens=config.get("chunk_min_tokens", 350),
            overlap_base=config.get("chunk_overlap_base", 100),
            oversize_block_policy=config.get("chunk_oversize_block_policy", "split"),
            oversize_block_limit=config.get("chunk_oversize_block_limit", 1200)
        ),
        Embedder(batch_size=config.get("batch_size", 16)),
        QdrantWriter(collection_name=config.get("collection_name", CONFIG.qdrant_collection))
    ]

    return PipelineDAG(steps)


def create_website_dag(config: Dict[str, Any]) -> PipelineDAG:
    """–°–æ–∑–¥–∞–µ—Ç DAG –¥–ª—è –≤–µ–±-—Å–∞–π—Ç–æ–≤."""
    steps = [
        Parser(),
        HtmlNormalizer(),
        ContentExtractor(),
        BaseNormalizer(),
        UnifiedChunkerStep(
            max_tokens=config.get("chunk_max_tokens", 600),
            min_tokens=config.get("chunk_min_tokens", 350),
            overlap_base=config.get("chunk_overlap_base", 100),
            oversize_block_policy=config.get("chunk_oversize_block_policy", "split"),
            oversize_block_limit=config.get("chunk_oversize_block_limit", 1200)
        ),
        Embedder(batch_size=config.get("batch_size", 16)),
        QdrantWriter(collection_name=config.get("collection_name", CONFIG.qdrant_collection))
    ]

    return PipelineDAG(steps)


def run_unified_indexing(
    source_type: str,
    config: Dict[str, Any],
    reindex_mode: str = "changed",
    clear_collection: bool = False
) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.

    Args:
        source_type: –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ ("docusaurus", "website")
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        reindex_mode: –†–µ–∂–∏–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ ("full", "changed")
        clear_collection: –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    """
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")

    # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
    if clear_collection:
        logger.warning("üóëÔ∏è –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π")
        _clear_qdrant_collection(config.get("collection_name", CONFIG.qdrant_collection))

    try:
        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if source_type == "docusaurus":
            metadata_mapper = _build_docusaurus_metadata_mapper(config.get("metadata"))
            adapter = DocusaurusAdapter(
                docs_root=config["docs_root"],
                site_base_url=config.get("site_base_url", "https://docs-chatcenter.edna.ru"),
                site_docs_prefix=config.get("site_docs_prefix", "/docs"),
                drop_prefix_all_levels=config.get("drop_prefix_all_levels", True),
                max_pages=config.get("max_pages"),
                top_level_meta=config.get("top_level_meta"),
                metadata_mapper=metadata_mapper,
            )
            dag = create_docusaurus_dag(config)

        elif source_type == "website":
            adapter = WebsiteAdapter(
                seed_urls=config["seed_urls"],
                base_url=config.get("base_url"),
                render_js=config.get("render_js", False),
                max_pages=config.get("max_pages")
            )
            dag = create_website_dag(config)

        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}")

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        writer = dag.steps[-1]  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ - QdrantWriter
        if isinstance(writer, QdrantWriter):
            logger.info("üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏...")
            writer.ensure_collection()

        # –ó–∞–ø—É—Å–∫–∞–µ–º DAG
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ DAG —Å {len(dag.steps)} —à–∞–≥–∞–º–∏:")
        for step in dag.steps:
            logger.info(f"  - {step.get_step_name()}")

        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞
        logger.info("üì• –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞...")
        documents = adapter.iter_documents()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ DAG
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ DAG...")
        stats = dag.run(documents)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        with get_state_manager() as state_manager:
            logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")

        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç QdrantWriter
        writer_stats = {}
        if isinstance(writer, QdrantWriter):
            writer_stats = writer.stats

        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.success(f"üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {source_type} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"  üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats.get('processed_docs', 0)}/{stats.get('total_docs', 0)}")
        logger.info(f"  ‚ùå –û—à–∏–±–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats.get('failed_docs', 0)}")
        logger.info(f"  üì¶ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {writer_stats.get('total_chunks', 'N/A')}")
        logger.info(f"  ‚úÖ –ß–∞–Ω–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {writer_stats.get('processed_chunks', 'N/A')}")
        logger.info(f"  ‚ùå –ß–∞–Ω–∫–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏: {writer_stats.get('failed_chunks', 'N/A')}")
        logger.info(f"  üî¢ –ë–∞—Ç—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {writer_stats.get('batches_processed', 'N/A')}")
        logger.info(f"  üéØ –ù—É–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {writer_stats.get('zero_dense_vectors', 'N/A')}")
        logger.info(f"  üíæ –ü–æ—Å–ª–µ–¥–Ω–∏–π upsert: {writer_stats.get('last_upsert_points', 'N/A')} —Ç–æ—á–µ–∫")
        logger.info(f"  ‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {stats.get('total_time', 0):.2f}s")

        return {
            "success": True,
            "source_type": source_type,
            "stats": stats,
            "message": f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {source_type} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"
        }

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {source_type}: {e}")
        return {
            "success": False,
            "source_type": source_type,
            "error": str(e),
            "message": f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {source_type} –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π"
        }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI."""
    parser = argparse.ArgumentParser(
        description="–ï–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"
    )

    parser.add_argument(
        "--source",
        choices=["docusaurus", "website"],
        required=False,
        help="–¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
    )

    parser.add_argument(
        "--config",
        help="–ü—É—Ç—å –∫ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)"
    )

    parser.add_argument(
        "--profile",
        help="–ò–º—è –ø—Ä–æ—Ñ–∏–ª—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (development/production/etc)"
    )

    parser.add_argument(
        "--docs-root",
        help="–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π (–¥–ª—è docusaurus)"
    )

    parser.add_argument(
        "--site-base-url",
        default="https://docs-chatcenter.edna.ru",
        help="–ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞"
    )

    parser.add_argument(
        "--site-docs-prefix",
        default="/docs",
        help="–ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ URL"
    )

    parser.add_argument(
        "--seed-urls",
        nargs="+",
        help="–ù–∞—á–∞–ª—å–Ω—ã–µ URL –¥–ª—è –æ–±—Ö–æ–¥–∞ (–¥–ª—è website)"
    )

    parser.add_argument(
        "--collection-name",
        default=CONFIG.qdrant_collection,
        help="–ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=16,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )

    parser.add_argument(
        "--chunk-max-tokens",
        type=int,
        default=CONFIG.chunk_max_tokens,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ"
    )

    parser.add_argument(
        "--chunk-min-tokens",
        type=int,
        default=CONFIG.chunk_min_tokens,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ"
    )

    parser.add_argument(
        "--chunk-overlap-base",
        type=int,
        default=100,
        help="–ë–∞–∑–æ–≤–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Ç–æ–∫–µ–Ω–∞—Ö"
    )

    parser.add_argument(
        "--reindex-mode",
        choices=["full", "changed"],
        default="changed",
        help="–†–µ–∂–∏–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"
    )

    parser.add_argument(
        "--clear-collection",
        action="store_true",
        help="–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π (—É–¥–∞–ª—è–µ—Ç –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ)"
    )

    parser.add_argument(
        "--render-js",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Playwright –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ JS (–¥–ª—è website)"
    )

    parser.add_argument(
        "--max-pages",
        type=int,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )

    args = parser.parse_args()

    if args.config:
        try:
            sources_to_run = load_sources_from_config(args.config, args.profile)
        except Exception as exc:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é {args.config}: {exc}")
            sys.exit(1)

        if not sources_to_run:
            logger.warning("–í –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ—Ç –≤–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            sys.exit(0)

        total_sources = len(sources_to_run)
        for idx, source in enumerate(sources_to_run, start=1):
            logger.info(
                "‚ñ∂Ô∏è –ò—Å—Ç–æ—á–Ω–∏–∫ {} ({}) [{}/{}]",
                source["name"],
                source["source_type"],
                idx,
                total_sources
            )
            result = run_unified_indexing(
                source["source_type"],
                source["config"],
                source.get("reindex_mode", args.reindex_mode),
                clear_collection=args.clear_collection and idx == 1
            )
            if not result["success"]:
                logger.error(
                    "‚ùå –ò—Å—Ç–æ—á–Ω–∏–∫ {} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –æ—à–∏–±–∫–æ–π: {}",
                    source["name"],
                    result.get("error", "unknown")
                )
                sys.exit(1)
        logger.success("üéâ –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
        sys.exit(0)

    if not args.source:
        parser.error("--source –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω --config")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = {
        "site_base_url": args.site_base_url,
        "site_docs_prefix": args.site_docs_prefix,
        "collection_name": args.collection_name,
        "batch_size": args.batch_size,
        "chunk_max_tokens": args.chunk_max_tokens,
        "chunk_min_tokens": args.chunk_min_tokens,
        "chunk_overlap_base": args.chunk_overlap_base,
        "reindex_mode": args.reindex_mode
    }

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if args.source == "docusaurus":
        if not args.docs_root:
            logger.error("–î–ª—è docusaurus –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è --docs-root")
            sys.exit(1)
        config["docs_root"] = args.docs_root
        if args.max_pages:
            config["max_pages"] = args.max_pages

    elif args.source == "website":
        if not args.seed_urls:
            logger.error("–î–ª—è website –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è --seed-urls")
            sys.exit(1)
        config["seed_urls"] = args.seed_urls
        config["render_js"] = args.render_js
        if args.max_pages:
            config["max_pages"] = args.max_pages

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    result = run_unified_indexing(args.source, config, args.reindex_mode, args.clear_collection)

    if result["success"]:
        logger.success("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {result['stats']}")
        sys.exit(0)
    else:
        logger.error(f"‚ùå –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π: {result['error']}")
        sys.exit(1)


if __name__ == "__main__":
    main()
