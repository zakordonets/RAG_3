"""
–ï–¥–∏–Ω—ã–π QdrantWriter –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
"""

import time
import uuid
from typing import List, Dict, Any, Optional
from loguru import logger
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, SparseVector, PayloadSchemaType, VectorParams, Distance, SparseVectorParams, UpdateCollection

from app.config import CONFIG
from app.services.core.embeddings import embed_batch_optimized
from ingestion.adapters.base import PipelineStep
from ingestion.chunking import text_hash


class QdrantWriter(PipelineStep):
    """
    –ï–¥–∏–Ω—ã–π –ø–∏—Å–∞—Ç–µ–ª—å –≤ Qdrant –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö.

    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —á–∞–Ω–∫–∏ –≤ –µ–¥–∏–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Ö –≤ Qdrant
    —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º dense –∏ sparse –≤–µ–∫—Ç–æ—Ä–æ–≤.
    """

    def __init__(self, collection_name: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç QdrantWriter.

        Args:
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ CONFIG)
        """
        self.collection_name = collection_name or CONFIG.qdrant_collection
        self.client = QdrantClient(
            url=CONFIG.qdrant_url,
            api_key=CONFIG.qdrant_api_key or None
        )
        self.batch_size = CONFIG.embedding_batch_size or 16
        self.stats = {
            "total_chunks": 0,
            "processed_chunks": 0,
            "failed_chunks": 0,
            "batches_processed": 0,
            "zero_dense_vectors": 0,  # –ü–æ–¥—Å—á–µ—Ç –Ω—É–ª–µ–≤—ã—Ö dense –≤–µ–∫—Ç–æ—Ä–æ–≤
            "last_upsert_points": 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª—å–Ω–æ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á
        }

    def process(self, data: Any) -> Any:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞–Ω–∫–∏ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Ö –≤ Qdrant.

        Args:
            data: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –∏–ª–∏ –æ–¥–∏–Ω–æ—á–Ω—ã–π —á–∞–Ω–∫

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if isinstance(data, list):
            chunks = data
        else:
            chunks = [data]

        if not chunks:
            return self.stats

        logger.info(f"QdrantWriter: –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(chunks)} —á–∞–Ω–∫–æ–≤")
        start_time = time.time()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –±–∞—Ç—á–∞–º–∏
        total_processed = 0
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            total_batches = (len(chunks) + self.batch_size - 1) // self.batch_size

            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á {batch_num}/{total_batches} ({len(batch)} —á–∞–Ω–∫–æ–≤)")

            try:
                processed_count = self._process_batch(batch)
                total_processed += processed_count
                self.stats["batches_processed"] += 1

                logger.info(f"‚úÖ –ë–∞—Ç—á {batch_num}/{total_batches} –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {processed_count} —á–∞–Ω–∫–æ–≤")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_num}: {e}")
                self.stats["failed_chunks"] += len(batch)
                continue

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        elapsed = time.time() - start_time
        self.stats["total_chunks"] += len(chunks)
        self.stats["processed_chunks"] += total_processed

        # –õ–æ–≥–∏—Ä—É–µ–º –¥–æ–ª—é –Ω—É–ª–µ–≤—ã—Ö dense –≤–µ–∫—Ç–æ—Ä–æ–≤
        zero_ratio = 0.0
        if self.stats["processed_chunks"] > 0:
            zero_ratio = self.stats["zero_dense_vectors"] / self.stats["processed_chunks"]
            if zero_ratio > 0.1:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 10% –Ω—É–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
                logger.warning(f"‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –Ω—É–ª–µ–≤—ã—Ö dense –≤–µ–∫—Ç–æ—Ä–æ–≤: {zero_ratio:.1%}")

        logger.success(f"üéâ QdrantWriter –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f}s")
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"  üìÑ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        logger.info(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed}")
        logger.info(f"  ‚ùå –û—à–∏–±–æ–∫: {len(chunks) - total_processed}")
        logger.info(f"  üî¢ –ë–∞—Ç—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['batches_processed']}")
        logger.info(f"  üéØ –ù—É–ª–µ–≤—ã—Ö dense –≤–µ–∫—Ç–æ—Ä–æ–≤: {self.stats['zero_dense_vectors']} ({zero_ratio:.1%})")
        logger.info(f"  üíæ –ü–æ—Å–ª–µ–¥–Ω–∏–π upsert: {self.stats['last_upsert_points']} —Ç–æ—á–µ–∫")

        return self.stats

    def get_step_name(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è —à–∞–≥–∞."""
        return "qdrant_writer"

    def _process_batch(self, chunks: List[Dict[str, Any]]) -> int:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –±–∞—Ç—á —á–∞–Ω–∫–æ–≤.

        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        if not chunks:
            return 0

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        texts = []
        for chunk in chunks:
            if isinstance(chunk, dict):
                text = chunk.get("text", "")
            else:
                text = str(chunk)
            texts.append(text)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        try:
            embedding_results = embed_batch_optimized(
                texts,
                max_length=CONFIG.embedding_max_length_doc,
                return_dense=True,
                return_sparse=CONFIG.use_sparse,
                context="document"
            )

            dense_vecs = embedding_results.get('dense_vecs', [[0.0] * 1024] * len(texts))
            sparse_results = embedding_results.get('lexical_weights', [{}] * len(texts))

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ dense –≤–µ–∫—Ç–æ—Ä–æ–≤
            expected_dim = CONFIG.embedding_dim  # 1024 –¥–ª—è BGE-M3
            if any(len(v) != expected_dim for v in dense_vecs):
                raise ValueError(f"dense dim mismatch: expected {expected_dim}, got {[len(v) for v in dense_vecs]}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            # Fallback –≤–µ–∫—Ç–æ—Ä—ã
            dense_vecs = [[0.0] * CONFIG.embedding_dim] * len(texts)
            sparse_results = [{}] * len(texts)
            self.stats["zero_dense_vectors"] += len(dense_vecs)
            logger.warning(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(dense_vecs)} –Ω—É–ª–µ–≤—ã—Ö dense –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ–æ–ª–±—ç–∫–∞")

        # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è Qdrant
        points = []
        for i, chunk in enumerate(chunks):
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                doc_id = chunk.get("payload", {}).get("doc_id", "unknown") if isinstance(chunk, dict) else "unknown"
                site_url = chunk.get("payload", {}).get("site_url", "unknown") if isinstance(chunk, dict) else "unknown"

                point = self._create_point(chunk, dense_vecs[i], sparse_results[i])
                points.append(point)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–∫–∏ –¥–ª—è —á–∞–Ω–∫–∞ {i} (doc_id={doc_id}, site_url={site_url}): {e}")
                continue

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ Qdrant —Å —Ä–µ—Ç—Ä–∞—è–º–∏
        if points:
            for attempt in range(3):
                try:
                    self.client.upsert(
                        collection_name=self.collection_name,
                        points=points,
                        wait=True
                    )
                    self.stats["last_upsert_points"] = len(points)
                    return len(points)
                except Exception as e:
                    logger.warning(f"upsert retry {attempt+1}/3: {e}")
                    if attempt < 2:  # –ù–µ –∂–¥–µ–º –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ø—ã—Ç–∫–∏
                        time.sleep(1.5 * (attempt + 1))
                    else:
                        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ Qdrant –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫: {e}")
                        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å {len(points)} —Ç–æ—á–µ–∫ –≤ –±–∞—Ç—á")
                        self.stats["last_upsert_points"] = 0

                        # Binary split –¥–ª—è –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏—è "–±–∏—Ç—ã—Ö" —Ç–æ—á–µ–∫
                        if len(chunks) > 1:
                            logger.info(f"–ü—Ä–æ–±—É–µ–º binary split: —Ä–∞–∑–±–∏–≤–∞–µ–º {len(chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ–ø–æ–ª–∞–º")
                            mid = len(chunks) // 2
                            first_half = self._process_batch(chunks[:mid])
                            second_half = self._process_batch(chunks[mid:])
                            return first_half + second_half

                        return 0

        return 0

    def _create_point(self, chunk: Dict[str, Any], dense_vec: List[float], sparse_data: Dict[str, Any]) -> PointStruct:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–æ—á–∫—É –¥–ª—è Qdrant –∏–∑ —á–∞–Ω–∫–∞.

        Args:
            chunk: –ß–∞–Ω–∫ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            dense_vec: –ü–ª–æ—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
            sparse_data: –†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

        Returns:
            PointStruct –¥–ª—è Qdrant
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —á–∞–Ω–∫–∞
        if isinstance(chunk, dict):
            text = chunk.get("text", "")
            payload = chunk.get("payload", {})
        else:
            text = str(chunk)
            payload = {}

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
        point_id = self._generate_point_id(chunk, text)

        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã —Å–æ–≥–ª–∞—Å–Ω–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ Qdrant
        vectors = {"dense": dense_vec}

        # –î–æ–±–∞–≤–ª—è–µ–º sparse –≤–µ–∫—Ç–æ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å (–≤ —Ç–æ–º –∂–µ –ø–æ–ª–µ vectors)
        if CONFIG.use_sparse and sparse_data:
            try:
                indices, values = self._convert_sparse_data(sparse_data)
                if indices:
                    # Sparse –≤–µ–∫—Ç–æ—Ä –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ —Ç–æ –∂–µ –ø–æ–ª–µ vectors —Å –∏–º–µ–Ω–µ–º "sparse"
                    vectors["sparse"] = SparseVector(indices=indices, values=values)
            except Exception as e:
                doc_id = payload.get("doc_id", "unknown")
                site_url = payload.get("site_url", "unknown")
                logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è sparse –≤–µ–∫—Ç–æ—Ä–∞ (doc_id={doc_id}, site_url={site_url}): {e}")

        # –°–æ–∑–¥–∞–µ–º payload
        qdrant_payload = self._create_payload(chunk, text, payload)

        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–æ—á–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        return PointStruct(
            id=point_id,
            vector=vectors,  # dense + sparse –≤ –æ–¥–Ω–æ–º –ø–æ–ª–µ vector (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ)
            payload=qdrant_payload
        )

    def _generate_point_id(self, chunk: Dict[str, Any], text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç ID —Ç–æ—á–∫–∏ –¥–ª—è Qdrant."""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º chunk_id –∏–∑ payload –µ—Å–ª–∏ –µ—Å—Ç—å
        if isinstance(chunk, dict) and "payload" in chunk:
            chunk_id = chunk["payload"].get("chunk_id")
            if chunk_id:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π chunk_id (–≤–∫–ª—é—á–∞—è chunk_index) –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                base_hash = text_hash(chunk_id)

                # –°–æ–∑–¥–∞–µ–º UUID –∏–∑ —Ö–µ—à–∞ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 32 —Å–∏–º–≤–æ–ª–∞)
                hex32 = base_hash.replace("-", "")[:32]
                # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 32 —Å–∏–º–≤–æ–ª–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if len(hex32) < 32:
                    hex32 = hex32.ljust(32, '0')
                return str(uuid.UUID(hex=hex32))

        # Fallback 1: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–ª—é—á doc_id#chunk_index
        if isinstance(chunk, dict) and "payload" in chunk:
            payload = chunk["payload"]
            doc_id = payload.get("doc_id")
            chunk_index = payload.get("chunk_index")
            if doc_id and chunk_index is not None:
                stable_key = f"{doc_id}#{chunk_index}"
                base_hash = text_hash(stable_key)
                hex32 = base_hash.replace("-", "")[:32]
                if len(hex32) < 32:
                    hex32 = hex32.ljust(32, '0')
                return str(uuid.UUID(hex=hex32))

        # Fallback 2: —Å–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏–π ID –∏–∑ —Ç–µ–∫—Å—Ç–∞
        raw_hash = text_hash(text)
        hex32 = raw_hash.replace("-", "")[:32]
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 32 —Å–∏–º–≤–æ–ª–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(hex32) < 32:
            hex32 = hex32.ljust(32, '0')
        return str(uuid.UUID(hex=hex32))

    def _convert_sparse_data(self, sparse_data: Dict[str, Any]) -> tuple[List[int], List[float]]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç sparse –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç Qdrant."""
        if not sparse_data:
            return [], []

        # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ª–æ–≤–∞—Ä—å lexical_weights
        if isinstance(sparse_data, dict):
            # –ë—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ {"indices": [...], "values": [...]}
            if "indices" in sparse_data and "values" in sparse_data:
                indices = sparse_data["indices"]
                values = sparse_data["values"]
                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω—É–ª–µ–≤—ã–µ –≤–µ—Å–∞
                valid_pairs = [(idx, val) for idx, val in zip(indices, values) if val != 0.0]
                if valid_pairs:
                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –≤–µ—Å–∞ (desc)
                    sorted_pairs = sorted(valid_pairs, key=lambda x: abs(x[1]), reverse=True)
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    max_indices = 1000
                    if len(sorted_pairs) > max_indices:
                        sorted_pairs = sorted_pairs[:max_indices]
                    return [idx for idx, _ in sorted_pairs], [val for _, val in sorted_pairs]
                return [], []

            # –û–±—ã—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç lexical_weights: {"token_id": weight}
            valid_items = []
            for k, v in sparse_data.items():
                try:
                    idx = int(k)
                    weight = float(v) if not hasattr(v, 'item') else float(v.item())
                    if weight != 0.0:  # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω—É–ª–µ–≤—ã–µ –≤–µ—Å–∞
                        valid_items.append((idx, weight))
                except ValueError:
                    logger.warning(f"sparse key is not int, got {k!r}; check embedder output")
                    continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –≤–µ—Å–∞ (desc)
            sorted_items = sorted(valid_items, key=lambda x: abs(x[1]), reverse=True)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            max_indices = 1000
            if len(sorted_items) > max_indices:
                sorted_items = sorted_items[:max_indices]

            indices = [idx for idx, _ in sorted_items]
            values = [weight for _, weight in sorted_items]

            return indices, values

        return [], []

    def _clean_numpy_types(self, value: Any) -> Any:
        """–û—á–∏—â–∞–µ—Ç numpy —Ç–∏–ø—ã –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ JSON."""
        try:
            import numpy as np
            if isinstance(value, np.ndarray):  # numpy array
                return value.tolist()
            elif isinstance(value, (np.float32, np.float64, np.int32, np.int64)):  # numpy scalar
                return value.item()
            elif isinstance(value, dict):
                return {k: self._clean_numpy_types(v) for k, v in value.items()}
            elif isinstance(value, (list, tuple)):
                return [self._clean_numpy_types(item) for item in value]
            else:
                return value
        except ImportError:
            # –ï—Å–ª–∏ numpy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return value

    def _create_payload(self, chunk: Dict[str, Any], text: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç payload –¥–ª—è Qdrant."""
        qdrant_payload = {}

        # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ –ø–æ–ª—è –∏–∑ payload
        for key, value in payload.items():
            if value is not None:
                qdrant_payload[key] = self._clean_numpy_types(value)

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        qdrant_payload["text"] = text
        qdrant_payload["indexed_at"] = time.time()
        qdrant_payload["indexed_via"] = "unified_pipeline"

        # –û—á–∏—â–∞–µ–º —Ç—è–∂–µ–ª—ã–µ –ø–æ–ª—è
        heavy_fields = ["content", "html", "raw", "raw_content", "dom"]
        for field in heavy_fields:
            qdrant_payload.pop(field, None)

        return qdrant_payload

    def create_payload_indexes(self) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è payload –ø–æ–ª–µ–π."""
        try:
            # –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å enum —Ç–∏–ø–∞–º–∏
            indexes = [
                {"field_name": "category", "field_schema": PayloadSchemaType.KEYWORD},
                {"field_name": "groups_path", "field_schema": PayloadSchemaType.KEYWORD},
                {"field_name": "title", "field_schema": PayloadSchemaType.TEXT},
                {"field_name": "source", "field_schema": PayloadSchemaType.KEYWORD},
                {"field_name": "content_type", "field_schema": PayloadSchemaType.KEYWORD},
                {"field_name": "site_url", "field_schema": PayloadSchemaType.KEYWORD},  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: site_url –≤–º–µ—Å—Ç–æ canonical_url
                {"field_name": "indexed_at", "field_schema": PayloadSchemaType.FLOAT},
                {"field_name": "doc_id", "field_schema": PayloadSchemaType.KEYWORD},  # –£–¥–æ–±–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å/–ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                {"field_name": "heading_path", "field_schema": PayloadSchemaType.KEYWORD},  # –î–ª—è —Ç–∞—Ä–≥–µ—Ç–Ω—ã—Ö –±—É—Å—Ç–æ–≤/—Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
                {"field_name": "page_type", "field_schema": PayloadSchemaType.KEYWORD},  # –î–ª—è –±—É—Å—Ç–æ–≤ –ø–æ —Ç–∏–ø—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            ]

            for index_config in indexes:
                try:
                    self.client.create_payload_index(
                        collection_name=self.collection_name,
                        **index_config
                    )
                    logger.info(f"–°–æ–∑–¥–∞–Ω –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–ª—è: {index_config['field_name']}")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è {index_config['field_name']}: {e}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤: {e}")

    def ensure_collection(self) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω—É–∂–Ω–æ–π —Å—Ö–µ–º–æ–π –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è
            info = self.client.get_collection(self.collection_name)
            logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º sparse –≤–µ–∫—Ç–æ—Ä—ã (–º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å –≤ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö Qdrant)
            try:
                # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ sparse –≤–µ–∫—Ç–æ—Ä–æ–≤
                has_sparse = False
                if hasattr(info.config, 'sparse_vectors_config'):
                    sparse_config = info.config.sparse_vectors_config
                    has_sparse = sparse_config is not None and len(sparse_config) > 0

                if CONFIG.use_sparse and not has_sparse:
                    logger.info("–î–æ–±–∞–≤–ª—è–µ–º sparse_vectors_config –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
                    self.client.update_collection(
                        collection_name=self.collection_name,
                        sparse_vectors_config={"sparse": SparseVectorParams()}
                    )
                    logger.success("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω sparse_vectors_config –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
                elif CONFIG.use_sparse and has_sparse:
                    logger.info("‚úÖ Sparse –≤–µ–∫—Ç–æ—Ä—ã —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ/–¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ sparse –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")

            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã –µ—Å—Ç—å
            self.create_payload_indexes()
            return

        except Exception as e:
            # –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ—ë
            if "not found" in str(e).lower() or "does not exist" in str(e).lower():
                logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é")
            else:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {self.collection_name}: {e}")
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é {self.collection_name} —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å—Ö–µ–º–æ–π")

            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω—É–ª—è
            logger.info(f"–°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é {self.collection_name} —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å—Ö–µ–º–æ–π")

            # –°—Ö–µ–º–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤: named dense + named sparse
            vectors_config = {
                "dense": VectorParams(
                    size=CONFIG.embedding_dim,  # 1024 –¥–ª—è BGE-M3
                    distance=Distance.COSINE,
                    hnsw_config={
                        "m": CONFIG.qdrant_hnsw_m,
                        "ef_construct": CONFIG.qdrant_hnsw_ef_construct,
                        "full_scan_threshold": CONFIG.qdrant_hnsw_full_scan_threshold,
                    }
                )
            }

            # Sparse –≤–µ–∫—Ç–æ—Ä—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            sparse_vectors_config = {"sparse": SparseVectorParams()} if CONFIG.use_sparse else None

            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=vectors_config,
                sparse_vectors_config=sparse_vectors_config
            )

            logger.success(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} —Å–æ–∑–¥–∞–Ω–∞ —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å—Ö–µ–º–æ–π")

            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã payload
            self.create_payload_indexes()

    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        return self.stats.copy()
