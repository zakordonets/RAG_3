# ADR-002: Adaptive Chunking Strategy

## Статус
✅ **Принято и реализовано**

## Метаданные
- **Дата решения**: 30 сентября 2024
- **Дата реализации**: Октябрь 2024
- **Версия**: v4.3.0+
- **Авторы**: RAG System Team

---

## Контекст и проблема

### Исходная ситуация

Система использовала **фиксированный размер чанков** (250 токенов для всех типов).

### Первоначальная гипотеза (сентябрь 2024)

Увеличение до 410-614 токенов (оптимум для BGE-M3 по документации) улучшит качество.

### Проблемы с большими чанками (350-600)

**Эксперимент показал**:
1. ❌ **Смешивание контекста**: Информация из разных разделов сливалась в один ответ
2. ❌ **Потеря точности**: LLM генерировал ответы на основе нерелевантных частей чанка
3. ❌ **Плохая релевантность**: Один чанк содержал несколько несвязанных тем

### Пересмотр решения (октябрь 2024)

**Возврат к меньшим чанкам 150-300 токенов показал**:
- ✅ **Лучшая точность**: Каждый чанк фокусируется на одной теме
- ✅ **Качество ответов**: LLM не путает информацию из разных разделов
- ✅ **Четкая релевантность**: Проще определить, подходит ли чанк для ответа

---

## Решение

### UniversalChunker с меньшими чанками для лучшей точности

**Модуль**: `ingestion/chunking/universal_chunker.py`

**Финальное решение** (после экспериментов): **150-300 токенов**

### Эволюция параметров

| Версия | min/max tokens | Результат | Причина изменения |
|--------|----------------|-----------|-------------------|
| v4.2.0 | 250 фикс. | Базовый | Начальная реализация |
| v4.3.0 | 350-600 | ❌ Смешивание контекста | Попытка оптимизации под BGE-M3 |
| **v4.3.1** | **150-300** | ✅ **Лучшее качество** | **Практический опыт** |

### Текущая реализация (Production)

```python
# Реальная конфигурация (ingestion/config.yaml)
UniversalChunker(
    max_tokens=300,              # Уменьшено для точности
    min_tokens=150,              # Уменьшено для гибкости
    overlap_base=50,             # Адаптивный overlap
    oversize_block_policy="split",
    oversize_block_limit=600     # Снижено пропорционально
)
```

### Почему 150-300, а не 410-614?

**Теория** (BGE-M3 docs): Оптимум 410-614 токенов для embedding quality

**Практика** (реальное использование):
1. **Фокус на одной теме** - меньше чанки = меньше шума
2. **Точность LLM** - модель не путает информацию из разных частей
3. **Релевантность retrieval** - проще определить, подходит ли чанк
4. **Качество ответов** - меньше "галлюцинаций" из нерелевантного контекста

**Вывод**: Для RAG-систем с длинными документами **качество ответа > качество embeddings**.

### Адаптация под структуру

Chunker **адаптируется через структурное разбиение**:

| Элемент | Обработка | Типичный размер |
|---------|-----------|-----------------|
| **Заголовок + параграф** | Атомарно | 150-250 токенов |
| **Code block** | Не разбивается | 100-400 токенов |
| **Список (list)** | Сохранение целостности | 100-300 токенов |
| **Таблица** | Атомарна или по строкам | 150-500 токенов |

### Конфигурация

**Рекомендуемая production конфигурация**:
```yaml
# ingestion/config.yaml
chunk:
  max_tokens: 300
  min_tokens: 150
  overlap_base: 50
  oversize_block_policy: "split"
  oversize_block_limit: 600
```

**Environment variables**:
```bash
CHUNK_MAX_TOKENS=300
CHUNK_MIN_TOKENS=150
CHUNK_OVERLAP_BASE=50
```

---

## Обоснование

### Почему 150-300 токенов?

**Ключевое открытие**: Меньшие чанки = лучшее качество ответов.

| Фактор | Объяснение |
|--------|------------|
| **LLM precision** | Короткие чанки содержат одну тему - LLM не путается |
| **Retrieval accuracy** | Проще определить релевантность короткого фрагмента |
| **Context isolation** | Информация из разных разделов не смешивается |
| **User experience** | Более точные и фокусированные ответы |

### Trade-offs

**Минусы меньших чанков**:
- ⚠️ Больше точек в Qdrant (увеличение storage)
- ⚠️ Потенциально хуже embedding quality (теоретически)
- ⚠️ Может потребоваться больше retrieval candidates

**Плюсы меньших чанков** (перевешивают):
- ✅ **Значительно лучше** качество финальных ответов
- ✅ Меньше "галлюцинаций" и смешивания контекста
- ✅ Более релевантные результаты поиска
- ✅ Лучший user experience

### Параметры

- **150-300 токенов**: Баланс между контекстом и точностью
- **50 overlap**: Адаптивный (меньше для меньших чанков)
- **600 limit**: Максимум для oversize блоков

---

## Альтернативы

### Рассмотренные и протестированные варианты

| Вариант | Параметры | Результат | Решение |
|---------|-----------|-----------|---------|
| **Фиксированный 250** | 250 токенов | Базовый уровень | ❌ Устарел |
| **Теоретический оптимум BGE-M3** | 410-614 токенов | ❌ Смешивание контекста | ❌ Отклонено |
| **Большие чанки** | 350-600 токенов | ❌ Информация сливается | ❌ Отклонено |
| **Средние чанки** | **150-300 токенов** | ✅ Лучшее качество ответов | ✅ **Принято** |
| **Очень мелкие** | <150 токенов | Потеря контекста | ❌ Отклонено |

### Lessons Learned

**Критический инсайт**:
> Теоретический оптимум для embedding model (410-614) НЕ означает оптимум для RAG system end-to-end.

**Почему большие чанки не сработали**:
- Один чанк охватывал несколько подразделов
- LLM не мог определить, какая часть чанка релевантна
- Генерировались ответы, смешивающие информацию из разных тем

**Почему 150-300 работает лучше**:
- Один чанк = одна конкретная тема/подраздел
- LLM получает чистый, фокусированный контекст
- Retrieval возвращает точно релевантные фрагменты

---

## Последствия

### Положительные

- ✅ **Качество ответов**: Значительное улучшение (subjective, но очевидное)
- ✅ **Точность**: Нет смешивания информации из разных разделов
- ✅ **Релевантность**: Более точные результаты retrieval
- ✅ **Структурное разбиение**: Сохранение логической целостности
- ✅ **Гибкость**: Настройка через config.yaml

### Негативные (приемлемые)

- ⚠️ **Больше точек в Qdrant**: ~2x от первоначального (150-300 vs 250-500)
- ⚠️ **Больше storage**: Пропорционально количеству чанков
- ⚠️ **Может требоваться больше k**: В retrieval для покрытия документа

### Mitigation

**Storage**:
- Qdrant эффективно сжимает данные
- Увеличение storage приемлемо для улучшения quality

**Retrieval**:
- Используем k=20-30 вместо k=10-15
- Auto-merge помогает объединять связанные чанки из одного документа

**Performance**:
- Chunking остается быстрым (<100ms на документ)
- Embedding generation в batch режиме

---

## Статус реализации

- ✅ UniversalChunker реализован (`ingestion/chunking/universal_chunker.py`)
- ✅ Интеграция в DAG pipeline (`ingestion/pipeline/chunker.py`)
- ✅ Конфигурация через YAML (`ingestion/config.yaml`)
- ✅ Testing coverage >80%
- ✅ Production deployment

**Результат**: Решение успешно внедрено, качество поиска улучшилось.

### Отклонения от первоначального плана

**Итерация 1** (сентябрь 2024):
- Планировалось: 350-600 токенов (оптимизация под BGE-M3)
- Разные стратегии для SHORT/MEDIUM/LONG документов
- Enum типы: DocumentType, ChunkingStrategy

**Итерация 2** (октябрь 2024 - реализация):
- Реализовано: Единый UniversalChunker
- Упрощено: Без enum, структурное разбиение вместо стратегий
- Параметры: 350-600 (defaults в коде)

**Итерация 3** (октябрь 2024 - production тестирование):
- ❌ Обнаружена проблема: смешивание контекста из разных разделов
- ❌ Качество ответов неудовлетворительное
- ✅ Решение: Уменьшение до **150-300 токенов**

**Финальная реализация** (v4.3.1):
- ✅ UniversalChunker с параметрами **150-300**
- ✅ Структурное разбиение (заголовки, параграфы, code blocks)
- ✅ Overlap 50 (адаптивный)
- ✅ Проблема решена - качество ответов отличное

### Текущие параметры в системе

**Production configuration**:

| Файл | min_tokens | max_tokens | Статус |
|------|------------|------------|--------|
| `ingestion/config.yaml` | **150** | **300** | ✅ Актуально (production) |
| `app/config/app_config.py` | 410 | 500 | ⚠️ Устарело (требует обновления) |

**Действие**: Обновить `app_config.py` на 150-300 для консистентности.

### Важный вывод

**Теория vs Практика**:
- 📚 Теория: BGE-M3 оптимален при 410-614 токенов
- 🔬 Практика: RAG quality лучше при 150-300 токенов
- 🎯 **Вывод**: Оптимизация end-to-end системы ≠ оптимизация отдельного компонента

**Этот опыт критически важен** для понимания, что embedding quality - это не единственный фактор в RAG pipeline.

---

## Метрики успеха

### Целевые метрики

| Метрика | Целевое значение | Статус |
|---------|------------------|--------|
| **Chunk size distribution** | >70% в диапазоне 150-300 токенов | ✅ По дизайну |
| **Качество ответов** | Субъективное улучшение | ✅ Достигнуто |
| **Chunking errors** | <1% ошибок | ✅ Стабильно |
| **Processing time** | <100ms на документ | ✅ Достигнуто |

### Фактический результат (v4.3.1)

**Качественные улучшения**:
- ✅ Нет смешивания информации из разных разделов
- ✅ LLM генерирует точные, фокусированные ответы
- ✅ Пользователи получают релевантную информацию
- ✅ Снижение "галлюцинаций" в ответах

**Техническая реализация**:
- ✅ UniversalChunker с параметрами **150-300**
- ✅ Структурное разбиение сохраняет целостность
- ✅ Стабильная работа в production
- ✅ Конфигурируемость через YAML/ENV

### Аналитика

```bash
# Анализ распределения размеров чанков
python scripts/deep_analysis.py

# Проверка конкретного документа
python scripts/check_file_indexed.py --url "https://docs..."
```

---

## Связанные документы

- [Indexing & Data Structure](indexing_and_data_structure.md#система-chunking) - Детали реализации
- [Technical Specification](technical_specification.md) - Параметры chunking
- ADR-001: BGE-M3 Unified Embeddings

---

**Последнее обновление**: 9 октября 2024
