# –ü–ª–∞–Ω —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ —Å–∏—Å—Ç–µ–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö

## üéØ –¶–µ–ª–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞

1. **–ò—Å–ø—Ä–∞–≤–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É** —Å –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
2. **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** –∑–∞ —Å—á–µ—Ç —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
3. **–£–ø—Ä–æ—Å—Ç–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É** –ø—É—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
4. **–ü–æ–≤—ã—Å–∏—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å** —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é

## üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã

### 1. –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞–Ω–Ω—ã—Ö
- `parse_faq_content()` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç **—Å–ø–∏—Å–æ–∫** –≤–º–µ—Å—Ç–æ —Å–ª–æ–≤–∞—Ä—è
- `parse_guides()` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç **—Å–ª–æ–≤–∞—Ä—å**
- Pipeline –æ–∂–∏–¥–∞–µ—Ç –ø–æ–ª–µ `content`, EdnaDocsDataSource –æ–∂–∏–¥–∞–µ—Ç `text`
- –†–µ–∑—É–ª—å—Ç–∞—Ç: –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è FAQ —Å—Ç—Ä–∞–Ω–∏—Ü, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –≤ RAG-pipeline

### 2. –ò–∑–±—ã—Ç–æ—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –¢—Ä–∏ —É—Ä–æ–≤–Ω—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏: `crawler.py` ‚Üí `universal_loader.py` ‚Üí `parsers.py`
- –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∫–∞–∂–¥–æ–º –ø–∞—Ä—Å–µ—Ä–µ
- –î–≤–æ–π–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ HTML (BeautifulSoup –¥–≤–∞–∂–¥—ã)

### 3. –ù–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è BeautifulSoup –æ–±—ä–µ–∫—Ç–æ–≤
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –≤–º–µ—Å—Ç–æ –±–∞—Ç—á–µ–≤–æ–π
- –ò–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

## üìã –ü–ª–∞–Ω —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞

### –≠—Ç–∞–ø 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ (0.5 –¥–Ω—è)

#### 0.1 –°–æ–∑–¥–∞—Ç—å —Å–∫—Ä–∏–ø—Ç –æ—á–∏—Å—Ç–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
```python
# scripts/clear_collection.py
from app.services.retrieval import client, COLLECTION
from loguru import logger

def clear_collection():
    """–û—á–∏—â–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é Qdrant –∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é —Å —á–∏—Å—Ç–æ–π —Å—Ö–µ–º–æ–π"""
    try:
        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
        client.delete_collection(COLLECTION)
        logger.info(f"Collection {COLLECTION} deleted")

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω–æ–≤–æ–π —Å—Ö–µ–º–æ–π
        from scripts.init_qdrant import create_collection
        create_collection()
        logger.info(f"Collection {COLLECTION} recreated with new schema")

    except Exception as e:
        logger.error(f"Failed to clear collection: {e}")
        raise

def backup_existing_data():
    """–°–æ–∑–¥–∞–µ—Ç –±—ç–∫–∞–ø —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)"""
    try:
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ JSON
        scroll_result = client.scroll(
            collection_name=COLLECTION,
            limit=10000,
            with_payload=True,
            with_vectors=False
        )

        import json
        from datetime import datetime

        backup_file = f"backup_collection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(scroll_result[0], f, ensure_ascii=False, indent=2)

        logger.info(f"Backup saved to {backup_file}")

    except Exception as e:
        logger.warning(f"Failed to create backup: {e}")

if __name__ == "__main__":
    backup_existing_data()
    clear_collection()
```

#### 0.2 –û–±–Ω–æ–≤–∏—Ç—å —Å—Ö–µ–º—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏
```python
# scripts/init_qdrant.py - –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞
def create_collection():
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω–æ–≤–æ–π —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π —Å—Ö–µ–º–æ–π"""

    # –ù–æ–≤–∞—è —Å—Ö–µ–º–∞ –±–µ–∑ legacy –ø–æ–ª–µ–π
    vectors_config = {
        "dense": VectorParams(
            size=1024,  # BGE-M3 —Ä–∞–∑–º–µ—Ä
            distance=Distance.COSINE
        ),
        "sparse": VectorParams(
            size=768,   # SPLADE —Ä–∞–∑–º–µ—Ä
            distance=Distance.DOT
        )
    }

    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ payload
    payload_schema = {
        "url": "text",
        "title": "text",
        "content": "text",  # –í—Å–µ–≥–¥–∞ 'content', –Ω–µ 'text'
        "page_type": "keyword",
        "source": "keyword",
        "language": "keyword",
        "chunk_index": "integer",
        "content_length": "integer",
        "indexed_at": "float",
        "indexed_via": "keyword"
    }

    client.create_collection(
        collection_name=COLLECTION,
        vectors_config=vectors_config,
        payload_schema=payload_schema
    )
```

### –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (1-2 –¥–Ω—è)

#### 1.1 –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º PluginManager
**–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï**: –ü–ª–∞–Ω –¥–æ–ª–∂–µ–Ω –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º `PluginManager`, –∞ –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É.

```python
# app/config/sources.py - –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è
from app.abstractions.data_source import plugin_manager, DataSourceBase
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

class SourceType(Enum):
    """–¢–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    DOCS_SITE = "docs_site"
    API_DOCS = "api_docs"
    BLOG = "blog"
    FAQ = "faq"
    EXTERNAL = "external"

@dataclass
class SourceConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö - –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø —Å PluginManager"""
    name: str
    base_url: str
    source_type: SourceType
    strategy: str = "auto"
    use_cache: bool = True
    max_pages: int = None
    crawl_deny_prefixes: List[str] = None
    sitemap_path: str = "/sitemap.xml"
    seed_urls: List[str] = None
    metadata_patterns: Dict[str, Dict[str, str]] = None

class SourcesRegistry:
    """–†–µ–µ—Å—Ç—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö - –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ô —Å PluginManager"""

    def __init__(self):
        self._sources: Dict[str, SourceConfig] = {}
        self._load_default_sources()

    def _load_default_sources(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        self.register(SourceConfig(
            name="edna_docs",
            base_url="https://docs-chatcenter.edna.ru/",
            source_type=SourceType.DOCS_SITE,
            strategy="jina",
            use_cache=True,
            sitemap_path="/sitemap.xml",
            seed_urls=[
                "https://docs-chatcenter.edna.ru/",
                "https://docs-chatcenter.edna.ru/docs/start/",
                "https://docs-chatcenter.edna.ru/docs/agent/",
                "https://docs-chatcenter.edna.ru/docs/supervisor/",
                "https://docs-chatcenter.edna.ru/docs/admin/",
                "https://docs-chatcenter.edna.ru/docs/chat-bot/",
                "https://docs-chatcenter.edna.ru/docs/api/index/",
                "https://docs-chatcenter.edna.ru/docs/faq/",
                "https://docs-chatcenter.edna.ru/blog/",
            ],
            crawl_deny_prefixes=[
                "https://docs-chatcenter.edna.ru/api/",
                "https://docs-chatcenter.edna.ru/admin/",
                "https://docs-chatcenter.edna.ru/supervisor/",
            ],
            metadata_patterns={
                r'/docs/start/': {'section': 'start', 'user_role': 'all', 'page_type': 'guide'},
                r'/docs/agent/': {'section': 'agent', 'user_role': 'agent', 'page_type': 'guide'},
                r'/docs/supervisor/': {'section': 'supervisor', 'user_role': 'supervisor', 'page_type': 'guide'},
                r'/docs/admin/': {'section': 'admin', 'user_role': 'admin', 'page_type': 'guide'},
                r'/docs/api/': {'section': 'api', 'user_role': 'integrator', 'page_type': 'api'},
                r'/blog/': {'section': 'changelog', 'user_role': 'all', 'page_type': 'release_notes'},
                r'/faq': {'section': 'faq', 'user_role': 'all', 'page_type': 'faq'},
            }
        ))

    def register(self, config: SourceConfig):
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Ä–µ–µ—Å—Ç—Ä–µ"""
        self._sources[config.name] = config

    def get(self, name: str) -> SourceConfig:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ –∏–º–µ–Ω–∏"""
        if name not in self._sources:
            raise ValueError(f"Source '{name}' not found. Available: {list(self._sources.keys())}")
        return self._sources[name]

    def get_all_urls(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ URL –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        urls = []
        for source in self._sources.values():
            if source.seed_urls:
                urls.extend(source.seed_urls)
        return urls

    def get_source_config_for_plugin(self, name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è PluginManager"""
        config = self.get(name)
        return {
            "base_url": config.base_url,
            "strategy": config.strategy,
            "use_cache": config.use_cache,
            "max_pages": config.max_pages,
            "crawl_deny_prefixes": config.crawl_deny_prefixes,
            "sitemap_path": config.sitemap_path,
            "seed_urls": config.seed_urls,
            "metadata_patterns": config.metadata_patterns
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä
sources_registry = SourcesRegistry()

# –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ï —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def get_source_config(name: str) -> SourceConfig:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    return sources_registry.get(name)

def get_plugin_config(name: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è PluginManager"""
    return sources_registry.get_source_config_for_plugin(name)

def list_available_sources() -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    return list(sources_registry._sources.keys())
```

#### 1.2 –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å –ò–ù–¢–ï–ì–†–ê–¶–ò–ï–ô page_type
**–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï**: –ù–æ–≤—ã–µ –ø–∞—Ä—Å–µ—Ä—ã –¥–æ–ª–∂–Ω—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è page_type.
```python
# app/config/sources.py
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

class SourceType(Enum):
    """–¢–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    DOCS_SITE = "docs_site"
    API_DOCS = "api_docs"
    BLOG = "blog"
    FAQ = "faq"
    EXTERNAL = "external"

@dataclass
class SourceConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    name: str
    base_url: str
    source_type: SourceType
    strategy: str = "auto"
    use_cache: bool = True
    max_pages: int = None
    crawl_deny_prefixes: List[str] = None
    sitemap_path: str = "/sitemap.xml"
    seed_urls: List[str] = None
    metadata_patterns: Dict[str, Dict[str, str]] = None

class SourcesRegistry:
    """–†–µ–µ—Å—Ç—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö - —É–±–∏—Ä–∞–µ—Ç —Ö–∞—Ä–¥–∫–æ–¥ URL-–æ–≤"""

    def __init__(self):
        self._sources: Dict[str, SourceConfig] = {}
        self._load_default_sources()

    def _load_default_sources(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        self.register(SourceConfig(
            name="edna_docs",
            base_url="https://docs-chatcenter.edna.ru/",
            source_type=SourceType.DOCS_SITE,
            strategy="jina",
            use_cache=True,
            sitemap_path="/sitemap.xml",
            seed_urls=[
                "https://docs-chatcenter.edna.ru/",
                "https://docs-chatcenter.edna.ru/docs/start/",
                "https://docs-chatcenter.edna.ru/docs/agent/",
                "https://docs-chatcenter.edna.ru/docs/supervisor/",
                "https://docs-chatcenter.edna.ru/docs/admin/",
                "https://docs-chatcenter.edna.ru/docs/chat-bot/",
                "https://docs-chatcenter.edna.ru/docs/api/index/",
                "https://docs-chatcenter.edna.ru/docs/faq/",
                "https://docs-chatcenter.edna.ru/blog/",
            ],
            crawl_deny_prefixes=[
                "https://docs-chatcenter.edna.ru/api/",  # API endpoints
                "https://docs-chatcenter.edna.ru/admin/",  # –ê–¥–º–∏–Ω –ø–∞–Ω–µ–ª—å
                "https://docs-chatcenter.edna.ru/supervisor/",  # –ü–∞–Ω–µ–ª—å —Å—É–ø–µ—Ä–≤–∞–π–∑–µ—Ä–∞
            ],
            metadata_patterns={
                r'/docs/start/': {'section': 'start', 'user_role': 'all', 'page_type': 'guide'},
                r'/docs/agent/': {'section': 'agent', 'user_role': 'agent', 'page_type': 'guide'},
                r'/docs/supervisor/': {'section': 'supervisor', 'user_role': 'supervisor', 'page_type': 'guide'},
                r'/docs/admin/': {'section': 'admin', 'user_role': 'admin', 'page_type': 'guide'},
                r'/docs/api/': {'section': 'api', 'user_role': 'integrator', 'page_type': 'api'},
                r'/blog/': {'section': 'changelog', 'user_role': 'all', 'page_type': 'release_notes'},
                r'/faq': {'section': 'faq', 'user_role': 'all', 'page_type': 'faq'},
            }
        ))

    def get(self, name: str) -> SourceConfig:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ –∏–º–µ–Ω–∏"""
        if name not in self._sources:
            raise ValueError(f"Source '{name}' not found. Available: {list(self._sources.keys())}")
        return self._sources[name]

    def get_all_urls(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ URL –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        urls = []
        for source in self._sources.values():
            if source.seed_urls:
                urls.extend(source.seed_urls)
        return urls

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä
sources_registry = SourcesRegistry()
```

#### 1.2 –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã
```python
# ingestion/processors/base.py
from abc import ABC, abstractmethod
from typing import Dict, Any, List
from dataclasses import dataclass

@dataclass
class ProcessedPage:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–µ–∑ legacy —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
    url: str
    title: str
    content: str  # –í—Å–µ–≥–¥–∞ 'content'
    page_type: str
    metadata: Dict[str, Any]

    def __post_init__(self):
        """–ü—Ä–æ—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        if not self.content or len(self.content.strip()) < 10:
            raise ValueError(f"Content too short for {self.url}")
        if not self.title:
            self.title = self._extract_title_from_url()

    def _extract_title_from_url(self) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–∑ URL"""
        import re
        path = self.url.split('/')[-1]
        return re.sub(r'[_-]', ' ', path).title()

class BaseParser(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤"""

    @abstractmethod
    def parse(self, url: str, content: str) -> ProcessedPage:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        pass

    def _validate_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if not isinstance(result, dict):
            raise ValueError(f"Parser must return dict, got {type(result)}")

        required_fields = ['title', 'content']
        for field in required_fields:
            if field not in result:
                raise ValueError(f"Missing required field: {field}")
            if not isinstance(result[field], str):
                raise ValueError(f"Field '{field}' must be string, got {type(result[field])}")

        return result

class ContentProcessor:
    """–ï–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""

    def __init__(self):
        self.parsers = {
            'jina_reader': JinaParser(),
            'html': HTMLParser(),
            'markdown': MarkdownParser(),
        }
        self._soup_cache = {}  # –ö–µ—à BeautifulSoup –æ–±—ä–µ–∫—Ç–æ–≤

    def process(self, url: str, content: str, strategy: str = 'auto') -> ProcessedPage:
        """–ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            content_type = self._detect_content_type(content, strategy)
            parser = self.parsers[content_type]
            return parser.parse(url, content)
        except Exception as e:
            logger.error(f"Error processing {url}: {e}")
            return self._create_error_page(url, str(e))

    def _detect_content_type(self, content: str, strategy: str) -> str:
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if strategy == 'jina' or content.startswith("Title:") and "URL Source:" in content:
            return 'jina_reader'
        elif content.startswith("<!DOCTYPE html") or content.startswith("<html"):
            return 'html'
        elif content.startswith("#"):
            return 'markdown'
        return 'html'  # Fallback

    def _create_error_page(self, url: str, error: str) -> ProcessedPage:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ—à–∏–±–∫–æ–π"""
        return ProcessedPage(
            url=url,
            title="Error",
            content=f"Error processing page: {error}",
            page_type="error",
            metadata={"error": error}
        )
```

#### 1.3 –ö–∞–∫ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–≥–∞–π–¥)
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç–µ–ø–µ—Ä—å –¥–µ–ª–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä `app/config/sources.py`.

–®–∞–≥–∏:
1) –û–ø–∏—Å–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ —á–µ—Ä–µ–∑ `SourceConfig` –∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –≤ `SourcesRegistry`.
2) –£–∫–∞–∑–∞—Ç—å `base_url`, —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∑–∞–≥—Ä—É–∑–∫–∏ (`strategy`), `seed_urls`/`sitemap_path`, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ `crawl_deny_prefixes` –∏ `metadata_patterns`.
3) –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º `source_name`.

–ü—Ä–∏–º–µ—Ä ‚Äî –¥–æ–±–∞–≤–∏—Ç—å –≤–Ω–µ—à–Ω–∏–π —Å–∞–π—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
```python
# app/config/sources.py
from app.config.sources import SourcesRegistry, SourceConfig, SourceType

def _load_default_sources(self):
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ...

    self.register(SourceConfig(
        name="partner_docs",
        base_url="https://docs.partner.example/",
        source_type=SourceType.DOCS_SITE,
        strategy="html",           # –∏–ª–∏ "jina"/"auto"
        use_cache=True,
        sitemap_path="/sitemap.xml",
        seed_urls=[
            "https://docs.partner.example/",
            "https://docs.partner.example/getting-started/",
        ],
        crawl_deny_prefixes=[
            "https://docs.partner.example/admin/",
        ],
        metadata_patterns={
            r"/getting-started/": {"section": "start", "user_role": "all", "page_type": "guide"},
            r"/api/": {"section": "api", "user_role": "integrator", "page_type": "api"},
        }
    ))
```

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ (–±–µ–∑ —Ö–∞—Ä–¥–∫–æ–¥–∞ URL):
```python
# ingestion/pipeline.py
result = crawl_and_index(
    source_name="partner_docs",   # –∏–º—è –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞
    incremental=True,
    max_pages=50
)
```

–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ —Ç–µ—Å—Ç–∞—Ö:
```python
from app.config.sources import get_source_config

def test_partner_docs_source_config():
    cfg = get_source_config("partner_docs")
    assert cfg.base_url.startswith("https://docs.partner.example/")
    assert cfg.strategy in ("html", "jina", "auto")
    assert len(cfg.seed_urls) > 0 or cfg.sitemap_path
```

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
- –î–ª—è —Å–∞–π—Ç–æ–≤ —Å —Ç—è–∂–µ–ª—ã–º HTML –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `strategy="jina"` (–±—ã—Å—Ç—Ä–µ–µ –∏ —É—Å—Ç–æ–π—á–∏–≤–µ–µ).
- –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ `sitemap.xml` –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –Ω–µ–≥–æ, `seed_urls` –æ—Å—Ç–∞–≤–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏.
- –í—Å–µ–≥–¥–∞ –∑–∞–¥–∞–≤–∞—Ç—å `crawl_deny_prefixes` –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∞–¥–º–∏–Ω–æ–∫/–Ω–µ—Ü–µ–ª–µ–≤—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤.

–ü—Ä–∏–º–µ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:

1) –í–Ω–µ—à–Ω–∏–π –±–ª–æ–≥ (RSS/Sitemap + HTML):
```python
# app/config/sources.py
self.register(SourceConfig(
    name="company_blog",
    base_url="https://blog.company.example/",
    source_type=SourceType.BLOG,
    strategy="html",              # —Ä–µ–Ω–¥–µ—Ä HTML, –∏–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    use_cache=True,
    sitemap_path="/sitemap.xml",  # –µ—Å–ª–∏ –µ—Å—Ç—å
    seed_urls=[
        "https://blog.company.example/",
    ],
    crawl_deny_prefixes=[
        "https://blog.company.example/admin/",
        "https://blog.company.example/login/",
    ],
    metadata_patterns={
        r"/tags/": {"section": "tags", "page_type": "taxonomy"},
        r"/category/": {"section": "category", "page_type": "taxonomy"},
    }
))

# ingestion/pipeline.py
result = crawl_and_index(source_name="company_blog", incremental=True, max_pages=100)

# tests
from app.config.sources import get_source_config
def test_company_blog_source_config():
    cfg = get_source_config("company_blog")
    assert cfg.base_url.startswith("https://blog.company.example/")
    assert cfg.strategy == "html"
    assert cfg.sitemap_path or len(cfg.seed_urls) > 0
```

2) API‚Äë–ø–æ—Ä—Ç–∞–ª (–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è, –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ Jina):
```python
# app/config/sources.py
self.register(SourceConfig(
    name="api_portal",
    base_url="https://api.company.example/docs/",
    source_type=SourceType.API_DOCS,
    strategy="jina",              # –±—ã—Å—Ç—Ä–µ–µ –∏ —É—Å—Ç–æ–π—á–∏–≤–µ–µ –ø—Ä–æ—Ç–∏–≤ –∞–Ω—Ç–∏–±–æ—Ç–∞
    use_cache=True,
    sitemap_path="/sitemap.xml",
    seed_urls=[
        "https://api.company.example/docs/",
        "https://api.company.example/docs/reference/",
    ],
    crawl_deny_prefixes=[
        "https://api.company.example/swagger.json",  # –±–∏–Ω–∞—Ä–Ω—ã–µ/JSON —Å—Ö–µ–º—ã –Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
    ],
    metadata_patterns={
        r"/reference/": {"section": "api", "user_role": "integrator", "page_type": "api"},
        r"/guides/": {"section": "guides", "user_role": "all", "page_type": "guide"},
    }
))

# ingestion/pipeline.py
result = crawl_and_index(source_name="api_portal", incremental=False, max_pages=200)

# tests
from app.config.sources import get_source_config
def test_api_portal_source_config():
    cfg = get_source_config("api_portal")
    assert cfg.base_url.endswith("/docs/")
    assert cfg.strategy in ("jina", "auto")
    assert any("/reference/" in u for u in (cfg.seed_urls or [])) or cfg.sitemap_path
```


#### 1.2 –°–æ–∑–¥–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
```python
# ingestion/processors/jina_parser.py
class JinaParser(BaseParser):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Jina Reader"""

    def parse(self, url: str, content: str) -> ProcessedPage:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç Jina Reader"""
        if not content:
            raise ValueError("Empty content")

        lines = content.split('\n')
        title = self._extract_title(lines)
        content_text = self._extract_content(lines)
        metadata = self._extract_metadata(lines)

        result = {
            'title': title,
            'content': content_text,
            **metadata
        }

        validated = self._validate_result(result)

        return ProcessedPage(
            url=url,
            title=validated['title'],
            content=validated['content'],
            page_type=self._detect_page_type(url),
            metadata=validated
        )

    def _extract_title(self, lines: List[str]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        for line in lines:
            if line.startswith("Title:"):
                title_part = line.split("Title:")[1].strip()
                if "|" in title_part:
                    return title_part.split("|")[0].strip()
                return title_part
        return ""

    def _extract_content(self, lines: List[str]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content_started = False
        content_lines = []

        for line in lines:
            if line.startswith("Markdown Content:"):
                content_started = True
                continue

            if content_started:
                if line.startswith(("Title:", "URL Source:", "Published Time:")):
                    break
                content_lines.append(line)

        return '\n'.join(content_lines).strip()

    def _extract_metadata(self, lines: List[str]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        metadata = {}

        for line in lines[:20]:  # –ü–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫
            line = line.strip()
            if line.startswith("URL Source:"):
                metadata['url_source'] = line[11:].strip()
            elif line.startswith("Content Length:"):
                try:
                    metadata['content_length'] = int(line[15:].strip())
                except ValueError:
                    pass
            elif line.startswith("Language Detected:"):
                metadata['language_detected'] = line[18:].strip()
            # ... –¥—Ä—É–≥–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

        return metadata

# ingestion/processors/html_parser.py
class HTMLParser(BaseParser):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä HTML —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""

    def __init__(self):
        self._soup_cache = {}

    def parse(self, url: str, content: str) -> ProcessedPage:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        soup = self._get_soup(content)

        title = self._extract_title(soup)
        content_text = self._extract_content(soup)
        metadata = self._extract_metadata(soup, url)

        result = {
            'title': title,
            'content': content_text,
            **metadata
        }

        validated = self._validate_result(result)

        return ProcessedPage(
            url=url,
            title=validated['title'],
            content=validated['content'],
            page_type=self._detect_page_type(url),
            metadata=validated
        )

    def _get_soup(self, content: str) -> BeautifulSoup:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ BeautifulSoup —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        if content_hash not in self._soup_cache:
            self._soup_cache[content_hash] = BeautifulSoup(content, "lxml")
        return self._soup_cache[content_hash]

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        # Docusaurus –∑–∞–≥–æ–ª–æ–≤–æ–∫
        h1 = soup.select_one('.theme-doc-markdown h1')
        if h1:
            return h1.get_text(' ', strip=True)

        # –û–±—ã—á–Ω—ã–π title
        if soup.title:
            return soup.title.get_text(strip=True)

        return ""

    def _extract_content(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # Docusaurus –∫–æ–Ω—Ç–µ–Ω—Ç
        main = soup.select_one('.theme-doc-markdown')
        if main:
            return self._extract_text_from_element(main)

        # Fallback –∫ body
        return self._extract_text_from_element(soup)

    def _extract_text_from_element(self, element) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        parts = []

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ h1
        h1 = element.find('h1')
        if h1:
            parts.append(h1.get_text(' ', strip=True))

        # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏ —Å–ø–∏—Å–∫–∏
        for node in element.find_all(['p', 'li']):
            txt = node.get_text(' ', strip=True)
            if txt:
                parts.append(txt)

        return "\n\n".join(parts)

# ingestion/processors/markdown_parser.py
class MarkdownParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä Markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""

    def parse(self, url: str, content: str) -> ProcessedPage:
        """–ü–∞—Ä—Å–∏–Ω–≥ Markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        lines = content.split('\n')
        title = self._extract_title(lines)
        content_text = self._clean_markdown(content)

        result = {
            'title': title,
            'content': content_text
        }

        validated = self._validate_result(result)

        return ProcessedPage(
            url=url,
            title=validated['title'],
            content=validated['content'],
            page_type=self._detect_page_type(url),
            metadata=validated
        )

    def _extract_title(self, lines: List[str]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–∑ Markdown"""
        for line in lines:
            if line.startswith('# '):
                return line[2:].strip()
        return ""

    def _clean_markdown(self, content: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ Markdown –æ—Ç —Ä–∞–∑–º–µ—Ç–∫–∏"""
        import re

        # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        content = re.sub(r'^#{1,6}\s+', '', content, flags=re.MULTILINE)

        # –£–±–∏—Ä–∞–µ–º –∂–∏—Ä–Ω—ã–π –∏ –∫—É—Ä—Å–∏–≤
        content = re.sub(r'\*\*(.*?)\*\*', r'\1', content)
        content = re.sub(r'\*(.*?)\*', r'\1', content)

        # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)

        return content.strip()
```

#### 1.3 –°–æ–∑–¥–∞—Ç—å —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
```python
# ingestion/processors/metadata_extractor.py
class MetadataExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ URL –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""

    @staticmethod
    def extract_url_metadata(url: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ URL"""
        metadata = {}

        patterns = {
            r'/docs/start/': {'section': 'start', 'user_role': 'all', 'page_type': 'guide'},
            r'/docs/agent/': {'section': 'agent', 'user_role': 'agent', 'page_type': 'guide'},
            r'/docs/supervisor/': {'section': 'supervisor', 'user_role': 'supervisor', 'page_type': 'guide'},
            r'/docs/admin/': {'section': 'admin', 'user_role': 'admin', 'page_type': 'guide'},
            r'/docs/api/': {'section': 'api', 'user_role': 'integrator', 'page_type': 'api'},
            r'/blog/': {'section': 'changelog', 'user_role': 'all', 'page_type': 'release_notes'},
            r'/faq': {'section': 'faq', 'user_role': 'all', 'page_type': 'faq'},
        }

        for pattern, meta in patterns.items():
            if re.search(pattern, url):
                metadata.update(meta)
                break

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
        if 'admin' in url:
            metadata['permissions'] = 'ADMIN'
        elif 'supervisor' in url:
            metadata['permissions'] = 'SUPERVISOR'
        elif 'agent' in url:
            metadata['permissions'] = 'AGENT'
        else:
            metadata['permissions'] = 'ALL'

        return metadata

    @staticmethod
    def detect_page_type(url: str, content: str = None) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        url_lower = url.lower()

        if 'faq' in url_lower:
            return 'faq'
        elif 'api' in url_lower:
            return 'api'
        elif any(keyword in url_lower for keyword in ['release', 'changelog', 'blog']):
            return 'release_notes'
        else:
            return 'guide'
```

### –≠—Ç–∞–ø 2: TDD —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å —Ç–µ—Å—Ç–∞–º–∏ (1.5 –¥–Ω—è)

#### 2.1 –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —É—Ç–∏–ª–∏—Ç
```python
# tests/test_utils.py
import pytest
from typing import Dict, Any, List
from dataclasses import dataclass

@dataclass
class TestCase:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–ª—É—á–∞—è"""
    name: str
    url: str
    content: str
    strategy: str
    expected_type: str
    expected_fields: List[str]
    should_fail: bool = False

class TestDataFactory:
    """–§–∞–±—Ä–∏–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    @staticmethod
    def create_jina_test_cases() -> List[TestCase]:
        """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è Jina Reader"""
        return [
            TestCase(
                name="Jina FAQ",
                url="https://docs-chatcenter.edna.ru/faq",
                content="""Title: FAQ - –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã
URL Source: https://docs-chatcenter.edna.ru/faq
Content Length: 1500
Language Detected: Russian
Markdown Content:

# FAQ

**Q: –ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º—É?**
A: –°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ "–ù–∞—Å—Ç—Ä–æ–π–∫–∞".

**Q: –ö–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å –∞–≥–µ–Ω—Ç–∞?**
A: –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª "–ê–≥–µ–Ω—Ç—ã" –∏ –Ω–∞–∂–º–∏—Ç–µ "–î–æ–±–∞–≤–∏—Ç—å".""",
                strategy="jina",
                expected_type="faq",
                expected_fields=["title", "content", "content_length", "language_detected"]
            ),
            TestCase(
                name="Jina API Guide",
                url="https://docs-chatcenter.edna.ru/docs/api/create-agent",
                content="""Title: –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ API
URL Source: https://docs-chatcenter.edna.ru/docs/api/create-agent
Content Length: 2000
Language Detected: Russian
Markdown Content:

# –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ API

## –û–ø–∏—Å–∞–Ω–∏–µ

API –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –≤ —Å–∏—Å—Ç–µ–º–µ.

## –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

- `name` - –∏–º—è –∞–≥–µ–Ω—Ç–∞
- `email` - email –∞–≥–µ–Ω—Ç–∞
- `role` - —Ä–æ–ª—å –∞–≥–µ–Ω—Ç–∞""",
                strategy="jina",
                expected_type="api",
                expected_fields=["title", "content", "content_length", "language_detected"]
            )
        ]

    @staticmethod
    def create_html_test_cases() -> List[TestCase]:
        """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è HTML"""
        return [
            TestCase(
                name="HTML Docusaurus",
                url="https://docs-chatcenter.edna.ru/docs/agent/quick-start",
                content="""<!DOCTYPE html>
<html>
<head>
    <title>–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –∞–≥–µ–Ω—Ç–∞ | edna Chat Center</title>
</head>
<body>
    <nav class="theme-doc-breadcrumbs">
        <a href="/docs">–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è</a> > <a href="/docs/agent">–ê–≥–µ–Ω—Ç</a>
    </nav>
    <article class="theme-doc-markdown">
        <h1>–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –∞–≥–µ–Ω—Ç–∞</h1>
        <p>–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –±—ã—Å—Ç—Ä–æ–º—É –Ω–∞—á–∞–ª—É —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ –≤ —Å–∏—Å—Ç–µ–º–µ.</p>
        <h2>–ù–∞—Å—Ç—Ä–æ–π–∫–∞</h2>
        <p>–î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:</p>
        <ol>
            <li>–í–æ–π–¥–∏—Ç–µ –≤ —Å–∏—Å—Ç–µ–º—É</li>
            <li>–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø—Ä–æ—Ñ–∏–ª—å</li>
            <li>–ù–∞—á–Ω–∏—Ç–µ —Ä–∞–±–æ—Ç—É —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏</li>
        </ol>
    </article>
</body>
</html>""",
                strategy="html",
                expected_type="guide",
                expected_fields=["title", "content", "breadcrumb_path", "section_headers"]
            ),
            TestCase(
                name="HTML Generic",
                url="https://docs-chatcenter.edna.ru/faq",
                content="""<!DOCTYPE html>
<html>
<head>
    <title>FAQ - –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã</title>
</head>
<body>
    <h1>FAQ</h1>
    <div class="faq-content">
        <h2>–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã</h2>
        <p><strong>Q: –ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º—É?</strong></p>
        <p>A: –°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ "–ù–∞—Å—Ç—Ä–æ–π–∫–∞".</p>
        <p><strong>Q: –ö–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å –∞–≥–µ–Ω—Ç–∞?</strong></p>
        <p>A: –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª "–ê–≥–µ–Ω—Ç—ã" –∏ –Ω–∞–∂–º–∏—Ç–µ "–î–æ–±–∞–≤–∏—Ç—å".</p>
    </div>
</body>
</html>""",
                strategy="html",
                expected_type="faq",
                expected_fields=["title", "content"]
            )
        ]

    @staticmethod
    def create_error_test_cases() -> List[TestCase]:
        """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫"""
        return [
            TestCase(
                name="Empty Content",
                url="https://example.com/empty",
                content="",
                strategy="auto",
                expected_type="error",
                expected_fields=["title", "content"],
                should_fail=True
            ),
            TestCase(
                name="Invalid HTML",
                url="https://example.com/invalid",
                content="<html><head><title>Test</title></head><body><h1>Test</h1><p>Content</p>",  # –ù–µ –∑–∞–∫—Ä—ã—Ç </body>
                strategy="html",
                expected_type="guide",
                expected_fields=["title", "content"]
            ),
            TestCase(
                name="Malformed Jina",
                url="https://example.com/malformed",
                content="Title: Test\nInvalid content without proper structure",
                strategy="jina",
                expected_type="error",
                expected_fields=["title", "content"],
                should_fail=True
            )
        ]

class TestValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

    @staticmethod
    def validate_processed_page(result, test_case: TestCase) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç ProcessedPage"""
        validation_result = {
            "passed": True,
            "errors": [],
            "warnings": []
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if not hasattr(result, '__class__') or result.__class__.__name__ != 'ProcessedPage':
            validation_result["passed"] = False
            validation_result["errors"].append(f"Expected ProcessedPage, got {type(result)}")
            return validation_result

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        required_attrs = ['url', 'title', 'content', 'page_type', 'metadata']
        for attr in required_attrs:
            if not hasattr(result, attr):
                validation_result["passed"] = False
                validation_result["errors"].append(f"Missing required attribute: {attr}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
        if hasattr(result, 'url') and result.url != test_case.url:
            validation_result["warnings"].append(f"URL mismatch: expected {test_case.url}, got {result.url}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if hasattr(result, 'page_type') and result.page_type != test_case.expected_type:
            validation_result["warnings"].append(f"Page type mismatch: expected {test_case.expected_type}, got {result.page_type}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if hasattr(result, 'content'):
            if not result.content or len(result.content.strip()) < 10:
                validation_result["passed"] = False
                validation_result["errors"].append("Content is too short or empty")
            elif len(result.content.strip()) < 50:
                validation_result["warnings"].append("Content is quite short")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if hasattr(result, 'title'):
            if not result.title or len(result.title.strip()) < 3:
                validation_result["warnings"].append("Title is too short or empty")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if hasattr(result, 'metadata'):
            for field in test_case.expected_fields:
                if field not in result.metadata:
                    validation_result["warnings"].append(f"Expected metadata field missing: {field}")

        return validation_result
```

#### 2.2 –¢–µ—Å—Ç—ã —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö
```python
# tests/test_parser_format_consistency.py
import pytest
from ingestion.processors.content_processor import ContentProcessor
from ingestion.processors.jina_parser import JinaParser
from ingestion.processors.html_parser import HTMLParser

class TestParserFormatConsistency:
    """–¢–µ—Å—Ç—ã —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""

    def test_all_parsers_return_processed_page(self):
        """–¢–µ—Å—Ç —á—Ç–æ –≤—Å–µ –ø–∞—Ä—Å–µ—Ä—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç ProcessedPage"""
        processor = ContentProcessor()

        test_cases = [
            {
                'url': 'https://example.com/jina',
                'content': 'Title: Test\nURL Source: https://example.com\nMarkdown Content:\n# Test\nContent here',
                'strategy': 'jina'
            },
            {
                'url': 'https://example.com/html',
                'content': '<html><head><title>Test</title></head><body><h1>Test</h1><p>Content here</p></body></html>',
                'strategy': 'html'
            },
            {
                'url': 'https://example.com/markdown',
                'content': '# Test\n\nContent here',
                'strategy': 'markdown'
            }
        ]

        for case in test_cases:
            result = processor.process(case['url'], case['content'], case['strategy'])

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç - ProcessedPage
            assert isinstance(result, ProcessedPage), f"Expected ProcessedPage, got {type(result)}"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            assert hasattr(result, 'url')
            assert hasattr(result, 'title')
            assert hasattr(result, 'content')
            assert hasattr(result, 'page_type')
            assert hasattr(result, 'metadata')

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –ø–æ–ª–µ–π
            assert isinstance(result.title, str)
            assert isinstance(result.content, str)
            assert isinstance(result.page_type, str)
            assert isinstance(result.metadata, dict)

    def test_faq_parser_fix(self):
        """–¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è FAQ –ø–∞—Ä—Å–µ—Ä–∞"""
        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è FAQ
        faq_content = """Title: FAQ - –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã
URL Source: https://docs-chatcenter.edna.ru/faq
Markdown Content:

# FAQ

**Q: –ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º—É?**
A: –°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ "–ù–∞—Å—Ç—Ä–æ–π–∫–∞".

**Q: –ö–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å –∞–≥–µ–Ω—Ç–∞?**
A: –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª "–ê–≥–µ–Ω—Ç—ã" –∏ –Ω–∞–∂–º–∏—Ç–µ "–î–æ–±–∞–≤–∏—Ç—å".
"""

        processor = ContentProcessor()
        result = processor.process("https://docs-chatcenter.edna.ru/faq", faq_content, "jina")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç - ProcessedPage (–Ω–µ —Å–ø–∏—Å–æ–∫!)
        assert isinstance(result, ProcessedPage)
        assert result.page_type == 'faq'
        assert 'FAQ' in result.title
        assert '–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º—É' in result.content
        assert '–¥–æ–±–∞–≤–∏—Ç—å –∞–≥–µ–Ω—Ç–∞' in result.content

    def test_html_parser_caching(self):
        """–¢–µ—Å—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è HTML –ø–∞—Ä—Å–µ—Ä–∞"""
        html_content = '<html><head><title>Test</title></head><body><h1>Test</h1></body></html>'

        parser = HTMLParser()

        # –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤
        result1 = parser.parse("https://example.com", html_content)

        # –í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ —Å —Ç–µ–º –∂–µ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        result2 = parser.parse("https://example.com", html_content)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
        assert result1.title == result2.title
        assert result1.content == result2.content

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–µ—à –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        assert len(parser._soup_cache) == 1

    def test_sources_configuration(self):
        """–¢–µ—Å—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
        from app.config.sources import sources_registry

        assert "edna_docs" in sources_registry.list_all()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é edna_docs
        config = sources_registry.get("edna_docs")
        assert config.base_url == "https://docs-chatcenter.edna.ru/"
        assert config.strategy == "jina"
        assert len(config.seed_urls) > 0
        assert len(config.metadata_patterns) > 0

    def test_performance_improvement(self):
        """–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        import time

        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_pages = [
            {
                'url': f'https://example.com/page{i}',
                'content': f'<html><head><title>Page {i}</title></head><body><h1>Page {i}</h1><p>Content {i}</p></body></html>'
            }
            for i in range(100)
        ]

        processor = ContentProcessor()

        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        start_time = time.time()
        results = []
        for page in test_pages:
            result = processor.process(page['url'], page['content'], 'html')
            results.append(result)
        duration = time.time() - start_time

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
        assert len(results) == 100

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –∑–∞ —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è
        assert duration < 10.0  # 10 —Å–µ–∫—É–Ω–¥ –º–∞–∫—Å–∏–º—É–º –¥–ª—è 100 —Å—Ç—Ä–∞–Ω–∏—Ü

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–Ω—ã
        for result in results:
            assert isinstance(result, ProcessedPage)
            assert result.title
            assert result.content
```

#### 2.2 –¢–µ—Å—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å pipeline
```python
# tests/test_pipeline_integration.py
import pytest
from ingestion.processors.content_processor import ContentProcessor
from ingestion.pipeline import crawl_and_index
from app.sources.edna_docs_source import EdnaDocsDataSource

class TestPipelineIntegration:
    """–¢–µ—Å—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å pipeline"""

    def test_pipeline_processing_sanity(self):
        """–°–∞–Ω–∏—Ç–∏-—Ç–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–æ–≤—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º (–±–µ–∑ legacy)."""
        test_pages = [
            {
                'url': 'https://docs-chatcenter.edna.ru/faq',
                'html': '<html><head><title>FAQ</title></head><body><h1>FAQ</h1><p>FAQ content</p></body></html>'
            },
            {
                'url': 'https://docs-chatcenter.edna.ru/api',
                'html': '<html><head><title>API</title></head><body><h1>API</h1><p>API content</p></body></html>'
            }
        ]

        processor = ContentProcessor()

        for page_data in test_pages:
            result = processor.process(page_data['url'], page_data['html'], 'html')
            assert hasattr(result, 'content') and result.content.strip()

    def test_edna_docs_source_adapter(self):
        """–¢–µ—Å—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞: ProcessedPage.content -> payload.text"""
        test_html = '<html><head><title>Test Guide</title></head><body><h1>Test Guide</h1><p>Guide content</p></body></html>'

        source = EdnaDocsDataSource({
            'base_url': 'https://docs-chatcenter.edna.ru/',
            'strategy': 'html'
        })

        parsed_content = source._parse_content('https://docs-chatcenter.edna.ru/guide', test_html)

        assert 'text' in parsed_content
        assert parsed_content['text'].strip()

    def test_sources_configuration_integration(self):
        """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è
        from app.config.sources import get_source_config
        config = get_source_config("edna_docs")
        assert config.base_url == "https://docs-chatcenter.edna.ru/"
        assert config.strategy == "jina"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ seed URLs –¥–æ—Å—Ç—É–ø–Ω—ã
        assert len(config.seed_urls) > 0
        assert "https://docs-chatcenter.edna.ru/" in config.seed_urls

    def test_end_to_end_pipeline_clean_start(self):
        """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å —á–∏—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π"""
        # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–æ–º
        from scripts.clear_collection import clear_collection
        clear_collection()

        # –¢–µ—Å—Ç —Å –Ω–µ–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü
        from app.services.optimized_pipeline import run_optimized_indexing
        result = run_optimized_indexing(
            source_name="edna_docs",
            max_pages=5,
            chunk_strategy="adaptive"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ
        assert result['success'] == True
        assert result['pages'] > 0
        assert result['chunks'] > 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ—Ç –æ—à–∏–±–æ–∫
        assert result.get('errors', 0) == 0
```

### –≠—Ç–∞–ø 3: –ú–∏–≥—Ä–∞—Ü–∏—è pipeline (1 –¥–µ–Ω—å)

#### 3.1 –û–±–Ω–æ–≤–∏—Ç—å ingestion/pipeline.py
```python
# ingestion/pipeline.py
from ingestion.processors.content_processor import ContentProcessor
from app.config.sources import get_source_config, get_all_crawl_urls

def crawl_and_index(source_name: str = "edna_docs", incremental: bool = True,
                   reindex_mode: str = "auto", max_pages: int = None) -> dict[str, Any]:
    """–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π pipeline —Å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    source_config = get_source_config(source_name)

    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º {'–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é ' if incremental else ''}–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_name}")
    logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: strategy={source_config.strategy}, use_cache={source_config.use_cache}")

    # 1) –ö—Ä–∞—É–ª–∏–Ω–≥ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    if reindex_mode == "cache_only":
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ ...
    else:
        pages = crawl_with_sitemap_progress(
            base_url=source_config.base_url,
            strategy=source_config.strategy,
            use_cache=source_config.use_cache,
            max_pages=max_pages or source_config.max_pages
        )

    # 2) –ù–û–í–û–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = ContentProcessor()

    all_chunks = []
    logger.info("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏...")

    with tqdm(total=len(pages), desc="Processing pages") as pbar:
        for p in pages:
            url = p["url"]
            raw_content = p.get("text") or p.get("html") or ""

            if not raw_content:
                logger.warning(f"–ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è {url}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                pbar.update(1)
                continue

            try:
                # –ù–û–í–û–ï: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                processed_page = processor.process(url, raw_content, source_config.strategy)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ ProcessedPage
                text = processed_page.content
                title = processed_page.title
                page_type = processed_page.page_type

                if not text:
                    logger.warning(f"–ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è {url}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    pbar.update(1)
                    continue

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
                chunks_text = chunk_text(text)

                if not chunks_text:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è {url}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    pbar.update(1)
                    continue

                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Å —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π payload
                for i, ct in enumerate(chunks_text):
                    payload = {
                        "url": url,
                        "title": title,
                        "content": ct,  # –í—Å–µ–≥–¥–∞ 'content', –Ω–µ 'text'
                        "page_type": page_type,
                        "source": source_name,
                        "language": "ru",
                        "chunk_index": i,
                        "content_length": len(text),
                        "indexed_at": time.time(),
                        "indexed_via": source_config.strategy,
                        **processed_page.metadata  # –í—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ ProcessedPage
                    }

                    all_chunks.append({
                        "text": ct,  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Qdrant API
                        "payload": payload,
                    })

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
                continue

            pbar.update(1)

    logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤, –Ω–∞—á–∏–Ω–∞–µ–º enhanced metadata-aware –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é...")

    # 3) –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è (–æ—Å—Ç–∞–µ—Ç—Å—è —Ç–∞ –∂–µ)
    metadata_indexer = MetadataAwareIndexer()
    indexed = metadata_indexer.index_chunks_with_metadata(all_chunks)

    return {"pages": len(pages), "chunks": indexed, "source": source_name}
```

#### 3.2 –û–±–Ω–æ–≤–∏—Ç—å app/sources/edna_docs_source.py
```python
# app/sources/edna_docs_source.py
from ingestion.processors.content_processor import ContentProcessor
from app.config.sources import get_source_config

@register_data_source("edna_docs")
class EdnaDocsDataSource(DataSourceBase):
    """Data source for edna documentation website"""

    def __init__(self, config: Dict[str, Any]):
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–µ—Å—Ç—Ä–∞
        source_config = get_source_config("edna_docs")

        self.base_url = config.get("base_url", source_config.base_url)
        self.strategy = config.get("strategy", source_config.strategy)
        self.use_cache = config.get("use_cache", source_config.use_cache)
        self.max_pages = config.get("max_pages", source_config.max_pages)
        self.processor = ContentProcessor()  # –ù–û–í–û–ï
        super().__init__(config)

    def _parse_content(self, url: str, html: str) -> Dict[str, str]:
        """Parse HTML content using new processor (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        try:
            # –ù–û–í–û–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            processed_page = self.processor.process(url, html, self.strategy)

            # –ü—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
            return {
                "content": processed_page.content,  # –ü—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
                "title": processed_page.title,
                "page_type": processed_page.page_type,
                **processed_page.metadata
            }
        except Exception as e:
            logger.warning(f"Parser failed for {url}, using fallback: {e}")
            return {"content": html, "title": ""}
```

#### 3.3 –û–±–Ω–æ–≤–∏—Ç—å app/services/optimized_pipeline.py
```python
# app/services/optimized_pipeline.py
from ingestion.processors.content_processor import ContentProcessor

class OptimizedPipeline:
    """Optimized indexing pipeline with improved architecture"""

    def __init__(self):
        self.indexer = MetadataAwareIndexer()
        self.processor = ContentProcessor()  # –ù–û–í–û–ï
        self.processed_chunks = 0
        self.errors = []

    def _process_pages_to_chunks(self, pages: List[Page], chunk_strategy: str = "adaptive") -> List[Dict[str, Any]]:
        """–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        chunks = []

        # –ù–û–í–û–ï: –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        for page_batch in self._batch_pages(pages, batch_size=10):
            batch_results = self._process_batch(page_batch, chunk_strategy)
            chunks.extend(batch_results)

        return chunks

    def _process_batch(self, pages: List[Page], chunk_strategy: str) -> List[Dict[str, Any]]:
        """–ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        results = []

        for page in pages:
            try:
                # –ù–û–í–û–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
                processed_page = self.processor.process(page.url, page.content)

                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
                chunks = self._create_chunks_from_processed_page(processed_page, chunk_strategy)
                results.extend(chunks)

            except Exception as e:
                logger.error(f"Error processing {page.url}: {e}")
                self.errors.append(str(e))

        return results

    def _create_chunks_from_processed_page(self, processed_page: ProcessedPage, chunk_strategy: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ ProcessedPage"""
        chunks = []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        optimal_size = self._get_optimal_chunk_size(processed_page)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
        chunks_text = chunk_text(processed_page.content, max_tokens=optimal_size)

        for i, chunk_text_content in enumerate(chunks_text):
            chunk = {
                "text": chunk_text_content,
                "payload": {
                    "url": processed_page.url,
                    "title": processed_page.title,
                    "page_type": processed_page.page_type,
                    "source": "docs-site",
                    "language": "ru",
                    "chunk_index": i,
                    "content_length": len(processed_page.content),
                    **processed_page.metadata
                }
            }
            chunks.append(chunk)

        return chunks
```

### –≠—Ç–∞–ø 4: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞ (0.5 –¥–Ω—è)

#### 4.1 –£–¥–∞–ª–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∞–π–ª—ã
```bash
# –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã
rm ingestion/universal_loader.py
rm ingestion/parsers.py
```

#### 4.2 –û–±–Ω–æ–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã
```python
# –û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ –∏–º–ø–æ—Ä—Ç—ã
# –ë—ã–ª–æ:
from ingestion.universal_loader import load_content_universal
from ingestion.parsers import extract_url_metadata

# –°—Ç–∞–ª–æ:
from ingestion.processors.content_processor import ContentProcessor
from ingestion.processors.metadata_extractor import MetadataExtractor
```

### –≠—Ç–∞–ø 5: –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —á–∏—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π (0.5 –¥–Ω—è)

#### 5.1 –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç pipeline —Å —á–∏—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π
```python
# tests/test_full_pipeline_refactored.py
def test_full_pipeline_with_refactored_processor():
    """–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç pipeline —Å —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–µ–Ω–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º –∏ —á–∏—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π"""
    # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–æ–º
    from scripts.clear_collection import clear_collection
    clear_collection()

    # –¢–µ—Å—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    from app.services.optimized_pipeline import run_optimized_indexing
    result = run_optimized_indexing(
        source_name="edna_docs",
        max_pages=10,
        chunk_strategy="adaptive"
    )

    assert result['success'] == True
    assert result['pages'] > 0
    assert result['chunks'] > 0

    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    from app.services.retrieval import search_documents
    results = search_documents("—Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å", limit=5)
    assert len(results) > 0

    # –¢–µ—Å—Ç —á—Ç–æ FAQ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
    faq_results = search_documents("FAQ", limit=5)
    assert len(faq_results) > 0
```

#### 5.2 –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å —á–∏—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π
```python
# tests/test_performance_benchmark_refactored.py
def test_performance_benchmark():
    """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —Å —á–∏—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π"""
    import time

    # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–æ–º
    from scripts.clear_collection import clear_collection
    clear_collection()

    # –¢–µ—Å—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    start_time = time.time()
    from app.services.optimized_pipeline import run_optimized_indexing
    result = run_optimized_indexing(
        source_name="edna_docs",
        max_pages=50,
        chunk_strategy="adaptive"
    )
    duration = time.time() - start_time

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –∑–∞ —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è
    assert duration < 120  # 2 –º–∏–Ω—É—Ç—ã –º–∞–∫—Å–∏–º—É–º –¥–ª—è 50 —Å—Ç—Ä–∞–Ω–∏—Ü
    assert result['success'] == True
    assert result['chunks'] > 0

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    # –û–∂–∏–¥–∞–µ–º –º–∏–Ω–∏–º—É–º 30% —É–ª—É—á—à–µ–Ω–∏–µ
    expected_duration = 120  # 2 –º–∏–Ω—É—Ç—ã
    assert duration < expected_duration, f"Performance regression: {duration}s > {expected_duration}s"
```

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
- ‚úÖ **100%** –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å FAQ –ø–∞—Ä—Å–µ—Ä–æ–º (—Å–ø–∏—Å–æ–∫ ‚Üí —Å–ª–æ–≤–∞—Ä—å)
- ‚úÖ **100%** —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
- ‚úÖ –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –Ω–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ legacy
- ‚úÖ **–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞** payload –±–µ–∑ legacy –ø–æ–ª–µ–π
- ‚úÖ **–ß–∏—Å—Ç–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è** –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **+40%** —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ HTML (–∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ BeautifulSoup)
- **+60%** —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–±–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
- **+80%** —Å–∫–æ—Ä–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏)
- **+30%** –æ–±—â–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å pipeline
- **–ë—ã—Å—Ç—Ä–∞—è –æ—á–∏—Å—Ç–∫–∞** –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

### –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å
- **-90%** –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—à–∏–±–æ–∫ (—É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è)
- **+100%** –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
- **+200%** –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (–≤–∞–ª–∏–¥–∞—Ü–∏—è)
- **–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** –±–µ–∑ legacy —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç—å
- **-40%** —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ (—É–ø—Ä–æ—â–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
- **+300%** —Å–∫–æ—Ä–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
- **+100%** –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏
- **–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è** –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö

### –ü–∞–º—è—Ç—å
- **-20%** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ (—É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è)
- **+50%** —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
- **–ß–∏—Å—Ç–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è** –±–µ–∑ legacy –¥–∞–Ω–Ω—ã—Ö

## üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞

1. **–í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç** - —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–æ–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ (–±–µ–∑ legacy)
2. **FAQ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ** - –Ω–µ—Ç –ø—É—Å—Ç—ã—Ö —á–∞–Ω–∫–æ–≤
3. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∞ –º–∏–Ω–∏–º—É–º –Ω–∞ 30%**
4. **–ö–æ–¥ —Å—Ç–∞–ª –ø—Ä–æ—â–µ –∏ –ø–æ–Ω—è—Ç–Ω–µ–µ** - –º–µ–Ω—å—à–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
5. **–õ–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –ø–∞—Ä—Å–µ—Ä—ã** - –º–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
6. **–ß–∏—Å—Ç–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è** - —É—Å–ø–µ—à–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –Ω–æ–≤–æ–π —Å—Ö–µ–º–æ–π –¥–∞–Ω–Ω—ã—Ö
7. **–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è** - –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö —É–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –∏–∑ –æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞

## üöÄ –ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

- **–î–µ–Ω—å 0.5**: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
- **–î–µ–Ω—å 1-2**: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –ø–∞—Ä—Å–µ—Ä–æ–≤
- **–î–µ–Ω—å 3**: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤ —Å —á–∏—Å—Ç–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π
- **–î–µ–Ω—å 4**: –ú–∏–≥—Ä–∞—Ü–∏—è pipeline
- **–î–µ–Ω—å 4.5**: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

**–û–±—â–µ–µ –≤—Ä–µ–º—è**: 4.5 –¥–Ω—è (—É–ø—Ä–æ—â–µ–Ω–∏–µ –∑–∞ —Å—á–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è legacy —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
**–†–∏—Å–∫**: –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π (—á–∏—Å—Ç–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è, –Ω–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
**–í—ã–≥–æ–¥–∞**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ + –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è + —É–ø—Ä–æ—â–µ–Ω–∏–µ)

## ‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ü–õ–ê–ù–ê

### 1. –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –Ω–µ–ø–æ–ª–Ω–æ—Ç–∞ –Ω–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
**–ü–†–û–ë–õ–ï–ú–ê**: –ü–ª–∞–Ω –≤–≤–æ–¥–∏—Ç SourcesRegistry, –Ω–æ –≤ —Ç–µ–∫—É—â–µ–º –∫–æ–¥–µ —É–∂–µ –µ—Å—Ç—å PluginManager.
**–†–ï–®–ï–ù–ò–ï**: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å SourcesRegistry —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º PluginManager:

```python
# app/config/sources.py - –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è
from app.abstractions.data_source import plugin_manager, DataSourceBase

class SourcesRegistry:
    """–†–µ–µ—Å—Ç—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö - –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ô —Å PluginManager"""

    def get_plugin_config(self, name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è PluginManager"""
        config = self.get(name)
        return {
            "base_url": config.base_url,
            "strategy": config.strategy,
            "use_cache": config.use_cache,
            "max_pages": config.max_pages,
            "crawl_deny_prefixes": config.crawl_deny_prefixes,
            "sitemap_path": config.sitemap_path,
            "seed_urls": config.seed_urls,
            "metadata_patterns": config.metadata_patterns
        }

# –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ï —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def get_plugin_config(name: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è PluginManager"""
    return sources_registry.get_plugin_config(name)
```

### 2. –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è page_type –≤ –Ω–æ–≤—ã—Ö –ø–∞—Ä—Å–µ—Ä–∞—Ö
**–ü–†–û–ë–õ–ï–ú–ê**: ProcessedPage —Ç—Ä–µ–±—É–µ—Ç page_type, –Ω–æ –≤ –ø—Ä–∏–º–µ—Ä–Ω–æ–º –∫–æ–¥–µ –Ω–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —É—Ç–∏–ª–∏—Ç.
**–†–ï–®–ï–ù–ò–ï**: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:

```python
# ingestion/processors/base.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è
class BaseParser(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å –ò–ù–¢–ï–ì–†–ê–¶–ò–ï–ô page_type"""

    def __init__(self):
        self._page_type_classifier = None

    def _get_page_type_classifier(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        if self._page_type_classifier is None:
            from app.abstractions.data_source import DataSourceBase
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è classify_page_by_url
            temp_source = type('TempSource', (DataSourceBase,), {})()
            self._page_type_classifier = temp_source
        return self._page_type_classifier

    def _detect_page_type(self, url: str, content: str = None) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏—Å–ø–æ–ª—å–∑—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ª–æ–≥–∏–∫—É"""
        classifier = self._get_page_type_classifier()
        page_type = classifier.classify_page_by_url(url)
        return page_type.value
```

### 3. –£–¥–∞–ª–µ–Ω–∏–µ ingestion/parsers.py —Ç—Ä–µ–±—É–µ—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π
**–ü–†–û–ë–õ–ï–ú–ê**: –§—É–Ω–∫—Ü–∏–∏ –∏–∑ parsers.py –∏—Å–ø–æ–ª—å–∑—É—é—Ç EdnaDocsDataSource –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã.
**–†–ï–®–ï–ù–ò–ï**: –°–æ–∑–¥–∞—Ç—å –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π:

```python
# ingestion/parsers_migration.py - –í–†–ï–ú–ï–ù–ù–´–ô —Ñ–∞–π–ª –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏
"""
–í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ parsers.py
–£–¥–∞–ª–∏—Ç—å –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π
"""

from ingestion.processors.content_processor import ContentProcessor
from loguru import logger

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
_content_processor = ContentProcessor()

def extract_url_metadata(url: str) -> Dict[str, Any]:
    """–ú–ò–ì–†–ê–¶–ò–û–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"""
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        # –≠—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–æ –ø–æ–ª–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏
        return {
            "url": url,
            "source": "migrated",
            "extracted_at": time.time()
        }
    except Exception as e:
        logger.warning(f"Migration fallback for extract_url_metadata: {e}")
        return {"url": url, "source": "fallback"}

# –î—Ä—É–≥–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ parsers.py –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
```

### 4. –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —ç—Ç–∞–ø—ã –º–∏–≥—Ä–∞—Ü–∏–∏

#### –≠—Ç–∞–ø 3.1: –°–æ–∑–¥–∞—Ç—å –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π (0.5 –¥–Ω—è)
```python
# 1. –°–æ–∑–¥–∞—Ç—å ingestion/parsers_migration.py
# 2. –û–±–Ω–æ–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–∞—Ö:
#    - ingestion/pipeline.py: from app.sources_registry import extract_url_metadata
#    - app/sources/edna_docs_source.py: from app.sources_registry import extract_url_metadata
#    - scripts/*.py: –æ–±–Ω–æ–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã

# 3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º —Å–ª–æ–µ–º
```

#### –≠—Ç–∞–ø 3.2: –û–±–Ω–æ–≤–∏—Ç—å pipeline —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π PluginManager
```python
# ingestion/pipeline.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è
from app.config.sources import get_plugin_config
from app.abstractions.data_source import plugin_manager

def crawl_and_index(source_name: str = "edna_docs", incremental: bool = True,
                   reindex_mode: str = "auto", max_pages: int = None) -> dict[str, Any]:
    """–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π pipeline —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π PluginManager"""

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è PluginManager
    source_config = get_plugin_config(source_name)

    # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ —á–µ—Ä–µ–∑ PluginManager
    source = plugin_manager.get_source(source_name, source_config)

    # –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–æ–π –∂–µ...
```

### 5. –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

- **–î–µ–Ω—å 0.5**: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
- **–î–µ–Ω—å 1**: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π PluginManager
- **–î–µ–Ω—å 2**: –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π page_type
- **–î–µ–Ω—å 3**: –°–æ–∑–¥–∞–Ω–∏–µ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è –∏ —Ç–µ—Å—Ç–æ–≤
- **–î–µ–Ω—å 4**: –ú–∏–≥—Ä–∞—Ü–∏—è pipeline —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
- **–î–µ–Ω—å 4.5**: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

**–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û**: –í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º PluginManager –∏ —Å–∏—Å—Ç–µ–º–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ page_type.
