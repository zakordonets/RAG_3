#!/usr/bin/env python3
"""
–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from app.services.retrieval import hybrid_search
from app.services.bge_embeddings import embed_unified

def test_improved_ranking():
    """–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
    query = "–ö–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ —á–∞—Ç-—Ü–µ–Ω—Ç—Ä–µ"

    print(f"üîç –¢–ï–°–¢ –£–õ–£–ß–®–ï–ù–ù–û–ì–û –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
    print("=" * 80)

    try:
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        embeddings = embed_unified(query, return_dense=True, return_sparse=True)
        dense_vec = embeddings['dense_vecs'][0]
        sparse_vec = embeddings['sparse_vecs'][0]

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        results = hybrid_search(
            query_dense=dense_vec,
            query_sparse=sparse_vec,
            k=10  # –¢–æ–ø-10 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        )

        print(f"üìä –¢–æ–ø-10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è:")
        print()

        for i, result in enumerate(results, 1):
            rrf_score = result.get('rrf_score', 0)
            boosted_score = result.get('boosted_score', rrf_score)
            payload = result.get('payload', {})

            title = payload.get('title', 'N/A')
            url = payload.get('url', 'N/A')
            text = payload.get('text', '').lower()

            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            keywords = ['–∫–∞–Ω–∞–ª', 'telegram', 'whatsapp', 'viber', '–∞–≤–∏—Ç–æ', '–≤–µ–±-–≤–∏–¥–∂–µ—Ç', '–º–æ–±–∏–ª—å–Ω—ã–π', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è']
            keyword_matches = sum(1 for keyword in keywords if keyword in text)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if 'blog' in url or '–≤–µ—Ä—Å–∏—è' in title.lower():
                doc_type = "üìù Release Notes"
            elif 'docs/start' in url or '—á—Ç–æ —Ç–∞–∫–æ–µ' in title.lower():
                doc_type = "üè† –ì–ª–∞–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            elif 'admin' in url:
                doc_type = "‚öôÔ∏è –ê–¥–º–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            elif 'api' in url:
                doc_type = "üîß API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            else:
                doc_type = "üìÑ –û–±—ã—á–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"

            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –±—É—Å—Ç–∏–Ω–≥–∞
            boost_factor = boosted_score / rrf_score if rrf_score > 0 else 1.0

            print(f"{i:2d}. {doc_type}")
            print(f"    üìù –ù–∞–∑–≤–∞–Ω–∏–µ: {title}")
            print(f"    üîó URL: {url}")
            print(f"    üìä RRF Score: {rrf_score:.6f}")
            print(f"    üöÄ Boosted Score: {boosted_score:.6f}")
            print(f"    ‚ö° Boost Factor: {boost_factor:.2f}x")
            print(f"    üéØ –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {keyword_matches}/8")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞
            text_preview = payload.get('text', '')[:150]
            print(f"    üìÑ –ü—Ä–µ–≤—å—é: {text_preview}...")

            # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            if keyword_matches >= 3:
                print(f"    ‚úÖ –í–´–°–û–ö–ê–Ø –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨")
            elif keyword_matches >= 2:
                print(f"    ‚ö†Ô∏è  –°–†–ï–î–ù–Ø–Ø –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨")
            else:
                print(f"    ‚ùå –ù–ò–ó–ö–ê–Ø –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨")

            print()

        # –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        print("=" * 80)
        print("üìà –ê–ù–ê–õ–ò–ó –£–õ–£–ß–®–ï–ù–ò–ô:")

        # –ù–∞–π–¥–µ–º "–ß—Ç–æ —Ç–∞–∫–æ–µ edna Chat Center"
        whatis_doc = None
        for result in results:
            if "docs/start/whatis" in result.get('payload', {}).get('url', ''):
                whatis_doc = result
                break

        if whatis_doc:
            position = next(i for i, r in enumerate(results, 1) if r == whatis_doc)
            print(f"   üè† '–ß—Ç–æ —Ç–∞–∫–æ–µ edna Chat Center': –ø–æ–∑–∏—Ü–∏—è {position}")

            # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ release notes –≤ —Ç–æ–ø–µ
            release_notes_count = sum(1 for r in results[:5] if 'blog' in r.get('payload', {}).get('url', ''))
            print(f"   üìù Release Notes –≤ —Ç–æ–ø-5: {release_notes_count}")

            # –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–æ–ø-5
            high_relevance_count = 0
            for r in results[:5]:
                text = r.get('payload', {}).get('text', '').lower()
                keywords = ['–∫–∞–Ω–∞–ª', 'telegram', 'whatsapp', 'viber', '–∞–≤–∏—Ç–æ', '–≤–µ–±-–≤–∏–¥–∂–µ—Ç', '–º–æ–±–∏–ª—å–Ω—ã–π', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è']
                keyword_matches = sum(1 for keyword in keywords if keyword in text)
                if keyword_matches >= 2:
                    high_relevance_count += 1

            print(f"   üéØ –í—ã—Å–æ–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤ —Ç–æ–ø-5: {high_relevance_count}")

            if position <= 2:
                print("   ‚úÖ –£–°–ü–ï–•: –ì–ª–∞–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤ —Ç–æ–ø-2!")
            else:
                print("   ‚ö†Ô∏è  –ì–ª–∞–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –≤ —Ç–æ–ø-2")

            if release_notes_count == 0:
                print("   ‚úÖ –£–°–ü–ï–•: Release Notes –∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ —Ç–æ–ø-5!")
            else:
                print("   ‚ö†Ô∏è  Release Notes –≤—Å–µ –µ—â–µ –≤ —Ç–æ–ø-5")

            if high_relevance_count >= 3:
                print("   ‚úÖ –£–°–ü–ï–•: –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–æ–ø-5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã!")
            else:
                print("   ‚ö†Ô∏è  –ú–∞–ª–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–æ–ø-5")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")

if __name__ == "__main__":
    test_improved_ranking()
