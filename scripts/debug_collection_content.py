#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º —Å –ª–æ–∞–¥–µ—Ä–æ–º
"""
import asyncio
import sys
import json
from pathlib import Path
from typing import Dict, List, Any

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—é app
sys.path.append(str(Path(__file__).parent.parent))

from app.services.retrieval import client, COLLECTION
from app.services.bge_embeddings import embed_unified
from app.services.retrieval import hybrid_search
from app.services.rerank import rerank
from qdrant_client.models import Filter


class CollectionDebugger:
    """–û—Ç–ª–∞–¥—á–∏–∫ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""

    def __init__(self):
        self.client = client
        self.collection = COLLECTION

    def check_collection_info(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            info = self.client.get_collection(self.collection)
            return {
                "exists": True,
                "points_count": info.points_count,
                "vectors_config": info.config.params.vectors,
                "status": info.status
            }
        except Exception as e:
            return {"exists": False, "error": str(e)}

    def sample_documents(self, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–±—Ä–∞–∑—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            results = self.client.scroll(
                collection_name=self.collection,
                limit=limit,
                with_payload=True
            )

            docs = []
            for doc in results[0]:
                payload = doc.payload
                content = str(payload.get("content", ""))

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                has_russian = any(ord(c) > 127 for c in content[:200]) if content else False
                is_empty = len(content.strip()) == 0

                docs.append({
                    "id": str(doc.id),
                    "title": payload.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                    "url": payload.get("url", "–ë–µ–∑ URL"),
                    "page_type": payload.get("page_type", "unknown"),
                    "language": payload.get("language", "unknown"),
                    "source": payload.get("source", "unknown"),
                    "content_length": len(content),
                    "content_preview": content[:200] + "..." if len(content) > 200 else content,
                    "has_russian": has_russian,
                    "is_empty": is_empty,
                    "indexed_via": payload.get("indexed_via", "unknown"),
                    "chunk_index": payload.get("chunk_index", "unknown")
                })

            return docs

        except Exception as e:
            return [{"error": str(e)}]

    def check_content_quality(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            results = self.client.scroll(
                collection_name=self.collection,
                limit=100,
                with_payload=True
            )

            docs = results[0]
            total_docs = len(docs)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            empty_docs = 0
            russian_docs = 0
            english_docs = 0
            total_content_length = 0
            content_lengths = []

            sources = {}
            page_types = {}
            languages = {}
            indexed_via = {}

            for doc in docs:
                payload = doc.payload
                content = str(payload.get("content", ""))

                # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content_length = len(content)
                content_lengths.append(content_length)
                total_content_length += content_length

                # –ü—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                if content_length == 0:
                    empty_docs += 1
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–∑—ã–∫
                    if any(ord(c) > 127 for c in content[:200]):
                        russian_docs += 1
                    else:
                        english_docs += 1

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
                source = payload.get("source", "unknown")
                sources[source] = sources.get(source, 0) + 1

                page_type = payload.get("page_type", "unknown")
                page_types[page_type] = page_types.get(page_type, 0) + 1

                language = payload.get("language", "unknown")
                languages[language] = languages.get(language, 0) + 1

                indexed_via_method = payload.get("indexed_via", "unknown")
                indexed_via[indexed_via_method] = indexed_via.get(indexed_via_method, 0) + 1

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if content_lengths:
                avg_length = total_content_length / len(content_lengths)
                min_length = min(content_lengths)
                max_length = max(content_lengths)
            else:
                avg_length = min_length = max_length = 0

            return {
                "total_documents": total_docs,
                "empty_documents": empty_docs,
                "russian_documents": russian_docs,
                "english_documents": english_docs,
                "russian_percentage": (russian_docs / total_docs * 100) if total_docs > 0 else 0,
                "empty_percentage": (empty_docs / total_docs * 100) if total_docs > 0 else 0,
                "average_content_length": avg_length,
                "min_content_length": min_length,
                "max_content_length": max_length,
                "sources": sources,
                "page_types": page_types,
                "languages": languages,
                "indexed_via": indexed_via
            }

        except Exception as e:
            return {"error": str(e)}

    def test_search_functionality(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞"""
        test_queries = [
            "–ö–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ —á–∞—Ç-—Ü–µ–Ω—Ç—Ä–µ?",
            "telegram",
            "–≤–∏–¥–∂–µ—Ç",
            "edna Chat Center",
            "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞"
        ]

        results = {}

        for query in test_queries:
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                embeddings = embed_unified(query, return_dense=True, return_sparse=True)

                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
                search_results = hybrid_search(
                    query_dense=embeddings['dense_vecs'][0],
                    query_sparse=embeddings['sparse_vecs'][0],
                    k=5
                )

                # –†–µ—Ä–∞–Ω–∫–∏–Ω–≥
                reranked = rerank(query, search_results, top_n=3)

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                result_analysis = []
                for i, doc in enumerate(reranked):
                    payload = doc.get("payload", {})
                    content = str(payload.get("content", ""))

                    result_analysis.append({
                        "rank": i + 1,
                        "title": payload.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                        "url": payload.get("url", "–ë–µ–∑ URL"),
                        "score": doc.get("boosted_score", 0.0),
                        "content_length": len(content),
                        "has_russian": any(ord(c) > 127 for c in content[:200]) if content else False,
                        "content_preview": content[:100] + "..." if len(content) > 100 else content
                    })

                results[query] = {
                    "total_found": len(search_results),
                    "reranked_count": len(reranked),
                    "results": result_analysis
                }

            except Exception as e:
                results[query] = {"error": str(e)}

        return results

    def check_specific_urls(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö URL –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        test_urls = [
            "https://docs-chatcenter.edna.ru/docs/start/whatis",
            "https://docs-chatcenter.edna.ru/docs/admin/widget/admin-widget-features",
            "https://docs-chatcenter.edna.ru/docs/sdk/sdk-mobilechat"
        ]

        results = {}

        for url in test_urls:
            try:
                filter_result = self.client.scroll(
                    collection_name=self.collection,
                    scroll_filter=Filter(
                        must=[
                            {'key': 'url', 'match': {'text': url}}
                        ]
                    ),
                    limit=5,
                    with_payload=True
                )

                found_docs = filter_result[0]
                results[url] = {
                    "found": len(found_docs),
                    "documents": [
                        {
                            "title": doc.payload.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                            "content_length": len(str(doc.payload.get("content", ""))),
                            "has_russian": any(ord(c) > 127 for c in str(doc.payload.get("content", ""))[:200]),
                            "indexed_via": doc.payload.get("indexed_via", "unknown")
                        }
                        for doc in found_docs
                    ]
                }

            except Exception as e:
                results[url] = {"error": str(e)}

        return results


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç–ª–∞–¥–∫–∏"""
    debugger = CollectionDebugger()

    print("üîç –û–¢–õ–ê–î–ö–ê –ö–û–õ–õ–ï–ö–¶–ò–ò")
    print("="*60)

    # 1. –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    print("\n1Ô∏è‚É£ –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
    info = debugger.check_collection_info()

    if info.get("exists"):
        print(f"   ‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {info['points_count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"   üìä –°—Ç–∞—Ç—É—Å: {info['status']}")
        print(f"   ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤: {len(info['vectors_config'])} —Ç–∏–ø–æ–≤")
    else:
        print(f"   ‚ùå –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {info.get('error', 'Unknown error')}")
        return

    # 2. –û–±—Ä–∞–∑—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("\n2Ô∏è‚É£ –û–±—Ä–∞–∑—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    samples = debugger.sample_documents(5)

    for i, doc in enumerate(samples, 1):
        if "error" in doc:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {doc['error']}")
            continue

        print(f"\n   üìÑ –î–æ–∫—É–º–µ–Ω—Ç {i}:")
        print(f"      ID: {doc['id']}")
        print(f"      –ó–∞–≥–æ–ª–æ–≤–æ–∫: {doc['title']}")
        print(f"      URL: {doc['url']}")
        print(f"      –¢–∏–ø: {doc['page_type']}")
        print(f"      –Ø–∑—ã–∫: {doc['language']}")
        print(f"      –ò—Å—Ç–æ—á–Ω–∏–∫: {doc['source']}")
        print(f"      –î–ª–∏–Ω–∞: {doc['content_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"      {'‚úÖ' if doc['has_russian'] else '‚ùå'} –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç")
        print(f"      {'‚ùå' if doc['is_empty'] else '‚úÖ'} –ù–µ –ø—É—Å—Ç–æ–π")
        print(f"      –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑: {doc['indexed_via']}")
        print(f"      –ü—Ä–µ–≤—å—é: {doc['content_preview']}")

    # 3. –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    print("\n3Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
    quality = debugger.check_content_quality()

    if "error" not in quality:
        print(f"   üìä –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {quality['total_documents']}")
        print(f"   üá∑üá∫ –†—É—Å—Å–∫–∏—Ö: {quality['russian_documents']} ({quality['russian_percentage']:.1f}%)")
        print(f"   üá¨üáß –ê–Ω–≥–ª–∏–π—Å–∫–∏—Ö: {quality['english_documents']}")
        print(f"   ‚ùå –ü—É—Å—Ç—ã—Ö: {quality['empty_documents']} ({quality['empty_percentage']:.1f}%)")
        print(f"   üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {quality['average_content_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üìè –ú–∏–Ω/–ú–∞–∫—Å: {quality['min_content_length']}/{quality['max_content_length']}")

        print(f"\n   üìÇ –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
        for source, count in quality['sources'].items():
            print(f"      {source}: {count}")

        print(f"\n   üìÑ –¢–∏–ø—ã —Å—Ç—Ä–∞–Ω–∏—Ü:")
        for page_type, count in quality['page_types'].items():
            print(f"      {page_type}: {count}")

        print(f"\n   üåê –Ø–∑—ã–∫–∏:")
        for language, count in quality['languages'].items():
            print(f"      {language}: {count}")

        print(f"\n   üîß –ú–µ—Ç–æ–¥—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:")
        for method, count in quality['indexed_via'].items():
            print(f"      {method}: {count}")

    # 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞
    print("\n4Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞:")
    search_results = debugger.test_search_functionality()

    for query, result in search_results.items():
        if "error" in result:
            print(f"   ‚ùå {query}: {result['error']}")
        else:
            print(f"   üîç {query}: –Ω–∞–π–¥–µ–Ω–æ {result['total_found']}, –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ {result['reranked_count']}")
            for doc in result['results']:
                print(f"      {doc['rank']}. {doc['title'][:50]}... (–¥–ª–∏–Ω–∞: {doc['content_length']}, {'üá∑üá∫' if doc['has_russian'] else 'üá¨üáß'})")

    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö URL
    print("\n5Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö URL:")
    url_results = debugger.check_specific_urls()

    for url, result in url_results.items():
        if "error" in result:
            print(f"   ‚ùå {url}: {result['error']}")
        else:
            print(f"   {'‚úÖ' if result['found'] > 0 else '‚ùå'} {url}: –Ω–∞–π–¥–µ–Ω–æ {result['found']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            for doc in result['documents']:
                print(f"      - {doc['title']} (–¥–ª–∏–Ω–∞: {doc['content_length']}, {'üá∑üá∫' if doc['has_russian'] else 'üá¨üáß'}, {doc['indexed_via']})")

    # 6. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º
    print("\n6Ô∏è‚É£ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º:")

    if quality.get('russian_percentage', 0) < 50:
        print("   ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –ú–∞–ª–æ —Ä—É—Å—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print("   üí° –†–ï–®–ï–ù–ò–ï: –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Å Jina Reader")

    if quality.get('empty_percentage', 0) > 50:
        print("   ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –ú–Ω–æ–≥–æ –ø—É—Å—Ç—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print("   üí° –†–ï–®–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞")

    if quality.get('average_content_length', 0) < 100:
        print("   ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç")
        print("   üí° –†–ï–®–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞–Ω–∫–∏–Ω–≥–∞")

    if not any("jina" in str(method) for method in quality.get('indexed_via', {}).keys()):
        print("   ‚ùå –ü–†–û–ë–õ–ï–ú–ê: Jina Reader –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è")
        print("   üí° –†–ï–®–ï–ù–ò–ï: –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å --strategy jina")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç (–±–µ–∑ –Ω–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤)
    full_report = {
        "collection_info": {
            "exists": info.get("exists", False),
            "points_count": info.get("points_count", 0),
            "status": info.get("status", "unknown")
        },
        "samples": samples,
        "content_quality": quality,
        "search_results": search_results,
        "url_results": url_results
    }

    report_file = Path("collection_debug_report.json")
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(full_report, f, ensure_ascii=False, indent=2)

    print(f"\nüíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_file}")
    print("\n" + "="*60)
    print("‚úÖ –û–¢–õ–ê–î–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(main())
