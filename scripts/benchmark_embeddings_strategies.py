#!/usr/bin/env python3
"""
Comprehensive benchmark for BGE-M3 embedding strategies
Compares ONNX, BGE, and Hybrid backends on different scenarios
"""
import sys
import os
sys.path.append(os.getcwd())

import time
import statistics
from typing import List, Dict, Any
from dataclasses import dataclass
from loguru import logger

from app.services.bge_embeddings import embed_unified, embed_batch_optimized
from app.config import CONFIG

@dataclass
class BenchmarkResult:
    backend: str
    scenario: str
    avg_time: float
    min_time: float
    max_time: float
    std_dev: float
    success_rate: float
    dense_shape: int
    sparse_tokens: int
    error_msg: str = ""

class EmbeddingsBenchmark:
    def __init__(self):
        self.results: List[BenchmarkResult] = []

        # Test scenarios
        self.test_texts = {
            "short_query": "ÐšÐ°Ðº Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ?",
            "medium_query": "ÐšÐ°Ðº Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ñ‡Ð°Ñ‚Ð¾Ð² Ð¸Ð· Ð²ÐµÐ±-Ð²Ð¸Ð´Ð¶ÐµÑ‚Ð° Ð½Ð° Ð¾Ñ‚Ð´ÐµÐ» Ð¿Ñ€Ð¾Ð´Ð°Ð¶ Ð² edna Chat Center?",
            "long_query": "ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ð¾ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ñ‚Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð²Ñ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ñ‡Ð°Ñ‚Ð¾Ð² Ð¸Ð· Ð²ÐµÐ±-Ð²Ð¸Ð´Ð¶ÐµÑ‚Ð° Ð½Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð´ÐµÐ» Ð¿Ñ€Ð¾Ð´Ð°Ð¶ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ñ€Ð°Ð±Ð¾Ñ‡ÐµÐ³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð¸ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ð¾Ð² Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ edna Chat Center",
            "doc_chunk": """
            ÐœÐ°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‡Ð°Ñ‚Ð¾Ð² Ð² edna Chat Center

            Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´ÑÑ‰Ð¸Ðµ Ñ‡Ð°Ñ‚Ñ‹ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð°Ð¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð°Ð²Ð¸Ð» Ð¸ ÑƒÑÐ»Ð¾Ð²Ð¸Ð¹. ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸:

            1. ÐœÐ°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð½Ð°Ð²Ñ‹ÐºÐ°Ð¼ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð¾Ð²
            2. Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð³Ñ€ÑƒÐ¿Ð¿Ð°Ð¼ Ð¸ Ð¾Ñ‚Ð´ÐµÐ»Ð°Ð¼
            3. Ð£Ñ‡ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‡ÐµÐ³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚Ð¸
            4. ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ VIP-ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð²
            5. Ð‘Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ° Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼ÐµÐ¶Ð´Ñƒ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð°Ð¼Ð¸

            ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÑÑ Ñ‡ÐµÑ€ÐµÐ· Ð¿Ð°Ð½ÐµÐ»ÑŒ Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€Ð° Ð² Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ "ÐœÐ°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ".
            """
        }

        self.batch_texts = [
            "ÐšÐ°Ðº Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð°?",
            "ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð²ÐµÐ±-Ð²Ð¸Ð´Ð¶ÐµÑ‚Ð°",
            "Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ CRM ÑÐ¸ÑÑ‚ÐµÐ¼Ð¾Ð¹",
            "ÐžÑ‚Ñ‡ÐµÑ‚Ñ‹ Ð¿Ð¾ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð°Ð¼",
            "Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑÐ¼Ð¸ Ñ‡Ð°Ñ‚Ð¾Ð²"
        ]

    def benchmark_single_embedding(self, backend: str, text: str, scenario: str, runs: int = 5) -> BenchmarkResult:
        """Benchmark single text embedding."""
        logger.info(f"Benchmarking {backend} backend: {scenario}")

        # Temporarily override backend
        original_backend = CONFIG.embeddings_backend
        CONFIG.__dict__['embeddings_backend'] = backend

        times = []
        successes = 0
        dense_shape = 0
        sparse_tokens = 0
        error_msg = ""

        for i in range(runs):
            try:
                start = time.time()
                result = embed_unified(
                    text,
                    return_dense=True,
                    return_sparse=CONFIG.use_sparse,
                    context="query" if "query" in scenario else "document"
                )
                duration = time.time() - start
                times.append(duration)
                successes += 1

                # Extract metrics from first successful run
                if successes == 1:
                    if result.get('dense_vecs'):
                        dense_shape = len(result['dense_vecs'][0])
                    if result.get('lexical_weights') and result['lexical_weights'][0]:
                        sparse_tokens = len(result['lexical_weights'][0])

            except Exception as e:
                error_msg = str(e)
                logger.warning(f"Run {i+1} failed: {e}")

        # Restore original backend
        CONFIG.__dict__['embeddings_backend'] = original_backend

        if times:
            return BenchmarkResult(
                backend=backend,
                scenario=scenario,
                avg_time=statistics.mean(times),
                min_time=min(times),
                max_time=max(times),
                std_dev=statistics.stdev(times) if len(times) > 1 else 0.0,
                success_rate=successes / runs,
                dense_shape=dense_shape,
                sparse_tokens=sparse_tokens,
                error_msg=error_msg
            )
        else:
            return BenchmarkResult(
                backend=backend,
                scenario=scenario,
                avg_time=0.0,
                min_time=0.0,
                max_time=0.0,
                std_dev=0.0,
                success_rate=0.0,
                dense_shape=0,
                sparse_tokens=0,
                error_msg=error_msg
            )

    def benchmark_batch_embedding(self, backend: str, runs: int = 3) -> BenchmarkResult:
        """Benchmark batch text embedding."""
        logger.info(f"Benchmarking {backend} backend: batch_processing")

        # Temporarily override backend
        original_backend = CONFIG.embeddings_backend
        CONFIG.__dict__['embeddings_backend'] = backend

        times = []
        successes = 0
        dense_shape = 0
        sparse_tokens = 0
        error_msg = ""

        for i in range(runs):
            try:
                start = time.time()
                result = embed_batch_optimized(
                    self.batch_texts,
                    return_dense=True,
                    return_sparse=CONFIG.use_sparse,
                    context="document"
                )
                duration = time.time() - start
                times.append(duration)
                successes += 1

                # Extract metrics from first successful run
                if successes == 1:
                    if result.get('dense_vecs'):
                        dense_shape = len(result['dense_vecs'][0])
                    if result.get('lexical_weights'):
                        # Average sparse tokens across batch
                        sparse_counts = [len(lex) for lex in result['lexical_weights'] if lex]
                        sparse_tokens = int(statistics.mean(sparse_counts)) if sparse_counts else 0

            except Exception as e:
                error_msg = str(e)
                logger.warning(f"Batch run {i+1} failed: {e}")

        # Restore original backend
        CONFIG.__dict__['embeddings_backend'] = original_backend

        if times:
            return BenchmarkResult(
                backend=backend,
                scenario="batch_processing",
                avg_time=statistics.mean(times),
                min_time=min(times),
                max_time=max(times),
                std_dev=statistics.stdev(times) if len(times) > 1 else 0.0,
                success_rate=successes / runs,
                dense_shape=dense_shape,
                sparse_tokens=sparse_tokens,
                error_msg=error_msg
            )
        else:
            return BenchmarkResult(
                backend=backend,
                scenario="batch_processing",
                avg_time=0.0,
                min_time=0.0,
                max_time=0.0,
                std_dev=0.0,
                success_rate=0.0,
                dense_shape=0,
                sparse_tokens=0,
                error_msg=error_msg
            )

    def run_comprehensive_benchmark(self):
        """Run comprehensive benchmark across all backends and scenarios."""
        backends = ["onnx", "bge", "hybrid"]

        logger.info("ðŸš€ Starting comprehensive embeddings benchmark")
        logger.info(f"Testing backends: {backends}")
        logger.info(f"Use sparse: {CONFIG.use_sparse}")

        # Single text benchmarks
        for backend in backends:
            for scenario, text in self.test_texts.items():
                result = self.benchmark_single_embedding(backend, text, scenario)
                self.results.append(result)

        # Batch processing benchmarks
        for backend in backends:
            result = self.benchmark_batch_embedding(backend)
            self.results.append(result)

    def print_results(self):
        """Print formatted benchmark results."""
        print("\n" + "="*100)
        print("ðŸŽ¯ EMBEDDINGS BENCHMARK RESULTS")
        print("="*100)

        # Group results by scenario
        scenarios = {}
        for result in self.results:
            if result.scenario not in scenarios:
                scenarios[result.scenario] = []
            scenarios[result.scenario].append(result)

        for scenario, results in scenarios.items():
            print(f"\nðŸ“Š {scenario.upper().replace('_', ' ')}")
            print("-" * 80)
            print(f"{'Backend':<10} {'Time (s)':<12} {'Success':<8} {'Dense':<8} {'Sparse':<8} {'Error'}")
            print("-" * 80)

            for result in results:
                time_str = f"{result.avg_time:.3f}Â±{result.std_dev:.3f}" if result.avg_time > 0 else "FAILED"
                success_str = f"{result.success_rate:.1%}"
                dense_str = str(result.dense_shape) if result.dense_shape > 0 else "-"
                sparse_str = str(result.sparse_tokens) if result.sparse_tokens > 0 else "-"
                error_str = result.error_msg[:30] + "..." if len(result.error_msg) > 30 else result.error_msg

                print(f"{result.backend:<10} {time_str:<12} {success_str:<8} {dense_str:<8} {sparse_str:<8} {error_str}")

    def get_performance_summary(self):
        """Generate performance summary and recommendations."""
        print("\n" + "="*100)
        print("ðŸ† PERFORMANCE SUMMARY & RECOMMENDATIONS")
        print("="*100)

        # Calculate average performance by backend
        backend_stats = {}
        for result in self.results:
            if result.success_rate > 0:  # Only consider successful runs
                if result.backend not in backend_stats:
                    backend_stats[result.backend] = []
                backend_stats[result.backend].append(result.avg_time)

        print("\nðŸ“ˆ Average Performance (successful runs only):")
        print("-" * 50)
        for backend, times in backend_stats.items():
            avg_time = statistics.mean(times)
            print(f"{backend.upper():<10}: {avg_time:.3f}s average")

        # Find best backend for each scenario type
        print("\nðŸŽ¯ Best Backend by Scenario:")
        print("-" * 50)

        # Group results by scenario again for this function
        scenarios = {}
        for result in self.results:
            if result.scenario not in scenarios:
                scenarios[result.scenario] = []
            scenarios[result.scenario].append(result)

        scenario_winners = {}
        for scenario, results in scenarios.items():
            successful_results = [r for r in results if r.success_rate > 0]
            if successful_results:
                best = min(successful_results, key=lambda x: x.avg_time)
                scenario_winners[scenario] = best
                print(f"{scenario.replace('_', ' ').title():<20}: {best.backend.upper()} ({best.avg_time:.3f}s)")

        # Overall recommendation
        print("\nðŸ’¡ RECOMMENDATIONS:")
        print("-" * 50)

        # Check system capabilities
        has_cuda = False
        has_directml = False

        try:
            import torch
            has_cuda = torch.cuda.is_available()
        except ImportError:
            pass

        try:
            import onnxruntime as ort
            has_directml = "DmlExecutionProvider" in ort.get_available_providers()
        except ImportError:
            pass

        if has_cuda:
            print("âœ… NVIDIA GPU detected â†’ Use EMBEDDINGS_BACKEND=bge for best performance")
        elif has_directml:
            print("âœ… AMD GPU detected â†’ Use EMBEDDINGS_BACKEND=hybrid for optimal Windows/AMD setup")
        else:
            print("âœ… CPU only â†’ Use EMBEDDINGS_BACKEND=onnx for consistency")

        print(f"âœ… Set EMBEDDINGS_BACKEND=auto for automatic optimal selection")

if __name__ == "__main__":
    benchmark = EmbeddingsBenchmark()

    logger.info("Configuration:")
    logger.info(f"  EMBEDDINGS_BACKEND: {CONFIG.embeddings_backend}")
    logger.info(f"  EMBEDDING_DEVICE: {CONFIG.embedding_device}")
    logger.info(f"  USE_SPARSE: {CONFIG.use_sparse}")
    logger.info(f"  MAX_LENGTH_QUERY: {CONFIG.embedding_max_length_query}")
    logger.info(f"  MAX_LENGTH_DOC: {CONFIG.embedding_max_length_doc}")

    benchmark.run_comprehensive_benchmark()
    benchmark.print_results()
    benchmark.get_performance_summary()

    logger.info("ðŸŽ‰ Comprehensive embeddings benchmark completed!")
