#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from app.services.retrieval import hybrid_search
from app.services.bge_embeddings import embed_unified

def get_full_search_results():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    query = "–ö–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ —á–∞—Ç-—Ü–µ–Ω—Ç—Ä–µ"

    print(f"üîç –ü–û–õ–ù–´–ô –°–ü–ò–°–û–ö –î–û–ö–£–ú–ï–ù–¢–û–í –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
    print("=" * 80)

    try:
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        embeddings = embed_unified(query, return_dense=True, return_sparse=True)
        dense_vec = embeddings['dense_vecs'][0]
        sparse_vec = embeddings['sparse_vecs'][0]

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –±–æ–ª—å—à–∏–º –ª–∏–º–∏—Ç–æ–º
        results = hybrid_search(
            query_dense=dense_vec,
            query_sparse=sparse_vec,
            k=50  # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        )

        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
        print()

        for i, result in enumerate(results, 1):
            score = result.get('rrf_score', 0)
            boosted_score = result.get('boosted_score', score)
            payload = result.get('payload', {})

            title = payload.get('title', 'N/A')
            url = payload.get('url', 'N/A')
            page_type = payload.get('page_type', 'N/A')
            source = payload.get('source', 'N/A')
            content_length = payload.get('content_length', 0)
            indexed_via = payload.get('indexed_via', 'unknown')

            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            text = payload.get('text', '').lower()
            keywords = ['–∫–∞–Ω–∞–ª', 'telegram', 'whatsapp', 'viber', '–∞–≤–∏—Ç–æ', '–≤–µ–±-–≤–∏–¥–∂–µ—Ç', '–º–æ–±–∏–ª—å–Ω—ã–π', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è']
            keyword_matches = sum(1 for keyword in keywords if keyword in text)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_type = "üìÑ –û–±—ã—á–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"
            if 'blog' in url or '–≤–µ—Ä—Å–∏—è' in title.lower():
                doc_type = "üìù Release Notes"
            elif 'docs/start' in url or '—á—Ç–æ —Ç–∞–∫–æ–µ' in title.lower():
                doc_type = "üè† –ì–ª–∞–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            elif 'admin' in url or '–Ω–∞—Å—Ç—Ä–æ–π–∫–∞' in title.lower():
                doc_type = "‚öôÔ∏è –ê–¥–º–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            elif 'api' in url:
                doc_type = "üîß API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"

            print(f"{i:2d}. {doc_type}")
            print(f"    üìù –ù–∞–∑–≤–∞–Ω–∏–µ: {title}")
            print(f"    üîó URL: {url}")
            print(f"    üìä RRF Score: {score:.6f}")
            print(f"    üöÄ Boosted Score: {boosted_score:.6f}")
            print(f"    üìè –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {content_length}")
            print(f"    üè∑Ô∏è  –¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_type}")
            print(f"    üìç –ò—Å—Ç–æ—á–Ω–∏–∫: {source}")
            print(f"    üîÑ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {indexed_via}")
            print(f"    üéØ –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {keyword_matches}/8")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞
            text_preview = payload.get('text', '')[:200]
            print(f"    üìÑ –ü—Ä–µ–≤—å—é: {text_preview}...")

            # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            if keyword_matches >= 3:
                print(f"    ‚úÖ –í–´–°–û–ö–ê–Ø –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨")
            elif keyword_matches >= 2:
                print(f"    ‚ö†Ô∏è  –°–†–ï–î–ù–Ø–Ø –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨")
            else:
                print(f"    ‚ùå –ù–ò–ó–ö–ê–Ø –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨")

            print()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print("=" * 80)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ò–ü–ê–ú –î–û–ö–£–ú–ï–ù–¢–û–í:")

        doc_types = {}
        high_relevance = 0
        medium_relevance = 0
        low_relevance = 0

        for result in results:
            payload = result.get('payload', {})
            url = payload.get('url', '')
            title = payload.get('title', '')
            text = payload.get('text', '').lower()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø
            if 'blog' in url or '–≤–µ—Ä—Å–∏—è' in title.lower():
                doc_type = "Release Notes"
            elif 'docs/start' in url or '—á—Ç–æ —Ç–∞–∫–æ–µ' in title.lower():
                doc_type = "–ì–ª–∞–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            elif 'admin' in url:
                doc_type = "–ê–¥–º–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            elif 'api' in url:
                doc_type = "API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            else:
                doc_type = "–î—Ä—É–≥–æ–µ"

            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1

            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            keywords = ['–∫–∞–Ω–∞–ª', 'telegram', 'whatsapp', 'viber', '–∞–≤–∏—Ç–æ', '–≤–µ–±-–≤–∏–¥–∂–µ—Ç', '–º–æ–±–∏–ª—å–Ω—ã–π', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è']
            keyword_matches = sum(1 for keyword in keywords if keyword in text)

            if keyword_matches >= 3:
                high_relevance += 1
            elif keyword_matches >= 2:
                medium_relevance += 1
            else:
                low_relevance += 1

        print()
        for doc_type, count in doc_types.items():
            print(f"   {doc_type}: {count}")

        print()
        print("üìà –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨:")
        print(f"   ‚úÖ –í—ã—Å–æ–∫–∞—è (3+ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤): {high_relevance}")
        print(f"   ‚ö†Ô∏è  –°—Ä–µ–¥–Ω—è—è (2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞): {medium_relevance}")
        print(f"   ‚ùå –ù–∏–∑–∫–∞—è (0-1 –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ): {low_relevance}")

        # –¢–æ–ø-5 —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö
        print()
        print("üèÜ –¢–û–ü-5 –°–ê–ú–´–• –†–ï–õ–ï–í–ê–ù–¢–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í:")
        relevant_results = []

        for result in results:
            payload = result.get('payload', {})
            text = payload.get('text', '').lower()
            keywords = ['–∫–∞–Ω–∞–ª', 'telegram', 'whatsapp', 'viber', '–∞–≤–∏—Ç–æ', '–≤–µ–±-–≤–∏–¥–∂–µ—Ç', '–º–æ–±–∏–ª—å–Ω—ã–π', '–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è']
            keyword_matches = sum(1 for keyword in keywords if keyword in text)

            if keyword_matches >= 2:  # –¢–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å 2+ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
                relevant_results.append((result, keyword_matches))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        relevant_results.sort(key=lambda x: x[1], reverse=True)

        for i, (result, keyword_matches) in enumerate(relevant_results[:5], 1):
            payload = result.get('payload', {})
            title = payload.get('title', 'N/A')
            url = payload.get('url', 'N/A')
            score = result.get('rrf_score', 0)

            print(f"   {i}. {title}")
            print(f"      URL: {url}")
            print(f"      Score: {score:.6f}")
            print(f"      –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {keyword_matches}/8")
            print()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")

if __name__ == "__main__":
    get_full_search_results()
