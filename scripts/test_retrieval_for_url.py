"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –ø–æ–ø–∞–¥–∞–µ—Ç –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞.
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python scripts/test_retrieval_for_url.py
"""
import sys
sys.path.insert(0, '.')

from app.config import CONFIG
from app.services.core.embeddings import embed_unified
from app.services.search.retrieval import hybrid_search, client, COLLECTION
from qdrant_client.models import Filter, FieldCondition, MatchValue
from loguru import logger

def check_url_in_collection(target_url: str) -> list[dict]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ URL –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant."""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–∞ –ø–æ–ª—è: url –∏ canonical_url
        results_url = client.scroll(
            collection_name=COLLECTION,
            scroll_filter=Filter(
                must=[FieldCondition(key="url", match=MatchValue(value=target_url))]
            ),
            limit=100,
            with_payload=True,
            with_vectors=False
        )
        
        results_canonical = client.scroll(
            collection_name=COLLECTION,
            scroll_filter=Filter(
                must=[FieldCondition(key="canonical_url", match=MatchValue(value=target_url))]
            ),
            limit=100,
            with_payload=True,
            with_vectors=False
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        points = []
        if results_url and results_url[0]:
            points.extend(results_url[0])
        if results_canonical and results_canonical[0]:
            points.extend(results_canonical[0])
        
        return [
            {
                "id": point.id,
                "payload": point.payload
            }
            for point in points
        ]
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ URL –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
        return []


def test_retrieval_for_query(query: str, target_url: str, top_k: int = 30):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç retrieval –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–ø–∞–¥–∞–µ—Ç –ª–∏ —Ü–µ–ª–µ–≤–æ–π URL.

    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        target_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∫–æ—Ç–æ—Ä—É—é –∏—â–µ–º
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    print("=" * 80)
    print(f"üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê RETRIEVAL")
    print("=" * 80)
    print(f"–ó–∞–ø—Ä–æ—Å: {query}")
    print(f"–¶–µ–ª–µ–≤–æ–π URL: {target_url}")
    print(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ø-{top_k} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("=" * 80)

    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ URL –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤–æ–æ–±—â–µ
    print("\nüì¶ –®–ê–ì 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è URL –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
    print("-" * 80)

    indexed_chunks = check_url_in_collection(target_url)

    if not indexed_chunks:
        print(f"‚ùå URL –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {target_url}")
        print("   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("   - –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –±—ã–ª–∞ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞")
        print("   - URL –∑–∞–ø–∏—Å–∞–Ω –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
        print("   - –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä—É–≥–æ–µ –∏–º—è")

        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ URL
        print("\nüîé –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö URL...")
        try:
            all_points = client.scroll(
                collection_name=COLLECTION,
                limit=10,
                with_payload=True,
                with_vectors=False
            )
            if all_points[0]:
                print(f"   –ù–∞–π–¥–µ–Ω–æ {len(all_points[0])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–∏–º–µ—Ä—ã URL:")
                for i, point in enumerate(all_points[0][:5], 1):
                    url = point.payload.get('url', point.payload.get('site_url', 'N/A'))
                    print(f"   {i}. {url}")
        except Exception as e:
            print(f"   –û—à–∏–±–∫–∞: {e}")

        return False

    print(f"‚úÖ URL –Ω–∞–π–¥–µ–Ω! –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(indexed_chunks)}")
    for i, chunk in enumerate(indexed_chunks[:3], 1):
        payload = chunk['payload']
        print(f"\n   –ß–∞–Ω–∫ {i}:")
        print(f"   - ID: {chunk['id']}")
        print(f"   - chunk_index: {payload.get('chunk_index', 'N/A')}")
        print(f"   - title: {payload.get('title', 'N/A')[:80]}")
        text_preview = payload.get('text', '')[:150].replace('\n', ' ')
        print(f"   - text: {text_preview}...")

    # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    print("\n\nüß¨ –®–ê–ì 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")
    print("-" * 80)

    try:
        embedding_result = embed_unified(
            query,
            max_length=CONFIG.embedding_max_length_query,
            return_dense=True,
            return_sparse=CONFIG.use_sparse,
            return_colbert=False,
            context="query"
        )

        q_dense = embedding_result.get('dense_vecs', [[]])[0]

        q_sparse = {"indices": [], "values": []}
        if CONFIG.use_sparse and embedding_result.get('lexical_weights'):
            lex_weights = embedding_result['lexical_weights'][0]
            if lex_weights:
                q_sparse = {
                    "indices": [int(k) for k in lex_weights.keys()],
                    "values": [float(v) for k, v in lex_weights.items()]
                }

        print(f"‚úÖ Embeddings —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã:")
        print(f"   - Dense –≤–µ–∫—Ç–æ—Ä: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {len(q_dense)}")
        print(f"   - Sparse –≤–µ–∫—Ç–æ—Ä: {len(q_sparse.get('indices', []))} –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings: {e}")
        import traceback
        traceback.print_exc()
        return False

    # 3. –í—ã–ø–æ–ª–Ω—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
    print("\n\nüîé –®–ê–ì 3: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    print("-" * 80)

    try:
        results = hybrid_search(
            query_dense=q_dense,
            query_sparse=q_sparse,
            k=top_k
        )

        print(f"‚úÖ –ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω: –ø–æ–ª—É—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

    # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ü–µ–ª–µ–≤–æ–π URL –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    print("\n\nüìä –®–ê–ì 4: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("-" * 80)

    target_found = False
    target_positions = []

    for idx, hit in enumerate(results, 1):
        payload = hit.get('payload', {})
        url = payload.get('url', payload.get('canonical_url', payload.get('site_url', '')))
        
        if url == target_url:
            target_found = True
            target_positions.append(idx)

            print(f"\n‚úÖ –¶–ï–õ–ï–í–û–ô URL –ù–ê–ô–î–ï–ù –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ #{idx}!")
            print(f"   - Score (RRF): {hit.get('rrf_score', 'N/A'):.4f}")
            print(f"   - Score (Boosted): {hit.get('boosted_score', 'N/A'):.4f}")
            print(f"   - Title: {payload.get('title', 'N/A')}")
            print(f"   - chunk_index: {payload.get('chunk_index', 'N/A')}")
            text_preview = payload.get('text', '')[:200].replace('\n', ' ')
            print(f"   - Text: {text_preview}...")

    if not target_found:
        print(f"\n‚ùå –¶–ï–õ–ï–í–û–ô URL –ù–ï –ù–ê–ô–î–ï–ù –≤ —Ç–æ–ø-{top_k} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")
        print("\nüîù –¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")

        for idx, hit in enumerate(results[:5], 1):
            payload = hit.get('payload', {})
            url = payload.get('url', payload.get('canonical_url', payload.get('site_url', 'N/A')))
            title = payload.get('title', 'N/A')

            print(f"\n   #{idx} (score: {hit.get('boosted_score', 0):.4f}):")
            print(f"   - URL: {url}")
            print(f"   - Title: {title[:80]}")
            text_preview = payload.get('text', '')[:150].replace('\n', ' ')
            print(f"   - Text: {text_preview}...")

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω
        print("\n\nüí° –í–û–ó–ú–û–ñ–ù–´–ï –ü–†–ò–ß–ò–ù–´:")
        print("   1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ —Ç–µ–∫—Å—Ç–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        print("   2. –î—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–º–µ—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π score –ø–æ boosting")
        print("   3. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π k (–ø–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å top_k)")
        print("   4. –ü—Ä–æ–±–ª–µ–º–∞ —Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤)")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        query_words = set(query.lower().split())
        print("\n   üîç –ê–Ω–∞–ª–∏–∑ —á–∞–Ω–∫–æ–≤ —Ü–µ–ª–µ–≤–æ–≥–æ URL:")
        for i, chunk in enumerate(indexed_chunks[:3], 1):
            text = chunk['payload'].get('text', '').lower()
            matched_words = query_words & set(text.split())
            print(f"   –ß–∞–Ω–∫ {i}: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å–ª–æ–≤ = {len(matched_words)}/{len(query_words)}")
            if matched_words:
                print(f"           –°–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Å–ª–æ–≤–∞: {', '.join(list(matched_words)[:5])}")

    else:
        print(f"\n\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢: URL –Ω–∞–π–¥–µ–Ω –Ω–∞ –ø–æ–∑–∏—Ü–∏—è—Ö {target_positions}")
        if target_positions[0] <= 10:
            print("   üëç –û—Ç–ª–∏—á–Ω–æ! URL –≤ —Ç–æ–ø-10")
        elif target_positions[0] <= 20:
            print("   ‚ö†Ô∏è  URL –≤ —Ç–æ–ø-20, –Ω–æ –Ω–µ –≤ —Ç–æ–ø-10. –í–æ–∑–º–æ–∂–Ω–æ —Å—Ç–æ–∏—Ç —É–ª—É—á—à–∏—Ç—å boosting")
        else:
            print("   ‚ö†Ô∏è  URL –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Ç–æ–ø-20. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å boosting –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é")

    print("\n" + "=" * 80)
    return target_found


if __name__ == "__main__":
    # –í–∞—à —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    QUERY = "–∫–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã —è –º–æ–≥—É –ø–æ–¥–∫–ª—é—á–∏—Ç—å?"
    TARGET_URL = "https://docs-chatcenter.edna.ru/docs/start/whatis"
    TOP_K = 30  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ø-30 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    result = test_retrieval_for_query(QUERY, TARGET_URL, TOP_K)

    print("\n" + "=" * 80)
    if result:
        print("‚úÖ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê: URL –ù–ê–ô–î–ï–ù")
    else:
        print("‚ùå –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê: URL –ù–ï –ù–ê–ô–î–ï–ù –∏–ª–∏ –ù–ï –ü–†–û–ò–ù–î–ï–ö–°–ò–†–û–í–ê–ù")
    print("=" * 80)
