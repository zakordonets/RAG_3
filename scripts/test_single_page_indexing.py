#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
"""
import asyncio
import json
import sys
from pathlib import Path
from loguru import logger

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—é app
sys.path.append(str(Path(__file__).parent.parent))

from ingestion.crawler import crawl_seed
from ingestion.parsers import extract_main_text
from ingestion.chunker import chunk_text_with_overlap
from ingestion.indexer import upsert_chunks
from bs4 import BeautifulSoup


async def test_single_page_indexing():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""

    # URL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_url = "https://docs-chatcenter.edna.ru/docs/start/whatis"

    print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {test_url}")

    try:
        # 1. –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print("\n1Ô∏è‚É£ –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        pages = crawl_seed([test_url])

        if not pages:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return

        page = pages[0]
        html = page["html"]
        url = page["url"]

        print(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(html)} —Å–∏–º–≤–æ–ª–æ–≤ HTML")

        # 2. –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
        print("\n2Ô∏è‚É£ –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç...")
        soup = BeautifulSoup(html, 'html.parser')
        main_text = extract_main_text(soup)

        print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(main_text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
        print(f"   üìù –ü—Ä–µ–≤—å—é: {main_text[:200]}...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        has_russian = any(ord(c) > 127 for c in main_text[:500])
        print(f"   {'‚úÖ' if has_russian else '‚ùå'} –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç: {'–î–ê' if has_russian else '–ù–ï–¢'}")

        # 3. –ß–∞–Ω–∫–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        print("\n3Ô∏è‚É£ –ß–∞–Ω–∫–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç...")
        chunks = chunk_text_with_overlap(main_text, min_tokens=50, max_tokens=500)

        print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
        russian_chunks = 0
        for i, chunk in enumerate(chunks):
            if any(ord(c) > 127 for c in chunk[:200]):
                russian_chunks += 1
                print(f"   üìÑ –ß–∞–Ω–∫ {i+1}: {chunk[:100]}...")

        print(f"   {'‚úÖ' if russian_chunks > 0 else '‚ùå'} –†—É—Å—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤: {russian_chunks}/{len(chunks)}")

        # 4. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        print("\n4Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title_elem = soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_type = "guide"
        if "/api/" in url:
            page_type = "api"
        elif "/faq/" in url:
            page_type = "faq"

        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        chunks_to_index = []
        for i, chunk_text in enumerate(chunks):
            chunk_data = {
                "text": chunk_text,
                "payload": {
                    "url": url,
                    "page_type": page_type,
                    "source": "docs-site",
                    "language": "ru",
                    "title": title,
                    "text": chunk_text,
                    "chunk_index": i
                }
            }
            chunks_to_index.append(chunk_data)

        print(f"   ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(chunks_to_index)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

        # 5. –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç)
        if russian_chunks > 0:
            print("\n5Ô∏è‚É£ –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏...")
            try:
                indexed_count = upsert_chunks(chunks_to_index)
                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {indexed_count} —á–∞–Ω–∫–æ–≤")
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        else:
            print("\n5Ô∏è‚É£ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é - –Ω–µ—Ç —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

        # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        print("\n6Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
        from app.services.retrieval import client, COLLECTION
        from qdrant_client.models import Filter

        # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        results = client.scroll(
            collection_name=COLLECTION,
            scroll_filter=Filter(
                must=[
                    {'key': 'url', 'match': {'text': test_url}}
                ]
            ),
            limit=10,
            with_payload=True
        )

        found_docs = results[0]
        print(f"   üìä –ù–∞–π–¥–µ–Ω–æ {len(found_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –∏–Ω–¥–µ–∫—Å–µ")

        for i, doc in enumerate(found_docs, 1):
            payload = doc.payload
            content = str(payload.get("content", ""))
            has_russian = any(ord(c) > 127 for c in content[:200])
            print(f"   {i}. {'‚úÖ' if has_russian else '‚ùå'} {payload.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]}...")
            print(f"      –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {content[:100]}...")

        # 7. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
        print("\n7Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫...")
        from app.services.bge_embeddings import embed_unified
        from app.services.retrieval import hybrid_search
        from app.services.rerank import rerank

        test_query = "–ö–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ —á–∞—Ç-—Ü–µ–Ω—Ç—Ä–µ?"
        print(f"   üîç –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {test_query}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = embed_unified(test_query, return_dense=True, return_sparse=True)

        # –ü–æ–∏—Å–∫
        search_results = hybrid_search(
            query_dense=embeddings['dense_vecs'][0],
            query_sparse=embeddings['sparse_vecs'][0],
            k=10
        )

        # –†–µ—Ä–∞–Ω–∫–∏–Ω–≥
        reranked = rerank(test_query, search_results, top_n=5)

        print(f"   üìä –ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        print(f"   üìä –ü–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {len(reranked)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–∞—à–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        our_page_found = False
        for i, doc in enumerate(reranked, 1):
            doc_url = doc.get("payload", {}).get("url", "")
            if test_url in doc_url:
                our_page_found = True
                print(f"   ‚úÖ –ù–∞—à–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {i}")
                break

        if not our_page_found:
            print(f"   ‚ùå –ù–∞—à–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ù–ï –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        print("\n" + "="*60)
        print("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
        print("="*60)
        print(f"‚úÖ HTML –ø–æ–ª—É—á–µ–Ω: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"{'‚úÖ' if has_russian else '‚ùå'} –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω: {len(main_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"‚úÖ –ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {len(chunks)}")
        print(f"{'‚úÖ' if russian_chunks > 0 else '‚ùå'} –†—É—Å—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤: {russian_chunks}")
        print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {len(found_docs)}")
        print(f"{'‚úÖ' if our_page_found else '‚ùå'} –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ø–æ–∏—Å–∫–µ: {'–î–ê' if our_page_found else '–ù–ï–¢'}")

        if has_russian and russian_chunks > 0 and our_page_found:
            print("\nüéâ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –†–ê–ë–û–¢–ê–ï–¢ –ö–û–†–†–ï–ö–¢–ù–û!")
        else:
            print("\n‚ö†Ô∏è –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–†–û–ë–õ–ï–ú–´ –° –ò–ù–î–ï–ö–°–ê–¶–ò–ï–ô!")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    asyncio.run(test_single_page_indexing())
