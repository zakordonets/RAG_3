#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤—ã—è–≤–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π.
"""

import sys
from pathlib import Path
import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from loguru import logger

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent.parent))

from app.services.retrieval import client, COLLECTION
from app.services.optimized_pipeline import run_optimized_indexing
from ingestion.universal_loader import load_content_universal
from ingestion.crawl_cache import get_crawl_cache


@dataclass
class ValidationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö."""
    total_pages: int
    valid_pages: int
    empty_pages: int
    error_pages: int
    quality_score: float
    issues: List[str]
    recommendations: List[str]


class DataValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    
    def __init__(self):
        self.client = client
        self.collection = COLLECTION
        self.issues = []
        self.recommendations = []
    
    def validate_collection_data(self) -> ValidationResult:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant."""
        logger.info("üîç –ù–∞—á–∏–Ω–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_info = self.client.get_collection(self.collection)
            total_points = collection_info.points_count
            
            logger.info(f"üìä –í—Å–µ–≥–æ —Ç–æ—á–µ–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {total_points}")
            
            if total_points == 0:
                return ValidationResult(
                    total_pages=0,
                    valid_pages=0,
                    empty_pages=0,
                    error_pages=0,
                    quality_score=0.0,
                    issues=["–ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞"],
                    recommendations=["–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö"]
                )
            
            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            sample_size = min(100, total_points)
            logger.info(f"üìã –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã–±–æ—Ä–∫—É –∏–∑ {sample_size} —Ç–æ—á–µ–∫...")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ—á–∫–∏
            scroll_result = self.client.scroll(
                collection_name=self.collection,
                limit=sample_size,
                with_payload=True,
                with_vectors=False
            )
            
            points = scroll_result[0]
            validation_results = self._validate_points(points)
            
            # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            scale_factor = total_points / sample_size
            scaled_results = self._scale_results(validation_results, scale_factor)
            
            return scaled_results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            return ValidationResult(
                total_pages=0,
                valid_pages=0,
                empty_pages=0,
                error_pages=0,
                quality_score=0.0,
                issues=[f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}"],
                recommendations=["–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant"]
            )
    
    def _validate_points(self, points: List[Dict]) -> ValidationResult:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–æ—á–µ–∫."""
        total_pages = len(points)
        valid_pages = 0
        empty_pages = 0
        error_pages = 0
        
        for point in points:
            payload = point.get('payload', {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            if self._is_valid_point(payload):
                valid_pages += 1
            elif self._is_empty_point(payload):
                empty_pages += 1
            else:
                error_pages += 1
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        quality_score = valid_pages / total_pages if total_pages > 0 else 0.0
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—ã
        issues = self._analyze_issues(points)
        recommendations = self._generate_recommendations(issues, quality_score)
        
        return ValidationResult(
            total_pages=total_pages,
            valid_pages=valid_pages,
            empty_pages=empty_pages,
            error_pages=error_pages,
            quality_score=quality_score,
            issues=issues,
            recommendations=recommendations
        )
    
    def _is_valid_point(self, payload: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ—á–∫–∞ –≤–∞–ª–∏–¥–Ω–æ–π."""
        required_fields = ['text', 'url', 'title']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        for field in required_fields:
            if field not in payload or not payload[field]:
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞
        text = payload.get('text', '')
        if len(text.strip()) < 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
        url = payload.get('url', '')
        if not url.startswith('http'):
            return False
        
        return True
    
    def _is_empty_point(self, payload: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ—á–∫–∞ –ø—É—Å—Ç–æ–π."""
        text = payload.get('text', '')
        return len(text.strip()) < 10
    
    def _analyze_issues(self, points: List[Dict]) -> List[str]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã—Ö."""
        issues = []
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—è–º
        field_stats = {}
        text_lengths = []
        
        for point in points:
            payload = point.get('payload', {})
            
            # –°—á–∏—Ç–∞–µ–º –ø–æ–ª—è
            for field in payload.keys():
                field_stats[field] = field_stats.get(field, 0) + 1
            
            # –°—á–∏—Ç–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
            text = payload.get('text', '')
            text_lengths.append(len(text))
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è
        required_fields = ['text', 'url', 'title', 'content_type', 'section']
        for field in required_fields:
            if field not in field_stats:
                issues.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ '{field}' –≤ {len(points) - field_stats.get(field, 0)} —Ç–æ—á–∫–∞—Ö")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        if text_lengths:
            avg_length = sum(text_lengths) / len(text_lengths)
            short_texts = sum(1 for length in text_lengths if length < 50)
            
            if avg_length < 100:
                issues.append(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞: {avg_length:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
            
            if short_texts > len(text_lengths) * 0.1:  # –ë–æ–ª–µ–µ 10% –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
                issues.append(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤: {short_texts} –∏–∑ {len(text_lengths)}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_types = {}
        for point in points:
            payload = point.get('payload', {})
            content_type = payload.get('content_type', 'unknown')
            content_types[content_type] = content_types.get(content_type, 0) + 1
        
        if 'unknown' in content_types:
            issues.append(f"–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ {content_types['unknown']} —Ç–æ—á–∫–∞—Ö")
        
        return issues
    
    def _generate_recommendations(self, issues: List[str], quality_score: float) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é."""
        recommendations = []
        
        if quality_score < 0.8:
            recommendations.append("–ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω—É—é –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞")
        
        if any("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ" in issue for issue in issues):
            recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
        
        if any("–¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞" in issue for issue in issues):
            recommendations.append("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã chunking –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–æ–≤")
        
        if any("—Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞" in issue for issue in issues):
            recommendations.append("–£–ª—É—á—à–∏—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ universal_loader")
        
        if not recommendations:
            recommendations.append("–î–∞–Ω–Ω—ã–µ –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏, —Ä–µ–≥—É–ª—è—Ä–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ")
        
        return recommendations
    
    def _scale_results(self, results: ValidationResult, scale_factor: float) -> ValidationResult:
        """–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é."""
        return ValidationResult(
            total_pages=int(results.total_pages * scale_factor),
            valid_pages=int(results.valid_pages * scale_factor),
            empty_pages=int(results.empty_pages * scale_factor),
            error_pages=int(results.error_pages * scale_factor),
            quality_score=results.quality_score,
            issues=results.issues,
            recommendations=results.recommendations
        )
    
    def validate_crawl_cache(self) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–µ—à–∞ crawling."""
        logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–µ—à–∞ crawling...")
        
        try:
            cache = get_crawl_cache()
            stats = cache.get_cache_stats()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∫–µ—à–∞
            cached_urls = list(cache.get_cached_urls())[:10]  # –ü–µ—Ä–≤—ã–µ 10 URL
            
            valid_cached = 0
            empty_cached = 0
            
            for url in cached_urls:
                page = cache.get_page(url)
                if page:
                    if page.html and len(page.html.strip()) > 100:
                        valid_cached += 1
                    else:
                        empty_cached += 1
            
            return {
                "total_cached": stats['total_pages'],
                "cache_size_mb": stats['total_size_mb'],
                "valid_cached": valid_cached,
                "empty_cached": empty_cached,
                "cache_quality": valid_cached / (valid_cached + empty_cached) if (valid_cached + empty_cached) > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–µ—à–∞: {e}")
            return {"error": str(e)}
    
    def test_content_loading(self, test_urls: List[str]) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö URL."""
        logger.info(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è {len(test_urls)} URL...")
        
        results = {
            "total_tested": len(test_urls),
            "successful": 0,
            "failed": 0,
            "empty_content": 0,
            "errors": []
        }
        
        for url in test_urls:
            try:
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
                strategies = ['auto', 'jina', 'html']
                
                for strategy in strategies:
                    try:
                        result = load_content_universal(url, "", strategy)
                        
                        if result.get('content') and len(result['content'].strip()) > 50:
                            results["successful"] += 1
                            break
                        elif not result.get('content') or len(result['content'].strip()) < 10:
                            results["empty_content"] += 1
                        else:
                            results["failed"] += 1
                            
                    except Exception as e:
                        results["errors"].append(f"{url} ({strategy}): {e}")
                        continue
                        
            except Exception as e:
                results["errors"].append(f"{url}: {e}")
                results["failed"] += 1
        
        return results


def print_validation_report(result: ValidationResult, cache_stats: Dict, loading_test: Dict):
    """–í—ã–≤–æ–¥–∏—Ç –æ—Ç—á–µ—Ç –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    print("\n" + "="*80)
    print("üìä –û–¢–ß–ï–¢ –û –í–ê–õ–ò–î–ê–¶–ò–ò –î–ê–ù–ù–´–•")
    print("="*80)
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {result.total_pages}")
    print(f"   –í–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {result.valid_pages} ({result.valid_pages/result.total_pages*100:.1f}%)")
    print(f"   –ü—É—Å—Ç—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {result.empty_pages} ({result.empty_pages/result.total_pages*100:.1f}%)")
    print(f"   –û—à–∏–±–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {result.error_pages} ({result.error_pages/result.total_pages*100:.1f}%)")
    print(f"   –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {result.quality_score*100:.1f}%")
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∫–µ—à–∞
    if "error" not in cache_stats:
        print(f"\nüíæ –ö–ï–® CRAWLING:")
        print(f"   –ó–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {cache_stats['total_cached']}")
        print(f"   –†–∞–∑–º–µ—Ä –∫–µ—à–∞: {cache_stats['cache_size_mb']} MB")
        print(f"   –ö–∞—á–µ—Å—Ç–≤–æ –∫–µ—à–∞: {cache_stats['cache_quality']*100:.1f}%")
    
    # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏
    print(f"\nüß™ –¢–ï–°–¢ –ó–ê–ì–†–£–ó–ö–ò –ö–û–ù–¢–ï–ù–¢–ê:")
    print(f"   –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ URL: {loading_test['total_tested']}")
    print(f"   –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {loading_test['successful']}")
    print(f"   –ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç: {loading_test['empty_content']}")
    print(f"   –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏: {loading_test['failed']}")
    
    # –ü—Ä–æ–±–ª–µ–º—ã
    if result.issues:
        print(f"\n‚ö†Ô∏è –û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:")
        for i, issue in enumerate(result.issues, 1):
            print(f"   {i}. {issue}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if result.recommendations:
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        for i, rec in enumerate(result.recommendations, 1):
            print(f"   {i}. {rec}")
    
    # –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
    if loading_test['errors']:
        print(f"\n‚ùå –û–®–ò–ë–ö–ò –ó–ê–ì–†–£–ó–ö–ò:")
        for error in loading_test['errors'][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"   ‚Ä¢ {error}")
        if len(loading_test['errors']) > 5:
            print(f"   ... –∏ –µ—â–µ {len(loading_test['errors']) - 5} –æ—à–∏–±–æ–∫")
    
    print("\n" + "="*80)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    print("üöÄ –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n")
    
    validator = DataValidator()
    
    # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    print("1Ô∏è‚É£ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant...")
    collection_result = validator.validate_collection_data()
    
    # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–µ—à–∞
    print("2Ô∏è‚É£ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–µ—à–∞ crawling...")
    cache_stats = validator.validate_crawl_cache()
    
    # 3. –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    print("3Ô∏è‚É£ –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
    test_urls = [
        "https://docs-chatcenter.edna.ru/docs/start/whatis",
        "https://docs-chatcenter.edna.ru/docs/agent/quick-start",
        "https://docs-chatcenter.edna.ru/docs/api/index",
        "https://docs-chatcenter.edna.ru/faq",
        "https://docs-chatcenter.edna.ru/blog"
    ]
    loading_test = validator.test_content_loading(test_urls)
    
    # 4. –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
    print_validation_report(collection_result, cache_stats, loading_test)
    
    # 5. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
    overall_quality = collection_result.quality_score
    cache_quality = cache_stats.get('cache_quality', 0)
    loading_quality = loading_test['successful'] / loading_test['total_tested'] if loading_test['total_tested'] > 0 else 0
    
    overall_score = (overall_quality + cache_quality + loading_quality) / 3
    
    print(f"\nüéØ –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê: {overall_score*100:.1f}%")
    
    if overall_score >= 0.9:
        print("‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö!")
    elif overall_score >= 0.7:
        print("‚ö†Ô∏è –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è")
    elif overall_score >= 0.5:
        print("‚ùå –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è")
    else:
        print("üö® –ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è")
    
    return overall_score >= 0.7


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
