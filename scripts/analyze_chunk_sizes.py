#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from app.services.retrieval import client, COLLECTION

def analyze_chunk_sizes():
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    print("üîç –ê–ù–ê–õ–ò–ó –†–ê–ó–ú–ï–†–û–í –ß–ê–ù–ö–û–í –ò –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ò–ù–§–û–†–ú–ê–¶–ò–ò")
    print("=" * 80)

    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        all_points = client.scroll(
            collection_name=COLLECTION,
            limit=10000,  # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            with_payload=True,
            with_vectors=False
        )[0]

        print(f"üìä –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {len(all_points)}")

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤
        content_lengths = []
        chunk_indices = []

        for point in all_points:
            payload = point.payload or {}
            content_length = payload.get('content_length', 0)
            chunk_index = payload.get('chunk_index', 0)

            content_lengths.append(content_length)
            chunk_indices.append(chunk_index)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º
        print("\nüìè –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –†–ê–ó–ú–ï–†–ê–ú –ö–û–ù–¢–ï–ù–¢–ê:")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(content_lengths)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max(content_lengths)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {sum(content_lengths) / len(content_lengths):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {sorted(content_lengths)[len(content_lengths)//2]} —Å–∏–º–≤–æ–ª–æ–≤")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
        ranges = [
            (0, 500, "–û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ"),
            (500, 1000, "–ú–∞–ª–µ–Ω—å–∫–∏–µ"),
            (1000, 2000, "–°—Ä–µ–¥–Ω–∏–µ"),
            (2000, 5000, "–ë–æ–ª—å—à–∏–µ"),
            (5000, float('inf'), "–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ")
        ]

        print("\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –†–ê–ó–ú–ï–†–ê–ú:")
        for min_size, max_size, label in ranges:
            count = sum(1 for length in content_lengths if min_size <= length < max_size)
            percentage = (count / len(content_lengths)) * 100
            print(f"   {label} ({min_size}-{max_size if max_size != float('inf') else '‚àû'}): {count} ({percentage:.1f}%)")

        # –ê–Ω–∞–ª–∏–∑ —á–∞–Ω–∫–æ–≤ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        print("\nüîç –ê–ù–ê–õ–ò–ó –ß–ê–ù–ö–û–í –ü–û –î–û–ö–£–ú–ï–ù–¢–ê–ú:")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ URL
        docs_by_url = {}
        for point in all_points:
            payload = point.payload or {}
            url = payload.get('url', 'unknown')
            chunk_index = payload.get('chunk_index', 0)
            content_length = payload.get('content_length', 0)

            if url not in docs_by_url:
                docs_by_url[url] = []
            docs_by_url[url].append((chunk_index, content_length))

        # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏
        multi_chunk_docs = []
        for url, chunks in docs_by_url.items():
            if len(chunks) > 1:
                total_length = sum(length for _, length in chunks)
                multi_chunk_docs.append((url, len(chunks), total_length))

        multi_chunk_docs.sort(key=lambda x: x[1], reverse=True)

        print(f"\nüìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏: {len(multi_chunk_docs)}")
        print("\nüèÜ –¢–û–ü-10 –î–û–ö–£–ú–ï–ù–¢–û–í –° –ù–ê–ò–ë–û–õ–¨–®–ò–ú –ö–û–õ–ò–ß–ï–°–¢–í–û–ú –ß–ê–ù–ö–û–í:")

        for i, (url, chunk_count, total_length) in enumerate(multi_chunk_docs[:10], 1):
            title = url.split('/')[-1] or url.split('/')[-2] or url
            print(f"   {i:2d}. {title}")
            print(f"       URL: {url}")
            print(f"       –ß–∞–Ω–∫–æ–≤: {chunk_count}")
            print(f"       –û–±—â–∞—è –¥–ª–∏–Ω–∞: {total_length} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"       –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {total_length // chunk_count} —Å–∏–º–≤–æ–ª–æ–≤")
            print()

        # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö
        print("üîç –ü–û–ò–°–ö –î–û–ö–£–ú–ï–ù–¢–û–í –° –ò–ù–§–û–†–ú–ê–¶–ò–ï–ô –û –ö–ê–ù–ê–õ–ê–•:")

        channel_docs = []
        channel_keywords = ['telegram', 'whatsapp', 'viber', '–∞–≤–∏—Ç–æ', '–∫–∞–Ω–∞–ª', '–∫–∞–Ω–∞–ª—ã']

        for point in all_points:
            payload = point.payload or {}
            text = payload.get('text', '').lower()
            url = payload.get('url', '')
            title = payload.get('title', '')
            content_length = payload.get('content_length', 0)

            # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_matches = sum(1 for keyword in channel_keywords if keyword in text)

            if keyword_matches >= 2:  # –î–æ–∫—É–º–µ–Ω—Ç—ã —Å 2+ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏ –∫–∞–Ω–∞–ª–æ–≤
                channel_docs.append((url, title, keyword_matches, content_length, text[:200]))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        channel_docs.sort(key=lambda x: x[2], reverse=True)

        print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ {len(channel_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö:")

        for i, (url, title, keyword_count, length, preview) in enumerate(channel_docs[:15], 1):
            print(f"   {i:2d}. {title}")
            print(f"       URL: {url}")
            print(f"       –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {keyword_count}/6")
            print(f"       –î–ª–∏–Ω–∞: {length} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"       –ü—Ä–µ–≤—å—é: {preview}...")
            print()

        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ "–ß—Ç–æ —Ç–∞–∫–æ–µ edna Chat Center"
        print("üéØ –ê–ù–ê–õ–ò–ó –î–û–ö–£–ú–ï–ù–¢–ê '–ß—Ç–æ —Ç–∞–∫–æ–µ edna Chat Center':")

        whatis_chunks = []
        for point in all_points:
            payload = point.payload or {}
            if 'docs/start/whatis' in payload.get('url', ''):
                chunk_index = payload.get('chunk_index', 0)
                content_length = payload.get('content_length', 0)
                text = payload.get('text', '')
                whatis_chunks.append((chunk_index, content_length, text))

        if whatis_chunks:
            whatis_chunks.sort(key=lambda x: x[0])
            print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(whatis_chunks)}")

            total_length = sum(length for _, length, _ in whatis_chunks)
            print(f"   –û–±—â–∞—è –¥–ª–∏–Ω–∞: {total_length} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {total_length // len(whatis_chunks)} —Å–∏–º–≤–æ–ª–æ–≤")

            print("\n   üìÑ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —á–∞–Ω–∫–æ–≤:")
            for i, (chunk_index, length, text) in enumerate(whatis_chunks, 1):
                print(f"      –ß–∞–Ω–∫ {chunk_index}: {length} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"         {text[:100]}...")
                print()
        else:
            print("   ‚ùå –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")

        small_chunks = sum(1 for length in content_lengths if length < 1000)
        small_percentage = (small_chunks / len(content_lengths)) * 100

        if small_percentage > 50:
            print(f"   ‚ö†Ô∏è  {small_percentage:.1f}% —á–∞–Ω–∫–æ–≤ –º–µ–Ω—å—à–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ - —Å–ª–∏—à–∫–æ–º –º–µ–ª–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ")
            print("   üìù –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –£–≤–µ–ª–∏—á–∏—Ç—å CHUNK_MIN_TOKENS –∏ CHUNK_MAX_TOKENS")

        if len(multi_chunk_docs) > len(all_points) * 0.3:
            print(f"   ‚ö†Ô∏è  –ú–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–±–∏—Ç–æ –Ω–∞ —á–∞–Ω–∫–∏ ({len(multi_chunk_docs)} –∏–∑ {len(all_points)})")
            print("   üìù –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—É—â–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
        print("\n‚öôÔ∏è  –¢–ï–ö–£–©–ò–ï –ù–ê–°–¢–†–û–ô–ö–ò –ß–ê–ù–ö–ò–ù–ì–ê:")
        from app.config import CONFIG
        print(f"   CHUNK_MIN_TOKENS: {CONFIG.chunk_min_tokens}")
        print(f"   CHUNK_MAX_TOKENS: {CONFIG.chunk_max_tokens}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")

if __name__ == "__main__":
    analyze_chunk_sizes()
