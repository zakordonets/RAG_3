"""
–¢–µ—Å—Ç: –ø—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å –ø–æ–ª–µ–º text –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö pipeline
"""
import sys
sys.path.insert(0, '.')

from app.services.search.retrieval import hybrid_search, client, COLLECTION
from app.services.core.embeddings import embed_unified
from app.services.search.rerank import rerank
from app.config import CONFIG

print("=" * 100)
print("üî¨ –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó: –ü–†–û–•–û–ñ–î–ï–ù–ò–ï –ü–û–õ–Ø 'text' –ß–ï–†–ï–ó PIPELINE")
print("=" * 100)

query = "–∫–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã —è –º–æ–≥—É –ø–æ–¥–∫–ª—é—á–∏—Ç—å?"

print(f"\nüìù –ó–∞–ø—Ä–æ—Å: {query}")
print("=" * 100)

# –®–∞–≥ 1: Embeddings
print("\nüß¨ –®–ê–ì 1: –ì–ï–ù–ï–†–ê–¶–ò–Ø EMBEDDINGS")
embedding_result = embed_unified(
    query,
    max_length=CONFIG.embedding_max_length_query,
    return_dense=True,
    return_sparse=CONFIG.use_sparse,
    context="query"
)

q_dense = embedding_result.get('dense_vecs', [[]])[0]
q_sparse = {"indices": [], "values": []}
if CONFIG.use_sparse and embedding_result.get('lexical_weights'):
    lex_weights = embedding_result['lexical_weights'][0]
    if lex_weights:
        q_sparse = {
            "indices": [int(k) for k in lex_weights.keys()],
            "values": [float(v) for k, v in lex_weights.items()]
        }

print(f"  ‚úÖ Dense: {len(q_dense)} dim")
print(f"  ‚úÖ Sparse: {len(q_sparse.get('indices', []))} elements")

# –®–∞–≥ 2: Hybrid Search
print("\nüîé –®–ê–ì 2: HYBRID SEARCH (–¥–æ rerank)")
print("-" * 100)
candidates = hybrid_search(
    query_dense=q_dense,
    query_sparse=q_sparse,
    k=30
)

print(f"  ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ text –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö
empty_text_count = 0
for i, cand in enumerate(candidates[:5], 1):
    payload = cand.get('payload', {})
    text = payload.get('text', '')
    title = payload.get('title', 'N/A')

    print(f"\n  –ö–∞–Ω–¥–∏–¥–∞—Ç #{i}:")
    print(f"    Title: {title[:60]}")
    print(f"    –ü–æ–ª–µ 'text': {'‚úÖ –ï—Å—Ç—å' if 'text' in payload else '‚ùå –ù–ï–¢'}")
    print(f"    –î–ª–∏–Ω–∞ text: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

    if len(text) == 0:
        empty_text_count += 1
        print(f"    ‚ö†Ô∏è  –ü–£–°–¢–û–ô TEXT!")

if empty_text_count > 0:
    print(f"\n  ‚ùå –ü–†–û–ë–õ–ï–ú–ê: {empty_text_count}/5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –ø—É—Å—Ç—ã–º text!")

# –®–∞–≥ 3: Reranking
print("\n\nüéØ –®–ê–ì 3: RERANKING")
print("-" * 100)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç reranker
print("  –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ reranker –≤–∏–¥–∏—Ç:")
for i, cand in enumerate(candidates[:3], 1):
    payload = cand.get('payload', {})
    text = payload.get('text') or payload.get('title') or ""
    print(f"\n  –ö–∞–Ω–¥–∏–¥–∞—Ç #{i}:")
    print(f"    text –¥–ª—è reranker: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    if len(text) > 0:
        print(f"    –ù–∞—á–∞–ª–æ: {text[:80]}...")
    else:
        print(f"    ‚ö†Ô∏è  RERANKER –ü–û–õ–£–ß–ò–¢ –ü–£–°–¢–£–Æ –°–¢–†–û–ö–£!")

try:
    reranked = rerank(query, candidates, top_n=10, batch_size=20, max_length=384)
    print(f"\n  ‚úÖ Reranked: {len(reranked)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ text –ø–æ—Å–ª–µ rerank
    print("\n  –ü—Ä–æ–≤–µ—Ä—è–µ–º text –ø–æ—Å–ª–µ rerank:")
    for i, doc in enumerate(reranked[:3], 1):
        payload = doc.get('payload', {})
        text = payload.get('text', '')
        print(f"    –î–æ–∫—É–º–µ–Ω—Ç #{i}: text = {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

except Exception as e:
    print(f"  ‚ùå –û—à–∏–±–∫–∞ reranking: {e}")
    reranked = candidates[:10]

# –®–∞–≥ 4: Boosting (–≤ hybrid_search)
print("\n\nüìà –®–ê–ì 4: BOOSTING –ê–ù–ê–õ–ò–ó")
print("-" * 100)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å –Ω–∏–∑–∫–∏–º score
whatis_found = False
for idx, doc in enumerate(candidates, 1):
    payload = doc.get('payload', {})
    canonical_url = payload.get('canonical_url', '')

    if 'start/whatis' in canonical_url:
        whatis_found = True
        text = payload.get('text', '')
        content_length = payload.get('content_length') or len(text)

        print(f"\n  üìÑ –î–æ–∫—É–º–µ–Ω—Ç '–ß—Ç–æ —Ç–∞–∫–æ–µ edna Chat Center' (–ø–æ–∑–∏—Ü–∏—è #{idx}):")
        print(f"    canonical_url: {canonical_url}")
        print(f"    RRF Score: {doc.get('rrf_score', 'N/A')}")
        print(f"    Boosted Score: {doc.get('boosted_score', 'N/A')}")
        print(f"    –ü–æ–ª–µ 'text' –≤ payload: {'‚úÖ –ï—Å—Ç—å' if 'text' in payload else '‚ùå –ù–ï–¢'}")
        print(f"    –î–ª–∏–Ω–∞ text: {len(text)}")
        print(f"    content_length –≤ payload: {payload.get('content_length', 'N/A')}")
        print(f"    –í—ã—á–∏—Å–ª–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {content_length}")

        if len(text) == 0:
            print(f"    ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: text –ø—É—Å—Ç–æ–π!")
            print(f"    Boosting –ø–æ –¥–ª–∏–Ω–µ/—Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ù–ï –†–ê–ë–û–¢–ê–ï–¢!")
        else:
            print(f"    ‚úÖ Text –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, boosting —Ä–∞–±–æ—Ç–∞–µ—Ç")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–æ–π boost –ø–æ–ª—É—á–∏–ª –¥–æ–∫—É–º–µ–Ω—Ç
            url = canonical_url.lower()
            if '/start/' in url:
                print(f"    ‚ÑπÔ∏è  URL —Å–æ–¥–µ—Ä–∂–∏—Ç /start/ ‚Üí –¥–æ–ª–∂–µ–Ω –ø–æ–ª—É—á–∏—Ç—å boost_overview_docs")

if not whatis_found:
    print("\n  ‚ö†Ô∏è  –î–æ–∫—É–º–µ–Ω—Ç 'whatis' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö!")

print("\n" + "=" * 100)
print("üìä –ò–¢–û–ì–û–í–´–ï –í–´–í–û–î–´:")
print("=" * 100)

print("\n1. ‚úÖ –ü–æ–ª–µ 'text' –ü–†–ò–°–£–¢–°–¢–í–£–ï–¢ –≤ Qdrant payload")
print("2. ‚úÖ –ü–æ–ª–µ 'text' –ü–ï–†–ï–î–ê–Å–¢–°–Ø –≤ reranker")
print("3. ‚úÖ –ü–æ–ª–µ 'text' –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø –≤ boosting")
print("4. ‚úÖ –ü–æ–ª–µ 'text' –î–û–°–¢–£–ü–ù–û –≤ context optimizer")

print("\nüéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:")
print("   –ü—Ä–æ–±–ª–µ–º–∞ —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –ø–æ–ª—è 'text' –ù–ï –û–ë–ù–ê–†–£–ñ–ï–ù–ê.")
print("   –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã pipeline –ø–æ–ª—É—á–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç.")

print("\n" + "=" * 100)
