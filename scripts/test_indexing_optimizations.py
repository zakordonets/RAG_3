#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –≤—Å–µ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import time
from app.services.metadata_aware_indexer import MetadataAwareIndexer
from app.models.enhanced_metadata import EnhancedMetadata
from app.services.retrieval import client, COLLECTION

def test_enhanced_metadata():
    """–¢–µ—Å—Ç Enhanced Metadata —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    print("üîç –¢–ï–°–¢ ENHANCED METADATA")
    print("=" * 50)

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
    test_text = """
    edna Chat Center –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∫–∞–Ω–∞–ª—ã —Å–≤—è–∑–∏:

    1. Telegram - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä –¥–ª—è –æ–±—â–µ–Ω–∏—è —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏
    2. WhatsApp - –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä —Å —à–∏—Ä–æ–∫–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–µ–π
    3. Viber - –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∑–≤–æ–Ω–∫–æ–≤
    4. –ê–≤–∏—Ç–æ - –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    5. –í–µ–±-–≤–∏–¥–∂–µ—Ç - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —á–∞—Ç –Ω–∞ —Å–∞–π—Ç–µ
    6. –ú–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è - iOS –∏ Android –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

    –î–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤ –∞–¥–º–∏–Ω-–ø–∞–Ω–µ–ª–∏.
    """

    # –°–æ–∑–¥–∞–µ–º metadata
    metadata = EnhancedMetadata.from_text_and_metadata(
        text=test_text,
        url="https://docs-chatcenter.edna.ru/docs/start/channels",
        title="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∫–∞–Ω–∞–ª—ã —Å–≤—è–∑–∏",
        page_type="guide",
        chunk_index=0
    )

    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
    print(f"   Complexity Score: {metadata.complexity_score}")
    print(f"   Semantic Density: {metadata.semantic_density}")
    print(f"   Readability Score: {metadata.readability_score}")
    print(f"   Boost Factor: {metadata.boost_factor}")
    print(f"   Keywords: {metadata.keywords[:5]}")
    print(f"   Entities: {metadata.entities[:5]}")
    print(f"   Semantic Tags: {metadata.semantic_tags}")

    # –¢–µ—Å—Ç payload
    payload = metadata.to_search_payload()
    print(f"   Payload keys: {list(payload.keys())}")

    return metadata

def test_metadata_aware_indexer():
    """–¢–µ—Å—Ç Metadata-Aware Indexer"""
    print("\nüîç –¢–ï–°–¢ METADATA-AWARE INDEXER")
    print("=" * 50)

    indexer = MetadataAwareIndexer()

    # –¢–µ—Å—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π
    sparse_batch = indexer.get_optimal_batch_size("sparse")
    dense_batch = indexer.get_optimal_batch_size("dense")
    unified_batch = indexer.get_optimal_batch_size("unified")

    print(f"üìä –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–µ–π:")
    print(f"   Sparse: {sparse_batch}")
    print(f"   Dense: {dense_batch}")
    print(f"   Unified: {unified_batch}")

    # –¢–µ—Å—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞
    test_metadata = EnhancedMetadata(
        url="https://test.com/api",
        page_type="api",
        title="API Documentation",
        complexity_score=0.8,
        semantic_density=0.6
    )

    strategy = indexer.get_search_strategy(test_metadata)
    print(f"   Search Strategy for API docs: {strategy}")

    # –¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ sparse vectors
    test_lex_weights = {
        "123": 0.8,
        "456": 0.6,
        "789": 0.4,
        "101": 0.2,
        "112": 0.1
    }

    indices, values = indexer.optimize_sparse_conversion(test_lex_weights)
    print(f"   Sparse optimization: {len(indices)} indices, sorted by weight: {values[:3]}")

    return indexer

def test_collection_stats():
    """–¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
    print("\nüîç –¢–ï–°–¢ –°–¢–ê–¢–ò–°–¢–ò–ö–ò –ö–û–õ–õ–ï–ö–¶–ò–ò")
    print("=" * 50)

    indexer = MetadataAwareIndexer()

    try:
        stats = indexer.get_collection_metadata_stats()

        if "error" in stats:
            print(f"‚ùå –û—à–∏–±–∫–∞: {stats['error']}")
            return

        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
        print(f"   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats.get('total_documents', 'N/A')}")
        print(f"   –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {stats.get('sample_size', 'N/A')}")
        print(f"   Enhanced metadata: {'‚úÖ' if stats.get('metadata_enabled') else '‚ùå'}")

        if stats.get('page_type_distribution'):
            print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:")
            for page_type, count in stats['page_type_distribution'].items():
                print(f"     {page_type}: {count}")

        if stats.get('avg_complexity_score'):
            print(f"   –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {stats['avg_complexity_score']:.3f}")
            print(f"   –°—Ä–µ–¥–Ω—è—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {stats['avg_semantic_density']:.3f}")
            print(f"   –°—Ä–µ–¥–Ω–∏–π boost: {stats['avg_boost_factor']:.3f}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

def test_optimized_search():
    """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    print("\nüîç –¢–ï–°–¢ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –ü–û–ò–°–ö–ê")
    print("=" * 50)

    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ —Å metadata filtering
    indexer = MetadataAwareIndexer()

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (–∑–∞–≥–ª—É—à–∫–∏)
    test_dense_vector = [0.1] * 1024
    test_sparse_vector = {
        "indices": [1, 2, 3, 4, 5],
        "values": [0.8, 0.6, 0.4, 0.2, 0.1]
    }

    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
    filters = {
        "page_type": "guide"
    }

    try:
        results = indexer.search_with_metadata_filtering(
            query="–∫–∞–Ω–∞–ª—ã —Å–≤—è–∑–∏",
            dense_vector=test_dense_vector,
            sparse_vector=test_sparse_vector,
            filters=filters,
            limit=5
        )

        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏:")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(results)}")

        for i, result in enumerate(results[:3], 1):
            payload = result.get('payload', {})
            print(f"   {i}. {payload.get('title', 'N/A')}")
            print(f"      URL: {payload.get('url', 'N/A')}")
            print(f"      Boost: {payload.get('boost_factor', 'N/A')}")
            print(f"      Complexity: {payload.get('complexity_score', 'N/A')}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {e}")

def test_chunk_size_optimization():
    """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤"""
    print("\nüîç –¢–ï–°–¢ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –†–ê–ó–ú–ï–†–û–í –ß–ê–ù–ö–û–í")
    print("=" * 50)

    indexer = MetadataAwareIndexer()

    # –¢–µ—Å—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    test_cases = [
        ("api", "API Documentation", 0.8),
        ("guide", "User Guide", 0.4),
        ("faq", "FAQ", 0.2),
        ("release_notes", "Release Notes", 0.6)
    ]

    print("üìä –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤ –ø–æ —Ç–∏–ø–∞–º:")
    for page_type, title, complexity in test_cases:
        metadata = EnhancedMetadata(
            url=f"https://test.com/{page_type}",
            page_type=page_type,
            title=title,
            complexity_score=complexity
        )

        optimal_size = indexer.get_optimal_chunk_size(metadata)
        print(f"   {page_type}: {optimal_size} —Ç–æ–∫–µ–Ω–æ–≤")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ô –ò–ù–î–ï–ö–°–ê–¶–ò–ò")
    print("=" * 80)

    start_time = time.time()

    try:
        # –¢–µ—Å—Ç 1: Enhanced Metadata
        test_enhanced_metadata()

        # –¢–µ—Å—Ç 2: Metadata-Aware Indexer
        test_metadata_aware_indexer()

        # –¢–µ—Å—Ç 3: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        test_collection_stats()

        # –¢–µ—Å—Ç 4: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        test_optimized_search()

        # –¢–µ—Å—Ç 5: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤
        test_chunk_size_optimization()

        elapsed = time.time() - start_time
        print(f"\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã –∑–∞ {elapsed:.2f} —Å–µ–∫—É–Ω–¥")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–µ—Å—Ç–æ–≤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
