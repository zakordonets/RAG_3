#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
"""
import asyncio
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—é app
sys.path.append(str(Path(__file__).parent.parent))

from app.services.retrieval import client, COLLECTION
from app.services.bge_embeddings import embed_unified
from app.services.retrieval import hybrid_search
from app.services.rerank import rerank
from qdrant_client.models import Filter


async def verify_reindexing():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""

    print("üîç –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ü–ï–†–ï–ò–ù–î–ï–ö–°–ê–¶–ò–ò")
    print("="*50)

    try:
        # 1. –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n1Ô∏è‚É£ –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        info = client.get_collection(COLLECTION)
        total_docs = info.points_count
        print(f"   üìä –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {total_docs}")

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        print("\n2Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        results = client.scroll(
            collection_name=COLLECTION,
            limit=100,
            with_payload=True
        )

        docs = results[0]
        russian_docs = 0
        empty_docs = 0
        total_content_length = 0

        for doc in docs:
            content = str(doc.payload.get("content", ""))
            if content:
                total_content_length += len(content)
                if any(ord(c) > 127 for c in content[:200]):
                    russian_docs += 1
            else:
                empty_docs += 1

        russian_pct = (russian_docs / len(docs) * 100) if docs else 0
        avg_length = total_content_length / len(docs) if docs else 0

        print(f"   üá∑üá∫ –†—É—Å—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {russian_docs} ({russian_pct:.1f}%)")
        print(f"   üìÑ –ü—É—Å—Ç—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {empty_docs}")
        print(f"   üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        print("\n3Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤:")
        key_terms = ["–∫–∞–Ω–∞–ª", "telegram", "–≤–∏–¥–∂–µ—Ç", "—á–∞—Ç-—Ü–µ–Ω—Ç—Ä", "edna"]

        for term in key_terms:
            try:
                filter_result = client.scroll(
                    collection_name=COLLECTION,
                    scroll_filter=Filter(
                        must=[
                            {'key': 'content', 'match': {'text': term}}
                        ]
                    ),
                    limit=1,
                    with_payload=True
                )
                found = len(filter_result[0]) > 0
                print(f"   {'‚úÖ' if found else '‚ùå'} '{term}': {'–Ω–∞–π–¥–µ–Ω' if found else '–ù–ï –Ω–∞–π–¥–µ–Ω'}")
            except Exception as e:
                print(f"   ‚ùå '{term}': –æ—à–∏–±–∫–∞ - {e}")

        # 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞
        print("\n4Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞:")
        test_queries = [
            "–ö–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ —á–∞—Ç-—Ü–µ–Ω—Ç—Ä–µ?",
            "–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å Telegram –±–æ—Ç–∞?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –≤–µ–±-–≤–∏–¥–∂–µ—Ç?",
            "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç edna Chat Center?",
            "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏"
        ]

        for query in test_queries:
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                embeddings = embed_unified(query, return_dense=True, return_sparse=True)

                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
                search_results = hybrid_search(
                    query_dense=embeddings['dense_vecs'][0],
                    query_sparse=embeddings['sparse_vecs'][0],
                    k=10
                )

                # –†–µ—Ä–∞–Ω–∫–∏–Ω–≥
                reranked = rerank(query, search_results, top_n=5)

                # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
                quality_score = min(1.0, len(reranked) / 5.0)
                print(f"   {quality_score:.2f} {query}")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                for i, doc in enumerate(reranked[:3], 1):
                    title = doc.get("payload", {}).get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                    url = doc.get("payload", {}).get("url", "–ë–µ–∑ URL")
                    print(f"      {i}. {title[:50]}...")

            except Exception as e:
                print(f"   ‚ùå {query}: –æ—à–∏–±–∫–∞ - {e}")

        # 5. –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        print("\n5Ô∏è‚É£ –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞:")

        if russian_pct >= 80 and avg_length > 100:
            print("   ‚úÖ –ö–ê–ß–ï–°–¢–í–û: –û–¢–õ–ò–ß–ù–û!")
            print("   üéâ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ!")
        elif russian_pct >= 50 and avg_length > 50:
            print("   ‚ö†Ô∏è –ö–ê–ß–ï–°–¢–í–û: –•–û–†–û–®–û")
            print("   üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
        else:
            print("   ‚ùå –ö–ê–ß–ï–°–¢–í–û: –ü–õ–û–•–û")
            print("   üö® –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è")

        # 6. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("\n6Ô∏è‚É£ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")

        if russian_pct < 50:
            print("   üîÑ –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é")
        if avg_length < 100:
            print("   üìù –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞–Ω–∫–∏–Ω–≥–∞")
        if empty_docs > 0:
            print("   üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç–µ –ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")

        print("\n" + "="*50)
        print("‚úÖ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("="*50)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {e}")


if __name__ == "__main__":
    asyncio.run(verify_reindexing())
