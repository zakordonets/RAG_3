#!/usr/bin/env python3
"""
–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö —Å–≤—è–∑–∏
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from app.services.retrieval import client, COLLECTION
from app.services.retrieval import hybrid_search
from app.services.bge_embeddings import embed_unified

def find_channels_info():
    """–ù–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö —Å–≤—è–∑–∏"""
    print("üîç –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö —Å–≤—è–∑–∏")
    print("=" * 70)

    # –ó–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–∞–Ω–∞–ª–∞—Ö
    queries = [
        "–ö–∞–∫–∏–µ –∫–∞–Ω–∞–ª—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ —á–∞—Ç-—Ü–µ–Ω—Ç—Ä–µ",
        "Telegram –∫–∞–Ω–∞–ª –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ",
        "WhatsApp –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è",
        "Viber –∫–∞–Ω–∞–ª",
        "–ê–≤–∏—Ç–æ –∫–∞–Ω–∞–ª",
        "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∫–∞–Ω–∞–ª—ã —Å–≤—è–∑–∏",
        "–∫–∞–Ω–∞–ª—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —á–∞—Ç-—Ü–µ–Ω—Ç—Ä—É",
        "–≤–µ–±-–≤–∏–¥–∂–µ—Ç –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∫–∞–Ω–∞–ª—ã"
    ]

    found_documents = {}

    for query in queries:
        print(f"\nüîé –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")

        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            embeddings = embed_unified(query, return_dense=True, return_sparse=True)
            dense_vec = embeddings['dense_vecs'][0]
            sparse_vec = embeddings['sparse_vecs'][0]

            # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            results = hybrid_search(
                query_dense=dense_vec,
                query_sparse=sparse_vec,
                k=10  # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            )

            print(f"   üìä –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")

            for i, result in enumerate(results[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5
                score = result.get('rrf_score', 0)
                payload = result.get('payload', {})
                title = payload.get('title', 'N/A')
                url = payload.get('url', 'N/A')
                text = payload.get('text', '')
                content_length = payload.get('content_length', 0)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                keywords = ['–∫–∞–Ω–∞–ª', 'telegram', 'whatsapp', 'viber', '–∞–≤–∏—Ç–æ', '–≤–µ–±-–≤–∏–¥–∂–µ—Ç', '–º–æ–±–∏–ª—å–Ω—ã–π']
                keyword_matches = sum(1 for keyword in keywords if keyword in text.lower())

                print(f"      {i+1}. Score: {score:.4f} | Keywords: {keyword_matches}/7 | Length: {content_length}")
                print(f"         Title: {title}")
                print(f"         URL: {url}")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                doc_key = f"{title}|{url}"
                if doc_key not in found_documents:
                    found_documents[doc_key] = {
                        'title': title,
                        'url': url,
                        'text': text,
                        'content_length': content_length,
                        'keyword_matches': keyword_matches,
                        'scores': []
                    }
                found_documents[doc_key]['scores'].append(score)

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞
                if keyword_matches > 0:
                    sentences = text.split('.')
                    relevant_sentences = []
                    for sentence in sentences:
                        if any(keyword in sentence.lower() for keyword in keywords):
                            relevant_sentences.append(sentence.strip())

                    if relevant_sentences:
                        print(f"         –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:")
                        for sentence in relevant_sentences[:2]:
                            print(f"            ‚Ä¢ {sentence[:100]}...")
                print()

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")

    # –ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("\n" + "=" * 70)
    print("üìä –ê–ù–ê–õ–ò–ó –ù–ê–ô–î–ï–ù–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í:")
    print()

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    sorted_docs = sorted(found_documents.items(),
                        key=lambda x: x[1]['keyword_matches'],
                        reverse=True)

    print("üèÜ –¢–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö:")
    for i, (doc_key, doc_info) in enumerate(sorted_docs[:10]):
        avg_score = sum(doc_info['scores']) / len(doc_info['scores'])
        print(f"   {i+1}. {doc_info['title']}")
        print(f"      URL: {doc_info['url']}")
        print(f"      –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {doc_info['keyword_matches']}/7")
        print(f"      –°—Ä–µ–¥–Ω–∏–π score: {avg_score:.4f}")
        print(f"      –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {doc_info['content_length']}")
        print()

    # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö
    print("üîç –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö...")

    all_results = client.scroll(
        collection_name=COLLECTION,
        limit=1000,
        with_payload=True,
        with_vectors=False
    )

    comprehensive_docs = []
    for point in all_results[0]:
        text = point.payload.get('text', '').lower()

        # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –º–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–æ–≤
        channels_found = []
        if 'telegram' in text:
            channels_found.append('telegram')
        if 'whatsapp' in text:
            channels_found.append('whatsapp')
        if 'viber' in text:
            channels_found.append('viber')
        if '–∞–≤–∏—Ç–æ' in text or 'avito' in text:
            channels_found.append('–∞–≤–∏—Ç–æ')
        if '–≤–µ–±-–≤–∏–¥–∂–µ—Ç' in text:
            channels_found.append('–≤–µ–±-–≤–∏–¥–∂–µ—Ç')
        if '–º–æ–±–∏–ª—å–Ω—ã–π' in text:
            channels_found.append('–º–æ–±–∏–ª—å–Ω—ã–π')

        if len(channels_found) >= 2:  # –î–æ–∫—É–º–µ–Ω—Ç—ã —Å 2+ –∫–∞–Ω–∞–ª–∞–º–∏
            comprehensive_docs.append({
                'title': point.payload.get('title', 'N/A'),
                'url': point.payload.get('url', 'N/A'),
                'channels': channels_found,
                'content_length': point.payload.get('content_length', 0)
            })

    if comprehensive_docs:
        print(f"\nüìã –î–æ–∫—É–º–µ–Ω—Ç—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–Ω–∞–ª–∞—Ö ({len(comprehensive_docs)} –Ω–∞–π–¥–µ–Ω–æ):")
        for doc in comprehensive_docs[:5]:
            print(f"   ‚Ä¢ {doc['title']}")
            print(f"     URL: {doc['url']}")
            print(f"     –ö–∞–Ω–∞–ª—ã: {', '.join(doc['channels'])}")
            print(f"     –î–ª–∏–Ω–∞: {doc['content_length']}")
            print()
    else:
        print(f"\n‚ùå –î–æ–∫—É–º–µ–Ω—Ç—ã —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

if __name__ == "__main__":
    find_channels_info()
