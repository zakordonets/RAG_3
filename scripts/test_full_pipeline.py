#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π end-to-end —Ç–µ—Å—Ç: –æ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–æ –∑–∞–ø–∏—Å–∏ –≤ Qdrant
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import time
from app.abstractions.data_source import plugin_manager
from app.services.optimized_pipeline import OptimizedPipeline
from app.services.metadata_aware_indexer import MetadataAwareIndexer
from ingestion.chunker import chunk_text
from app.config import CONFIG

# Import data sources
from app.sources import edna_docs_source


def test_full_pipeline_single_document():
    """–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç pipeline –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print("üîç –ü–û–õ–ù–´–ô –¢–ï–°–¢ PIPELINE - –û–î–ò–ù –î–û–ö–£–ú–ï–ù–¢")
    print("=" * 60)

    try:
        # –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        print("üìÑ –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
        edna_config = {
            "base_url": "https://docs-chatcenter.edna.ru/",
            "strategy": "jina",
            "use_cache": False,  # –ë–µ–∑ –∫–µ—à–∞ –¥–ª—è —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            "max_pages": 1
        }

        source = plugin_manager.get_source("edna_docs", edna_config)
        crawl_result = source.fetch_pages(max_pages=1)

        if not crawl_result.pages:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–æ–∫—É–º–µ–Ω—Ç—ã")
            return False

        page = crawl_result.pages[0]
        print(f"   ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –∏–∑–≤–ª–µ—á–µ–Ω:")
        print(f"      URL: {page.url}")
        print(f"      –ó–∞–≥–æ–ª–æ–≤–æ–∫: {page.title}")
        print(f"      –¢–∏–ø: {page.page_type.value}")
        print(f"      –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(page.content)} —Å–∏–º–≤–æ–ª–æ–≤")

        if len(page.content) == 0:
            print("   ‚ùå –ö–æ–Ω—Ç–µ–Ω—Ç –ø—É—Å—Ç–æ–π!")
            return False

        print(f"      –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {page.content[:200]}...")

        # –®–∞–≥ 2: Chunking
        print(f"\nüîß –®–∞–≥ 2: Chunking")
        chunks = chunk_text(page.content)
        print(f"   ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")

        if len(chunks) == 0:
            print("   ‚ùå –ß–∞–Ω–∫–∏ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!")
            return False

        for i, chunk in enumerate(chunks[:3]):
            print(f"      –ß–∞–Ω–∫ {i+1}: {len(chunk)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"         –ù–∞—á–∞–ª–æ: {chunk[:100]}...")

        # –®–∞–≥ 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        print(f"\nüìù –®–∞–≥ 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        chunks_for_indexing = []
        for i, chunk_text_content in enumerate(chunks):
            chunk = {
                "text": chunk_text_content,
                "payload": {
                    "url": page.url,
                    "title": page.title,
                    "page_type": page.page_type.value,
                    "source": page.source,
                    "language": page.language,
                    "chunk_index": i,
                    "content_length": len(chunk_text_content),
                    "test_run": True,  # –ú–∞—Ä–∫–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                    **page.metadata
                }
            }
            chunks_for_indexing.append(chunk)

        print(f"   ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(chunks_for_indexing)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

        # –®–∞–≥ 4: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant
        print(f"\nüóÑÔ∏è –®–∞–≥ 4: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant")
        indexer = MetadataAwareIndexer()

        start_time = time.time()
        indexed_count = indexer.index_chunks_with_metadata(chunks_for_indexing)
        indexing_time = time.time() - start_time

        print(f"   ‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        print(f"      –ó–∞–ø–∏—Å–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {indexed_count}")
        print(f"      –í—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {indexing_time:.2f}s")

        if indexed_count == 0:
            print("   ‚ùå –ß–∞–Ω–∫–∏ –Ω–µ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ Qdrant!")
            return False

        # –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ Qdrant
        print(f"\nüîç –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ Qdrant")
        from app.services.retrieval import client, COLLECTION

        # –ò—â–µ–º –Ω–∞—à–∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        search_result = client.scroll(
            collection_name=COLLECTION,
            scroll_filter={"must": [{"key": "test_run", "match": {"value": True}}]},
            limit=10,
            with_payload=True,
            with_vectors=False
        )

        found_chunks = search_result[0]
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ Qdrant: {len(found_chunks)}")

        if found_chunks:
            test_chunk = found_chunks[0]
            payload = test_chunk.payload
            print(f"      URL: {payload.get('url')}")
            print(f"      –ó–∞–≥–æ–ª–æ–≤–æ–∫: {payload.get('title')}")
            print(f"      Chunk index: {payload.get('chunk_index')}")
            print(f"      Content length: {payload.get('content_length')}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º enhanced metadata
            if 'complexity_score' in payload:
                print(f"      Enhanced metadata: ‚úÖ")
                print(f"         Complexity: {payload.get('complexity_score')}")
                print(f"         Semantic density: {payload.get('semantic_density')}")
                print(f"         Boost factor: {payload.get('boost_factor')}")
            else:
                print(f"      Enhanced metadata: ‚ùå")

        # –®–∞–≥ 6: –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print(f"\nüßπ –®–∞–≥ 6: –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        if found_chunks:
            test_ids = [str(chunk.id) for chunk in found_chunks]
            client.delete(
                collection_name=COLLECTION,
                points_selector=test_ids
            )
            print(f"   ‚úÖ –£–¥–∞–ª–µ–Ω–æ {len(test_ids)} —Ç–µ—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤")

        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ pipeline: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_optimized_pipeline_end_to_end():
    """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ pipeline end-to-end"""
    print("\nüîç –¢–ï–°–¢ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û PIPELINE END-TO-END")
    print("=" * 60)

    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π pipeline
        from app.services.optimized_pipeline import run_optimized_indexing
        result = run_optimized_indexing(
            source_name="edna_docs",
            max_pages=3,  # –ù–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —Ç–µ—Å—Ç–∞
            chunk_strategy="adaptive"
        )

        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª–Ω–æ–≥–æ pipeline:")
        print(f"   –£—Å–ø–µ—Ö: {'‚úÖ' if result['success'] else '‚ùå'}")
        print(f"   –°—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result.get('pages', 0)}")
        print(f"   –ß–∞–Ω–∫–æ–≤ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {result.get('chunks', 0)}")
        print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {result.get('duration', 0):.2f}s")
        print(f"   –û—à–∏–±–∫–∏: {result.get('errors', 0)}")

        if result['success'] and result.get('chunks', 0) > 0:
            print(f"   ‚úÖ Pipeline —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            indexer = MetadataAwareIndexer()
            stats = indexer.get_collection_metadata_stats()

            print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
            print(f"   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats.get('total_documents', 'N/A')}")
            print(f"   Enhanced metadata: {'‚úÖ' if stats.get('metadata_enabled') else '‚ùå'}")

            return True
        else:
            print(f"   ‚ùå Pipeline –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
            return False

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ pipeline: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_chunking_quality():
    """–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ chunking"""
    print("\nüîç –¢–ï–°–¢ –ö–ê–ß–ï–°–¢–í–ê CHUNKING")
    print("=" * 60)

    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ö–æ—Ä–æ—à–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        edna_config = {
            "base_url": "https://docs-chatcenter.edna.ru/",
            "strategy": "jina",
            "use_cache": False,
            "max_pages": 1
        }

        source = plugin_manager.get_source("edna_docs", edna_config)
        crawl_result = source.fetch_pages(max_pages=1)

        if not crawl_result.pages or len(crawl_result.pages[0].content) == 0:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º")
            return False

        page = crawl_result.pages[0]
        print(f"üìÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º chunking –¥–ª—è: {page.title}")
        print(f"   –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(page.content)} —Å–∏–º–≤–æ–ª–æ–≤")

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ chunking
        strategies = ["adaptive", "standard"]

        for strategy in strategies:
            print(f"\nüîß –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")

            pipeline = OptimizedPipeline()

            if strategy == "adaptive":
                chunks = pipeline._adaptive_chunk_page(page)
            else:
                chunks = pipeline._standard_chunk_page(page)

            print(f"   –ß–∞–Ω–∫–æ–≤: {len(chunks)}")

            if chunks:
                total_chars = sum(len(chunk['text']) for chunk in chunks)
                avg_chars = total_chars / len(chunks)

                print(f"   –û–±—â–∞—è –¥–ª–∏–Ω–∞: {total_chars} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞: {avg_chars:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ chunking
                if avg_chars < 100:
                    print(f"   ‚ö†Ô∏è –ß–∞–Ω–∫–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ")
                elif avg_chars > 2000:
                    print(f"   ‚ö†Ô∏è –ß–∞–Ω–∫–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ")
                else:
                    print(f"   ‚úÖ –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤
                for i, chunk in enumerate(chunks[:2]):
                    print(f"      –ß–∞–Ω–∫ {i+1}: {len(chunk['text'])} —Å–∏–º–≤–æ–ª–æ–≤")
                    print(f"         –ù–∞—á–∞–ª–æ: {chunk['text'][:100]}...")
            else:
                print(f"   ‚ùå –ß–∞–Ω–∫–∏ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
                return False

        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è chunking: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ü–û–õ–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï PIPELINE")
    print("=" * 80)

    results = []

    # –¢–µ—Å—Ç 1: –ü–æ–ª–Ω—ã–π pipeline –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    result1 = test_full_pipeline_single_document()
    results.append(("–ü–æ–ª–Ω—ã–π pipeline", result1))

    # –¢–µ—Å—Ç 2: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π pipeline
    result2 = test_optimized_pipeline_end_to_end()
    results.append(("–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π pipeline", result2))

    # –¢–µ—Å—Ç 3: –ö–∞—á–µ—Å—Ç–≤–æ chunking
    result3 = test_chunking_quality()
    results.append(("–ö–∞—á–µ—Å—Ç–≤–æ chunking", result3))

    # –ò—Ç–æ–≥–∏
    print("\n" + "=" * 80)
    print("üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 80)

    for test_name, result in results:
        status = "‚úÖ" if result else "‚ùå"
        print(f"{status} {test_name}: {'OK' if result else '–ü—Ä–æ–±–ª–µ–º–∞'}")

    successful_tests = sum(1 for _, result in results if result)
    print(f"\nüìà –ü—Ä–æ–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {successful_tests}/{len(results)}")

    if successful_tests == len(results):
        print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã! Pipeline —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞.")
    else:
        print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã –≤ pipeline, —Ç—Ä–µ–±—É—é—â–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.")


if __name__ == "__main__":
    main()
