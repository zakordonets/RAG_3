#!/usr/bin/env python3
"""
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–µ—à–µ–º crawling.
"""

from __future__ import annotations

import sys
import os
import argparse
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from loguru import logger
from ingestion.crawl_cache import get_crawl_cache


def show_cache_stats():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–µ—à–∞."""
    cache = get_crawl_cache()
    stats = cache.get_cache_stats()
    
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞ crawling:")
    print(f"  –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {stats['total_pages']}")
    print(f"  –†–∞–∑–º–µ—Ä –∫–µ—à–∞: {stats['total_size_mb']} MB")
    print(f"  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞: {stats['cache_dir']}")
    print(f"  –§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞: {stats['index_file']}")
    
    if stats['total_pages'] > 0:
        print("\nüìã –ó–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ URL:")
        urls = cache.get_cached_urls()
        for i, url in enumerate(sorted(urls), 1):
            cached_page = cache.get_page(url)
            if cached_page:
                print(f"  {i:3d}. {url}")
                print(f"       –¢–∏–ø: {cached_page.page_type}, –†–∞–∑–º–µ—Ä: {cached_page.content_length} –±–∞–π—Ç")
                print(f"       –ö–µ—à–∏—Ä–æ–≤–∞–Ω: {cached_page.cached_at}")


def clear_cache():
    """–û—á–∏—â–∞–µ—Ç –≤–µ—Å—å –∫–µ—à."""
    cache = get_crawl_cache()
    
    stats = cache.get_cache_stats()
    if stats['total_pages'] == 0:
        print("‚úÖ –ö–µ—à —É–∂–µ –ø—É—Å—Ç")
        return
    
    print(f"‚ö†Ô∏è  –í—ã —Å–æ–±–∏—Ä–∞–µ—Ç–µ—Å—å —É–¥–∞–ª–∏—Ç—å {stats['total_pages']} –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü ({stats['total_size_mb']} MB)")
    
    confirm = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/N): ").lower().strip()
    if confirm not in ['y', 'yes', '–¥–∞']:
        print("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
        return
    
    cache.clear_cache()
    print("‚úÖ –ö–µ—à –æ—á–∏—â–µ–Ω")


def validate_cache():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∫–µ—à–∞."""
    cache = get_crawl_cache()
    
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∫–µ—à–∞...")
    
    cached_urls = cache.get_cached_urls()
    valid_count = 0
    invalid_count = 0
    
    for url in cached_urls:
        try:
            cached_page = cache.get_page(url)
            if cached_page and cached_page.html:
                valid_count += 1
            else:
                invalid_count += 1
                print(f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
        except Exception as e:
            invalid_count += 1
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {url} - {e}")
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏:")
    print(f"  ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {valid_count}")
    print(f"  ‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {invalid_count}")
    
    if invalid_count > 0:
        print(f"\n‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {invalid_count} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
        confirm = input("–£–¥–∞–ª–∏—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏? (y/N): ").lower().strip()
        if confirm in ['y', 'yes', '–¥–∞']:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –æ—á–∏—Å—Ç–∫–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            print("üîß –û—á–∏—Å—Ç–∫–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞")


def test_cache_performance():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–µ—à–∞."""
    import time
    from ingestion.crawler import crawl_sitemap
    
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–µ—à–∞...")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    urls = crawl_sitemap()
    if not urls:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å URL –∏–∑ sitemap")
        return
    
    test_urls = urls[:10]  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 10 URL
    print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {len(test_urls)} URL")
    
    cache = get_crawl_cache()
    
    # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫–µ—à–∞
    print("\nüìñ –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫–µ—à–∞...")
    start_time = time.time()
    
    loaded_count = 0
    for url in test_urls:
        cached_page = cache.get_page(url)
        if cached_page:
            loaded_count += 1
    
    cache_time = time.time() - start_time
    
    print(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫–µ—à–∞: {loaded_count}/{len(test_urls)} —Å—Ç—Ä–∞–Ω–∏—Ü")
    print(f"  –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {cache_time:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"  –°–∫–æ—Ä–æ—Å—Ç—å: {loaded_count/cache_time:.1f} —Å—Ç—Ä–∞–Ω–∏—Ü/—Å–µ–∫" if cache_time > 0 else "  –°–∫–æ—Ä–æ—Å—Ç—å: –º–≥–Ω–æ–≤–µ–Ω–Ω–æ")


def rebuild_index():
    """–ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∫–µ—à–∞."""
    cache = get_crawl_cache()
    
    print("üîß –ü–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∫–µ—à–∞...")
    
    # –°–∫–∞–Ω–∏—Ä—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å —Ñ–∞–π–ª–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
    pages_dir = cache.pages_dir
    if not pages_dir.exists():
        print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    page_files = list(pages_dir.glob("*.json"))
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(page_files)} —Ñ–∞–π–ª–æ–≤ –∫–µ—à–∞")
    
    if len(page_files) == 0:
        print("‚úÖ –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∏ –∏–Ω–¥–µ–∫—Å–∞")
        return
    
    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å
    cache._index.clear()
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ —Ñ–∞–π–ª–æ–≤
    rebuilt_count = 0
    for page_file in page_files:
        try:
            import json
            with open(page_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            metadata = data.get('metadata', {})
            if 'url' in metadata:
                from ingestion.crawl_cache import PageCache
                page_cache = PageCache(**metadata)
                page_cache.html = ""  # –ù–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ –ø–∞–º—è—Ç—å
                page_cache.text = ""
                cache._index[page_cache.url] = page_cache
                rebuilt_count += 1
        except Exception as e:
            logger.warning(f"Failed to rebuild index for {page_file}: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    cache._save_index()
    
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω: {rebuilt_count} —Å—Ç—Ä–∞–Ω–∏—Ü")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(description="–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–µ—à–µ–º crawling")
    
    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥—ã')
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    subparsers.add_parser('stats', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–µ—à–∞')
    
    # –û—á–∏—Å—Ç–∫–∞
    subparsers.add_parser('clear', help='–û—á–∏—Å—Ç–∏—Ç—å –≤–µ—Å—å –∫–µ—à')
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    subparsers.add_parser('validate', help='–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∫–µ—à–∞')
    
    # –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    subparsers.add_parser('test', help='–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–µ—à–∞')
    
    # –ü–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω–¥–µ–∫—Å–∞
    subparsers.add_parser('rebuild', help='–ü–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∫–µ—à–∞')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        if args.command == 'stats':
            show_cache_stats()
        elif args.command == 'clear':
            clear_cache()
        elif args.command == 'validate':
            validate_cache()
        elif args.command == 'test':
            test_cache_performance()
        elif args.command == 'rebuild':
            rebuild_index()
        else:
            parser.print_help()
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {type(e).__name__}: {e}")
        raise


if __name__ == "__main__":
    main()


