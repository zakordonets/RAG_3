#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è crawling.
"""

from __future__ import annotations

import sys
import os
import time

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from loguru import logger
from ingestion.crawl_cache import get_crawl_cache
from ingestion.crawler import crawl_sitemap, crawl_with_sitemap_progress


def test_cache_basic_operations():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∫–µ—à–∞."""
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∫–µ—à–∞...")

    cache = get_crawl_cache()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_url = "https://test.example.com/page1"
    test_html = "<html><body>Test content</body></html>"
    test_text = "Test content"

    # –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    logger.info("üìù –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    page_cache = cache.save_page(test_url, test_html, test_text, title="Test Page", page_type="test")

    assert page_cache.url == test_url
    assert page_cache.content_length == len(test_html)
    assert page_cache.page_type == "test"
    logger.success("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")

    # –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è
    logger.info("üîç –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è...")
    assert cache.has_page(test_url) == True
    assert cache.has_page("https://nonexistent.com") == False
    logger.success("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç")

    # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏
    logger.info("üìñ –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    loaded_page = cache.get_page(test_url)

    assert loaded_page is not None
    assert loaded_page.url == test_url
    assert loaded_page.html == test_html
    assert loaded_page.text == test_text
    assert loaded_page.title == "Test Page"
    logger.success("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç")

    # –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏
    logger.info("üîÑ –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏...")
    assert cache.is_page_fresh(test_url, test_html) == True
    assert cache.is_page_fresh(test_url, "different content") == False
    logger.success("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç")

    # –¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è
    logger.info("üóëÔ∏è –¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    cache.remove_page(test_url)
    assert cache.has_page(test_url) == False
    logger.success("‚úÖ –£–¥–∞–ª–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")


def test_cache_with_real_data():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–µ—à —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏."""
    logger.info("üåê –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–µ—à–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏...")

    # –ü–æ–ª—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ URL –∏–∑ sitemap
    urls = crawl_sitemap()
    if not urls:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å URL –∏–∑ sitemap")
        return

    test_urls = urls[:3]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 URL –¥–ª—è —Ç–µ—Å—Ç–∞
    logger.info(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º —Å {len(test_urls)} URL")

    cache = get_crawl_cache()
    initial_stats = cache.get_cache_stats()

    logger.info(f"–ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–µ—à–∞: {initial_stats['total_pages']} —Å—Ç—Ä–∞–Ω–∏—Ü")

    # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ - –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω—ã
    logger.info("üöÄ –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ crawling (–±–µ–∑ –∫–µ—à–∞)...")
    start_time = time.time()

    pages1 = crawl_with_sitemap_progress(strategy="jina", use_cache=True)
    first_crawl_time = time.time() - start_time

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤—ã–µ URL
    test_pages1 = [p for p in pages1 if p["url"] in test_urls]

    logger.info(f"–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫: {len(test_pages1)} —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ {first_crawl_time:.2f} —Å–µ–∫")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å –≤ –∫–µ—à
    final_stats = cache.get_cache_stats()
    logger.info(f"–ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞: {final_stats['total_pages']} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–µ—à–µ")

    # –í—Ç–æ—Ä–æ–π –∑–∞–ø—É—Å–∫ - —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–ª–∂–Ω—ã –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –∏–∑ –∫–µ—à–∞
    logger.info("‚ö° –í—Ç–æ—Ä–æ–π –∑–∞–ø—É—Å–∫ crawling (—Å –∫–µ—à–µ–º)...")
    start_time = time.time()

    pages2 = crawl_with_sitemap_progress(strategy="jina", use_cache=True)
    second_crawl_time = time.time() - start_time

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤—ã–µ URL
    test_pages2 = [p for p in pages2 if p["url"] in test_urls]

    logger.info(f"–í—Ç–æ—Ä–æ–π –∑–∞–ø—É—Å–∫: {len(test_pages2)} —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ {second_crawl_time:.2f} —Å–µ–∫")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ
    if second_crawl_time > 0 and first_crawl_time > 0:
        speedup = first_crawl_time / second_crawl_time
        logger.info(f"üöÄ –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {speedup:.1f}x")

        if speedup > 1.5:
            logger.success("‚úÖ –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç crawling")
        else:
            logger.warning("‚ö†Ô∏è –£—Å–∫–æ—Ä–µ–Ω–∏–µ –º–µ–Ω—å—à–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π
    cached_count = sum(1 for p in test_pages2 if p.get("cached", False))
    logger.info(f"üìä –°—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∫–µ—à–∞: {cached_count}/{len(test_pages2)}")

    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
    url_to_content1 = {p["url"]: p.get("html", "") for p in test_pages1}
    url_to_content2 = {p["url"]: p.get("html", "") for p in test_pages2}

    content_matches = 0
    for url in test_urls:
        if url in url_to_content1 and url in url_to_content2:
            if url_to_content1[url] == url_to_content2[url]:
                content_matches += 1

    logger.info(f"üìã –ö–æ–Ω—Ç–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–∞–µ—Ç: {content_matches}/{len(test_urls)} —Å—Ç—Ä–∞–Ω–∏—Ü")

    if content_matches == len(test_urls):
        logger.success("‚úÖ –ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É")
    else:
        logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ")


def test_cache_incremental_update():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ."""
    logger.info("üîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è...")

    from ingestion.pipeline import crawl_and_index

    # –¢–µ—Å—Ç —Ä–µ–∂–∏–º–∞ cache_only
    logger.info("üìñ –¢–µ—Å—Ç —Ä–µ–∂–∏–º–∞ cache_only...")
    try:
        stats = crawl_and_index(reindex_mode="cache_only")
        logger.info(f"cache_only: {stats['pages']} —Å—Ç—Ä–∞–Ω–∏—Ü, {stats['chunks']} —á–∞–Ω–∫–æ–≤")
        logger.success("‚úÖ –†–µ–∂–∏–º cache_only —Ä–∞–±–æ—Ç–∞–µ—Ç")
    except Exception as e:
        logger.error(f"‚ùå –†–µ–∂–∏–º cache_only –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")

    # –¢–µ—Å—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
    logger.info("ü§ñ –¢–µ—Å—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞...")
    try:
        stats = crawl_and_index(reindex_mode="auto", use_cache=True)
        logger.info(f"auto: {stats['pages']} —Å—Ç—Ä–∞–Ω–∏—Ü, {stats['chunks']} —á–∞–Ω–∫–æ–≤")
        logger.success("‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç–∞–µ—Ç")
    except Exception as e:
        logger.error(f"‚ùå –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")


def test_cache_management():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–µ—à–µ–º."""
    logger.info("üõ†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–µ—à–µ–º...")

    cache = get_crawl_cache()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = cache.get_cache_stats()
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞:")
    logger.info(f"  –°—Ç—Ä–∞–Ω–∏—Ü: {stats['total_pages']}")
    logger.info(f"  –†–∞–∑–º–µ—Ä: {stats['total_size_mb']} MB")

    # –°–ø–∏—Å–æ–∫ URL
    cached_urls = cache.get_cached_urls()
    logger.info(f"üìã –ó–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö URL: {len(cached_urls)}")

    if cached_urls:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 URL
        for i, url in enumerate(list(cached_urls)[:5], 1):
            logger.info(f"  {i}. {url}")

        if len(cached_urls) > 5:
            logger.info(f"  ... –∏ –µ—â–µ {len(cached_urls) - 5} URL")

    # –¢–µ—Å—Ç –æ—á–∏—Å—Ç–∫–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π
    logger.info("üßπ –¢–µ—Å—Ç –æ—á–∏—Å—Ç–∫–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π...")

    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ URL –¥–ª—è —Ç–µ—Å—Ç–∞
    valid_urls = set(list(cached_urls)[:max(1, len(cached_urls) // 2)])  # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ª–æ–≤–∏–Ω—É

    old_count = len(cached_urls)
    cache.cleanup_old_pages(valid_urls)
    new_count = len(cache.get_cached_urls())

    logger.info(f"–ë—ã–ª–æ: {old_count}, —Å—Ç–∞–ª–æ: {new_count}, —É–¥–∞–ª–µ–Ω–æ: {old_count - new_count}")

    if old_count > new_count:
        logger.success("‚úÖ –û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç")
    else:
        logger.info("‚ÑπÔ∏è –£—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    logger.info("üß™ –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è crawling")
    logger.info("=" * 70)

    try:
        # 1. –ë–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
        test_cache_basic_operations()

        logger.info("\n" + "=" * 70)

        # 2. –†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_cache_with_real_data()

        logger.info("\n" + "=" * 70)

        # 3. –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        test_cache_incremental_update()

        logger.info("\n" + "=" * 70)

        # 4. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–µ—à–µ–º
        test_cache_management()

        logger.success("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")

    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {type(e).__name__}: {e}")
        raise


if __name__ == "__main__":
    main()
